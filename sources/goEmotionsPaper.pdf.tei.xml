<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GoEmotions: A Dataset of Fine-Grained Emotions</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2020-06-03">3 Jun 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,138.26,95.06,94.89,10.75"><forename type="first">Dorottya</forename><surname>Demszky</surname></persName>
							<email>ddemszky@stanford.edu</email>
						</author>
						<author>
							<persName coords="1,249.59,95.06,123.62,10.75"><forename type="first">Dana</forename><surname>Movshovitz-Attias</surname></persName>
							<email>danama@google.com</email>
						</author>
						<author>
							<persName coords="1,389.64,95.06,68.14,10.75"><forename type="first">Jeongwoo</forename><surname>Ko</surname></persName>
						</author>
						<author>
							<persName coords="1,177.25,111.85,62.64,10.75"><forename type="first">Alan</forename><surname>Cowen</surname></persName>
							<email>acowen@google.com</email>
						</author>
						<author>
							<persName coords="1,256.34,111.85,83.71,10.75"><forename type="first">Gaurav</forename><surname>Nemade</surname></persName>
							<email>gnemade@google.com</email>
						</author>
						<author>
							<persName coords="1,356.49,111.85,57.83,10.75"><forename type="first">Sujith</forename><surname>Ravi</surname></persName>
							<email>sravi@sravi.org</email>
						</author>
						<title level="a" type="main">GoEmotions: A Dataset of Fine-Grained Emotions</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 58th Annual Meeting of the Association for Computational Linguistics						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<date type="published" when="2020-06-03">3 Jun 2020</date>
						</imprint>
					</monogr>
					<idno type="MD5">858D7F32652CB79E460027DC1B54D48E</idno>
					<idno type="DOI">10.18653/v1/2020.acl-main.372</idno>
					<idno type="arXiv">arXiv:2005.00547v2[cs.CL]</idno>
					<ptr type="open-access" target="https://www.aclweb.org/anthology/2020.acl-main.372.pdf" />
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2-SNAPSHOT" ident="GROBID" when="2022-06-14T15:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p><s coords="1,89.01,234.63,184.25,8.64;1,89.01,246.59,185.90,8.64;1,89.01,258.54,184.25,8.64;1,89.01,270.50,63.92,8.64">Understanding emotion expressed in language has a wide range of applications, from building empathetic chatbots to detecting harmful online behavior.</s><s coords="1,155.99,270.50,117.27,8.64;1,89.01,282.45,184.25,8.64;1,89.01,294.41,185.90,8.64;1,89.01,306.36,89.40,8.64">Advancement in this area can be improved using large-scale datasets with a fine-grained typology, adaptable to multiple downstream tasks.</s><s coords="1,183.57,306.36,91.34,8.64;1,89.01,318.32,184.25,8.64;1,89.01,330.27,184.42,8.64;1,89.01,342.23,135.17,8.64">We introduce GoEmotions, the largest manually annotated dataset of 58k English Reddit comments, labeled for 27 emotion categories or Neutral.</s><s coords="1,228.53,342.23,46.38,8.64;1,89.01,354.18,184.25,8.64;1,89.01,366.14,167.28,8.64">We demonstrate the high quality of the annotations via Principal Preserved Component Analysis.</s><s coords="1,260.23,366.14,13.03,8.64;1,89.01,378.09,185.90,8.64;1,89.01,390.05,184.42,8.64;1,89.01,402.00,184.25,8.64;1,89.01,413.96,125.94,8.64">We conduct transfer learning experiments with existing emotion benchmarks to show that our dataset generalizes well to other domains and different emotion taxonomies.</s><s coords="1,226.75,413.96,48.16,8.64;1,89.01,425.91,184.25,8.64;1,89.01,437.87,12.45,8.64">Our BERTbased model achieves an average F1-score of .46</s><s coords="1,106.58,437.87,166.67,8.64;1,89.01,449.82,117.58,8.64;1,206.59,448.15,3.49,6.05">across our proposed taxonomy, leaving much room for improvement. 1</s></p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1" coords="1,72.00,475.67,82.81,10.75">Introduction</head><p><s coords="1,72.00,499.76,218.27,9.46;1,72.00,513.31,181.14,9.46">Emotion expression and detection are central to the human experience and social interaction.</s><s coords="1,256.54,513.31,33.73,9.46;1,72.00,526.86,218.27,9.46;1,71.61,540.41,218.66,9.46;1,72.00,553.96,218.27,9.46;1,72.00,567.51,211.20,9.46">With as many as a handful of words we are able to express a wide variety of subtle and complex emotions, and it has thus been a long-term goal to enable machines to understand affect and emotion <ref type="bibr" coords="1,218.06,567.51,60.44,9.46" target="#b31">(Picard, 1997)</ref>.</s></p><p><s coords="1,82.91,581.88,209.17,9.46;1,72.00,595.43,218.27,9.46;1,72.00,608.98,220.07,9.46;1,72.00,622.53,218.27,9.46;1,72.00,636.08,219.18,9.46;1,72.00,649.63,218.27,9.46;1,71.64,663.18,159.51,9.46">In the past decade, NLP researchers made available several datasets for language-based emotion classification for a variety of domains and applications, including for news headlines <ref type="bibr" coords="1,235.41,622.53,54.86,9.46;1,72.00,636.08,87.67,9.46" target="#b37">(Strapparava and Mihalcea, 2007)</ref>, tweets <ref type="bibr" coords="1,197.55,636.08,93.63,9.46" target="#b12">(CrowdFlower, 2016;</ref><ref type="bibr" coords="1,72.00,649.63,106.31,9.46">Mohammad et al., 2018)</ref>, and narrative sequences <ref type="bibr" coords="1,71.64,663.18,71.07,9.46" target="#b21">(Liu et al., 2019)</ref>, to name just a few.</s><s coords="1,234.54,663.18,57.54,9.46;1,72.00,676.73,220.08,9.46;1,72.00,690.28,219.00,9.46;1,72.00,703.82,220.08,9.46;1,307.28,410.74,218.54,9.46;1,306.92,424.29,116.37,9.46">However, existing available datasets are (1) mostly small, containing up to several thousand instances, and (2) cover a limited emotion taxonomy, with coarse clas- sification into Ekman <ref type="bibr" coords="1,403.63,410.74,71.12,9.46" target="#b17">(Ekman, 1992b)</ref> or Plutchik <ref type="bibr" coords="1,306.92,424.29,70.91,9.46" target="#b33">(Plutchik, 1980)</ref> emotions.</s></p><p><s coords="1,318.19,441.04,209.17,9.46;1,307.28,454.59,218.27,9.46;1,307.28,468.13,220.08,9.46;1,307.28,481.68,155.10,9.46">Recently, <ref type="bibr" coords="1,363.68,441.04,121.55,9.46" target="#b2">Bostan and Klinger (2018)</ref> have aggregated 14 popular emotion classification corpora under a unified framework that allows direct comparison of the existing resources.</s><s coords="1,472.29,481.68,54.61,9.46;1,307.28,495.23,218.27,9.46;1,307.28,508.78,220.07,9.46;1,307.28,522.33,218.27,9.46;1,306.89,535.88,188.58,9.46">Importantly, their analysis suggests annotation quality gaps in the largest manually annotated emotion classification dataset, CrowdFlower (2016), containing 40K tweets labeled for one of 13 emotions.</s><s coords="1,498.86,535.88,26.69,9.46;1,307.28,549.43,218.27,9.46;1,307.28,562.98,218.65,9.46;1,307.28,576.53,220.08,9.46;1,307.28,590.08,216.86,9.46">While their work enables such comparative evaluations, it highlights the need for a large-scale, consistently labeled emotion dataset over a fine-grained taxonomy, with demonstrated high-quality annotations.</s></p><p><s coords="1,318.19,606.82,207.36,9.46;1,307.28,620.37,218.65,9.46;1,307.28,633.92,218.26,9.46;1,307.28,647.47,218.27,9.46;1,307.28,661.02,141.19,9.46">To this end, we compiled GoEmotions, the largest human annotated dataset of 58k carefully selected Reddit comments, labeled for 27 emotion categories or Neutral, with comments extracted from popular English subreddits.</s><s coords="1,451.86,661.02,73.69,9.46;1,307.28,674.57,170.53,9.46">Table <ref type="table" coords="1,478.11,661.02,5.36,9.46" target="#tab_0">1</ref> shows an illustrative sample of our collected data.</s><s coords="1,481.12,674.57,44.42,9.46;1,307.28,688.12,218.27,9.46;1,307.28,701.67,158.78,9.46">We design our emotion taxonomy considering related work in psychology and coverage in our data.</s><s coords="1,469.38,701.67,56.16,9.46;1,307.28,715.22,220.08,9.46;1,307.28,728.77,218.27,9.46;1,307.28,742.32,220.08,9.46;1,307.28,755.86,218.27,9.46;2,72.00,66.67,220.08,9.46;2,72.00,80.22,218.27,9.46;2,72.00,93.76,220.07,9.46;2,72.00,107.31,75.45,9.46">In contrast to Ekman's taxonomy, which includes only one positive emotion (joy), our taxonomy includes a large number of positive, negative, and ambiguous emotion categories, making it suitable for downstream conversation understanding tasks that require a subtle understanding of emotion expression, such as the analysis of customer feedback or the enhancement of chatbots.</s></p><p><s coords="2,82.91,121.18,207.36,9.46;2,72.00,134.73,169.22,9.46">We include a thorough analysis of the annotated data and the quality of the annotations.</s><s coords="2,244.61,134.73,47.47,9.46;2,72.00,148.28,219.63,9.46;2,72.00,161.83,220.08,9.46;2,72.00,175.38,218.27,9.46;2,72.00,188.93,218.27,9.46;2,72.00,202.47,128.18,9.46">Via Principal Preserved Component Analysis <ref type="bibr" coords="2,230.01,148.28,61.62,9.46;2,72.00,161.83,28.25,9.46" target="#b11">(Cowen et al., 2019b)</ref>, we show a strong support for reliable dissociation among all 27 emotion categories, indicating the suitability of our annotations for building an emotion classification model.</s></p><p><s coords="2,82.91,216.34,209.17,9.46;2,72.00,229.89,218.27,9.46;2,72.00,243.44,220.08,9.46;2,72.00,256.99,220.17,9.46">We perform hierarchical clustering on the emotion judgments, finding that emotions related in intensity cluster together closely and that the toplevel clusters correspond to sentiment categories.</s><s coords="2,71.66,270.54,218.80,9.46;2,72.00,284.09,218.27,9.46;2,72.00,297.64,133.33,9.46">These relations among emotions allow for their potential grouping into higher-level categories, if desired for a downstream task.</s></p><p><s coords="2,82.91,311.50,209.16,9.46;2,72.00,325.05,220.18,9.46">We provide a strong baseline for modeling finegrained emotion classification over GoEmotions.</s><s coords="2,72.00,338.60,219.63,9.46;2,72.00,352.15,218.45,9.46;2,72.00,365.70,218.27,9.46;2,72.00,379.25,144.06,9.46">By fine-tuning a BERT-base model <ref type="bibr" coords="2,230.73,338.60,60.90,9.46;2,72.00,352.15,23.36,9.46" target="#b14">(Devlin et al., 2019)</ref>, we achieve an average F1-score of .46 over our taxonomy, .64 over an Ekman-style grouping into six coarse categories and .69</s><s coords="2,218.80,379.25,71.47,9.46;2,72.00,392.80,42.96,9.46">over a sentiment grouping.</s><s coords="2,119.01,392.80,173.06,9.46;2,72.00,406.35,218.65,9.46;2,72.00,419.90,219.94,9.46">These results leave much room for improvement, showcasing this task is not yet fully addressed by current state-of-the-art NLU models.</s></p><p><s coords="2,82.91,433.76,207.36,9.46;2,72.00,447.31,218.45,9.46;2,72.00,460.86,218.27,9.46;2,72.00,474.41,220.17,9.46">We conduct transfer learning experiments with existing emotion benchmarks to show that our data can generalize to different taxonomies and domains, such as tweets and personal narratives.</s><s coords="2,72.00,487.96,220.07,9.46;2,72.00,501.51,218.27,9.46;2,72.00,515.06,218.26,9.46;2,72.00,528.61,218.27,9.46;2,72.00,542.16,213.22,9.46">Our experiments demonstrate that given limited resources to label additional emotion classification data for specialized domains, our data can provide baseline emotion understanding and contribute to increasing model accuracy for the target domain.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2" coords="2,72.00,565.87,89.09,10.75">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1" coords="2,72.00,588.29,106.66,9.81">Emotion Datasets</head><p><s coords="2,72.00,606.82,220.08,9.46;2,72.00,620.37,220.08,9.46;2,72.00,633.92,218.27,9.46;2,72.00,647.47,220.08,9.46;2,72.00,661.02,47.65,9.46">Ever since Affective Text <ref type="bibr" coords="2,185.63,606.82,101.49,9.46;2,72.00,620.37,43.55,9.46" target="#b37">(Strapparava and Mihalcea, 2007)</ref>, the first benchmark for emotion recognition was introduced, the field has seen several emotion datasets that vary in size, domain and taxonomy (cf.</s><s coords="2,122.39,661.02,117.95,9.46"><ref type="bibr" coords="2,122.39,661.02,113.27,9.46" target="#b2">Bostan and Klinger, 2018)</ref>.</s><s coords="2,243.74,661.02,48.34,9.46;2,72.00,674.57,219.63,9.46;2,72.00,688.12,135.24,9.46">The majority of emotion datasets are constructed manually, but tend to be relatively small.</s><s coords="2,211.10,688.12,80.97,9.46;2,72.00,701.67,218.27,9.46;2,72.00,715.22,218.27,9.46;2,72.00,728.77,218.27,9.46;2,72.00,742.32,99.10,9.46">The largest manually labeled dataset is CrowdFlower (2016), with 39k labeled examples, which were found by <ref type="bibr" coords="2,260.57,715.22,29.70,9.46;2,72.00,728.77,82.46,9.46" target="#b2">Bostan and Klinger (2018)</ref> to be noisy in comparison with other emotion datasets.</s><s coords="2,174.51,742.32,117.57,9.46;2,72.00,755.86,218.27,9.46;2,307.28,66.67,220.07,9.46;2,307.28,80.22,120.82,9.46">Other datasets are automatically weakly-labeled, based on emotion-related hashtags on Twitter <ref type="bibr" coords="2,401.82,66.67,89.60,9.46" target="#b40">(Wang et al., 2012;</ref><ref type="bibr" coords="2,495.83,66.67,31.52,9.46;2,307.28,80.22,116.03,9.46" target="#b0">Abdul-Mageed and Ungar, 2017)</ref>.</s><s coords="2,432.94,80.22,92.61,9.46;2,307.28,93.76,218.27,9.46;2,307.28,107.31,218.45,9.46;2,307.28,120.86,78.16,9.46">We build our dataset manually, making it the largest human annotated dataset, with multiple annotations per example for quality assurance.</s></p><p><s coords="2,318.19,134.41,207.36,9.46;2,307.28,147.96,220.08,9.46;2,307.28,161.51,188.50,9.46">Several existing datasets come from the domain of Twitter, given its informal language and expressive content, such as emojis and hashtags.</s><s coords="2,500.39,161.51,25.34,9.46;2,307.28,175.06,218.27,9.46;2,307.28,188.61,220.08,9.46;2,307.28,199.63,218.27,11.99;2,307.28,215.71,218.26,9.46;2,307.28,229.26,218.45,9.46;2,307.28,242.81,184.84,9.46">Other datasets annotate news headlines <ref type="bibr" coords="2,452.24,175.06,73.30,9.46;2,307.28,188.61,71.84,9.46" target="#b37">(Strapparava and Mihalcea, 2007)</ref>, dialogs <ref type="bibr" coords="2,423.71,188.61,70.30,9.46" target="#b20">(Li et al., 2017)</ref>, fairytales <ref type="bibr" coords="2,330.91,202.16,78.79,9.46" target="#b1">(Alm et al., 2005)</ref>, movie subtitles <ref type="bibr" coords="2,489.08,199.63,36.46,11.99;2,307.28,215.71,49.90,9.46">( Öhman et al., 2018)</ref>, sentences based on FrameNet <ref type="bibr" coords="2,496.00,215.71,29.54,9.46;2,307.28,229.26,50.05,9.46" target="#b18">(Ghazi et al., 2015)</ref>, or self-reported experiences <ref type="bibr" coords="2,488.85,229.26,36.88,9.46;2,307.28,242.81,85.82,9.46" target="#b35">(Scherer and Wallbott, 1994)</ref> among other domains.</s><s coords="2,495.52,242.81,30.02,9.46;2,307.28,256.35,218.27,9.46;2,307.28,269.90,46.96,9.46">We are the first to build on Reddit comments for emotion prediction.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2" coords="2,307.28,291.74,115.97,9.81">Emotion Taxonomy</head><p><s coords="2,307.28,309.65,218.27,9.46;2,307.28,323.20,109.36,9.46">One of the main aspects distinguishing our dataset is its emotion taxonomy.</s><s coords="2,420.21,323.20,107.14,9.46;2,307.28,336.74,220.08,9.46;2,307.28,350.29,219.63,9.46;2,307.28,363.84,218.65,9.46;2,307.28,377.39,218.27,9.46;2,306.92,390.94,218.90,9.46;2,307.28,404.49,219.63,9.46;2,307.28,418.04,28.18,9.46">The vast majority of existing datasets contain annotations for minor variations of the 6 basic emotion categories (joy, anger, fear, sadness, disgust, and surprise) proposed by <ref type="bibr" coords="2,307.28,377.39,67.74,9.46" target="#b16">Ekman (1992a)</ref> and/or along affective dimensions (valence and arousal) that underpin the circumplex model of affect <ref type="bibr" coords="2,375.91,404.49,65.56,9.46" target="#b34">(Russell, 2003;</ref><ref type="bibr" coords="2,444.20,404.49,82.70,9.46;2,307.28,418.04,23.48,9.46" target="#b4">Buechel and Hahn, 2017)</ref>.</s></p><p><s coords="2,318.19,431.59,207.75,9.46;2,307.28,445.14,220.08,9.46;2,307.28,458.69,220.08,9.46;2,307.28,472.24,220.08,9.46;2,307.28,485.79,220.08,9.46;2,307.28,499.33,148.23,9.46">Recent advances in psychology have offered new conceptual and methodological approaches to capturing the more complex "semantic space" of emotion <ref type="bibr" coords="2,326.71,472.24,87.67,9.46" target="#b6">(Cowen et al., 2019a</ref>) by studying the distribution of emotion responses to a diverse array of stimuli via computational techniques.</s><s coords="2,459.54,499.33,66.01,9.46;2,307.28,512.88,220.08,9.46;2,307.28,526.43,218.27,9.46;2,307.00,539.98,218.54,9.46;2,306.92,553.53,218.63,9.46;2,306.92,567.08,219.01,9.46;2,306.92,580.63,220.44,9.46;2,307.28,594.18,121.43,9.46">Studies guided by these principles have identified 27 distinct varieties of emotional experience conveyed by short videos <ref type="bibr" coords="2,339.59,539.98,121.49,9.46" target="#b9">(Cowen and Keltner, 2017)</ref>, 13 by music <ref type="bibr" coords="2,306.92,553.53,105.15,9.46">(Cowen et al., in press)</ref>, 28 by facial expression <ref type="bibr" coords="2,306.92,567.08,116.81,9.46" target="#b10">(Cowen and Keltner, 2019)</ref>, 12 by speech prosody <ref type="bibr" coords="2,306.92,580.63,93.29,9.46" target="#b11">(Cowen et al., 2019b)</ref>, and 24 by nonverbal vocalization <ref type="bibr" coords="2,338.96,594.18,85.14,9.46" target="#b7">(Cowen et al., 2018)</ref>.</s><s coords="2,432.04,594.18,93.50,9.46;2,307.28,607.73,220.08,9.46;2,307.28,621.28,218.26,9.46;2,307.28,634.83,218.27,9.46;2,307.28,648.38,65.44,9.46">In this work, we build on these methods and findings to devise our granular taxonomy for text-based emotion recognition and study the dimensionality of language-based emotion space.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3" coords="2,307.28,670.22,166.37,9.81">Emotion Classification Models</head><p><s coords="2,307.28,688.12,218.27,9.46;2,307.28,701.67,220.08,9.46;2,307.28,715.22,14.73,9.46">Both feature-based and neural models have been used to build automatic emotion classification models.</s><s coords="2,325.38,715.22,201.98,9.46;2,307.28,728.77,220.07,9.46;2,307.28,742.32,165.51,9.46">Feature-based models often make use of handbuilt lexicons, such as the Valence Arousal Dominance Lexicon <ref type="bibr" coords="2,380.06,742.32,87.93,9.46" target="#b22">(Mohammad, 2018)</ref>.</s><s coords="2,479.14,742.32,48.21,9.46;2,307.28,755.86,218.27,9.46;3,72.00,66.67,220.08,9.46;3,72.00,80.22,220.08,9.46;3,72.00,93.76,220.08,9.46;3,72.00,107.31,218.27,9.46;3,72.00,120.86,218.26,9.46;3,72.00,134.41,163.18,9.46">Using representations from BERT <ref type="bibr" coords="2,421.16,755.86,90.62,9.46" target="#b14">(Devlin et al., 2019)</ref>, a transformer-based model with language model pretraining, has recently shown to reach state-of-theart performance on several NLP tasks, also including emotion prediction: the top-performing models in the EmotionX Challenge <ref type="bibr" coords="3,192.06,120.86,84.82,9.46" target="#b19">(Hsu and Ku, 2018)</ref> all employed a pre-trained BERT model.</s><s coords="3,238.60,134.41,51.66,9.46;3,72.00,147.96,218.27,9.46;3,72.00,161.51,171.51,9.46">We also use the BERT model in our experiments and we find that it outperforms our biLSTM model.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3" coords="3,72.00,184.41,81.70,10.75">GoEmotions</head><p><s coords="3,72.00,206.68,219.63,9.46;3,72.00,220.23,220.18,9.46">Our dataset is composed of 58K Reddit comments, labeled for one or more of 27 emotion(s) or Neutral.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1" coords="3,72.00,242.48,207.56,9.81">Selecting &amp; Curating Reddit comments</head><p><s coords="3,71.49,260.54,220.60,9.46;3,72.00,274.09,74.40,9.46;3,146.40,272.05,3.99,6.91;3,150.88,274.09,139.39,9.46;3,72.00,287.64,196.85,9.46">We use a Reddit data dump originating in the redditdata-tools project 2 , which contains comments from 2005 (the start of Reddit) to January 2019.</s><s coords="3,275.70,287.64,14.58,9.46;3,72.00,301.19,218.27,9.46;3,72.00,314.74,191.47,9.46">We select subreddits with at least 10k comments and remove deleted and non-English comments.</s></p><p><s coords="3,82.91,328.37,209.17,9.46;3,72.00,341.92,219.63,9.46;3,72.00,355.47,218.27,9.46;3,72.00,369.02,50.38,9.46">Reddit is known for a demographic bias leaning towards young male users <ref type="bibr" coords="3,203.98,341.92,87.65,9.46;3,72.00,355.47,23.41,9.46" target="#b15">(Duggan and Smith, 2013)</ref>, which is not reflective of a globally diverse population.</s><s coords="3,127.24,369.02,163.43,9.46;3,72.00,382.57,219.63,9.46;3,72.00,396.12,27.61,9.46">The platform also introduces a skew towards toxic, offensive language <ref type="bibr" coords="3,227.50,382.57,64.13,9.46;3,72.00,396.12,23.01,9.46" target="#b26">(Mohan et al., 2017)</ref>.</s><s coords="3,102.95,396.12,187.70,9.46;3,72.00,409.67,220.08,9.46;3,72.00,423.22,218.27,9.46;3,72.00,436.77,220.07,9.46;3,72.00,450.31,220.21,9.46;3,72.00,463.86,16.34,9.46">Thus, Reddit content has been used to study depression <ref type="bibr" coords="3,122.82,409.67,120.95,9.46" target="#b32">(Pirina and C ¸öltekin, 2018)</ref>, microaggressions <ref type="bibr" coords="3,116.53,423.22,104.38,9.46" target="#b3">(Breitfeller et al., 2019)</ref>, and Yanardag and Rahwan (2018) have shown the effect of using biased Reddit data by training a "psychopath" bot.</s><s coords="3,91.60,463.86,198.67,9.46;3,72.00,477.41,220.08,9.46;3,72.00,490.96,218.26,9.46;3,72.00,504.51,218.45,9.46;3,72.00,518.06,149.67,9.46">To address these concerns, and enable building broadly representative emotion models using GoEmotions, we take a series of data curation measures to ensure our data does not reinforce general, nor emotion-specific, language biases.</s><s coords="3,82.91,531.69,207.36,9.46;3,72.00,545.24,220.08,9.46;3,72.00,558.79,220.08,9.46;3,72.00,572.34,150.82,9.46">We identify harmful comments using pre-defined lists containing offensive/adult, vulgar (mildly offensive profanity), identity, and religion terms (included as supplementary material).</s><s coords="3,226.21,572.34,64.05,9.46;3,72.00,585.89,220.17,9.46">These are used for data filtering and masking, as described below.</s><s coords="3,72.00,599.44,218.64,9.46;3,72.00,612.99,218.27,9.46;3,72.00,626.54,198.35,9.46">Lists were internally compiled and we believe they are comprehensive and widely useful for dataset curation, however, they may not be complete.</s></p><p><s coords="3,72.00,647.31,92.38,9.81">Reducing profanity.</s><s coords="3,175.29,647.74,114.98,9.46;3,72.00,661.29,95.72,9.46;3,167.71,659.24,3.99,6.91;3,175.76,661.29,116.31,9.46;3,72.00,674.83,220.17,9.46">We remove subreddits that are not safe for work 3 and where 10%+ of comments include offensive/adult and vulgar tokens.</s><s coords="3,71.49,688.38,220.60,9.46;3,72.00,701.93,76.19,9.46">We remove remaining comments that include offensive/adult tokens.</s><s coords="3,151.59,701.93,138.68,9.46;3,72.00,715.48,218.27,9.46;3,307.28,66.67,81.82,9.46">Vulgar comments are preserved as we believe they are central to learning about negative emotions.</s><s coords="3,392.48,66.67,133.06,9.46;3,307.28,80.22,64.73,9.46">The dataset includes the list of filtered tokens.</s></p><p><s coords="3,307.28,100.05,73.66,9.81">Manual review.</s><s coords="3,391.84,100.48,134.08,9.46;3,307.28,114.03,218.27,9.46;3,307.28,127.57,218.45,9.46;3,307.28,141.12,165.96,9.46">We manually review identity comments and remove those offensive towards a particular ethnicity, gender, sexual orientation, or disability, to the best of our judgment.</s></p><p><s coords="3,307.28,160.96,76.92,9.81">Length filtering.</s><s coords="3,395.10,161.39,132.26,9.46;3,307.28,174.93,220.08,9.46;3,307.28,188.48,87.36,9.46">We apply NLTK's word tokenizer and select comments 3-30 tokens long, including punctuation.</s><s coords="3,397.82,188.48,127.72,9.46;3,307.28,202.03,220.08,9.46;3,307.28,215.58,218.27,9.46;3,306.88,229.13,147.75,9.46">To create a relatively balanced distribution of comment length, we perform downsampling, capping by the number of comments with the median token count (12).</s></p><p><s coords="3,307.28,248.97,98.00,9.81">Sentiment balancing.</s><s coords="3,416.19,249.39,109.36,9.46;3,307.28,262.94,218.27,9.46;3,307.28,276.49,220.07,9.46;3,307.28,290.04,24.69,9.46">We reduce sentiment bias by removing subreddits with little representation of positive, negative, ambiguous, or neutral sentiment.</s><s coords="3,335.36,290.04,190.18,9.46;3,307.28,303.59,218.27,9.46;3,307.28,317.14,153.79,9.46">To estimate a comment's sentiment, we run our emotion prediction model, trained on a pilot batch of 2.2k annotated examples.</s><s coords="3,466.04,317.14,59.51,9.46;3,307.28,330.69,218.27,9.46;3,307.28,344.24,218.26,9.46;3,307.28,357.79,218.27,9.46;3,307.28,371.34,191.32,9.46">The mapping of emotions into sentiment categories is found in Figure <ref type="figure" coords="3,337.50,344.24,4.01,9.46" target="#fig_1">2</ref>. We exclude subreddits consisting of more than 30% neutral comments or less than 20% of negative, positive, or ambiguous comments.</s></p><p><s coords="3,307.28,391.17,90.75,9.81">Emotion balancing.</s><s coords="3,408.94,391.60,118.41,9.46;3,307.28,405.15,220.07,9.46;3,307.28,418.70,64.32,9.46">We assign a predicted emotion to each comment using the pilot model described above.</s><s coords="3,376.25,418.70,149.67,9.46;3,307.28,432.25,218.27,9.46;3,307.28,445.79,220.07,9.46;3,307.28,459.34,86.97,9.46">Then, we reduce emotion bias by downsampling the weakly-labelled data, capping by the number of comments belonging to the median emotion count.</s></p><p><s coords="3,307.28,479.18,97.22,9.81">Subreddit balancing.</s><s coords="3,415.41,479.60,111.95,9.46;3,307.28,493.15,220.08,9.46;3,307.28,506.70,199.99,9.46">To avoid over representation of popular subreddits, we perform downsampling, capping by the median subreddit count.</s></p><p><s coords="3,318.19,520.25,207.36,9.46;3,307.28,533.80,211.79,9.46">From the remaining 315k comments (from 482 subreddits), we randomly sample for annotation.</s></p><p><s coords="3,307.28,553.64,43.18,9.81">Masking.</s><s coords="3,361.36,554.06,164.18,9.46;3,307.28,567.61,218.26,9.46;3,307.28,581.16,175.37,9.46">We mask proper names referring to people with a [NAME] token, using a BERT-based Named Entity Tagger <ref type="bibr" coords="3,403.11,581.16,74.87,9.46" target="#b39">(Tsai et al., 2019)</ref>.</s><s coords="3,486.02,581.16,39.80,9.46;3,307.28,594.71,181.83,9.46">We mask religion terms with a [RELIGION] token.</s><s coords="3,492.48,594.71,33.07,9.46;3,307.28,608.26,192.28,9.46">The list of these terms is included with our dataset.</s><s coords="3,503.92,608.26,21.63,9.46;3,307.28,621.81,220.08,9.46;3,307.28,635.36,16.67,9.46">Note that raters viewed unmasked comments during rating.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2" coords="3,307.28,656.67,132.03,9.81">Taxonomy of Emotions</head><p><s coords="3,306.76,674.57,219.16,9.46;3,307.28,688.12,153.22,9.46">When creating the taxonomy, we seek to jointly maximize the following objectives.</s></p><p><s coords="3,318.19,701.48,209.17,9.64;3,307.28,715.03,125.42,9.39">1. Provide greatest coverage in terms of emotions expressed in our data.</s><s coords="3,438.28,715.22,87.26,9.46;3,307.28,728.77,218.27,9.46;3,307.28,742.32,218.27,9.46;3,307.28,755.86,153.61,9.46">To address this, we manually labeled a small subset of the data, and ran a pilot task where raters can suggest emotion labels on top of the pre-defined set.</s></p><p><s coords="4,82.91,66.48,207.36,9.64;4,72.00,80.03,109.35,9.39">2. Provide greatest coverage in terms of kinds of emotional expression.</s><s coords="4,186.19,80.22,104.45,9.46;4,72.00,93.76,218.27,9.46;4,71.64,107.31,218.62,9.46;4,72.00,120.86,65.18,9.46">We consult psychology literature on emotion expression and recognition <ref type="bibr" coords="4,71.64,107.31,69.52,9.46" target="#b33">(Plutchik, 1980;</ref><ref type="bibr" coords="4,143.90,107.31,113.36,9.46" target="#b9">Cowen and Keltner, 2017;</ref><ref type="bibr" coords="4,260.01,107.31,30.25,9.46;4,72.00,120.86,60.28,9.46" target="#b11">Cowen et al., 2019b)</ref>.</s><s coords="4,146.25,120.86,144.02,9.46;4,72.00,134.41,220.07,9.46;4,72.00,147.96,218.27,9.46;4,72.00,161.51,218.27,9.46;4,72.00,175.06,218.27,9.46;4,72.00,188.61,218.27,9.46;4,72.00,202.16,52.25,9.46">Since, to our knowledge, there has not been research that identifies principal categories for emotion recognition in the domain of text (see Section 2.2), we consider those emotions that are identified as basic in other domains (video and speech) and that we can assume to apply to text as well.</s></p><p><s coords="4,82.91,215.52,207.36,9.64;4,72.00,229.07,92.53,9.39">3. Limit overlap among emotions and limit the number of emotions.</s><s coords="4,170.11,229.26,120.15,9.46;4,72.00,242.81,218.27,9.46;4,72.00,256.35,132.07,9.46">We do not want to include emotions that are too similar, since that makes the annotation task more difficult.</s><s coords="4,207.45,256.35,84.63,9.46;4,72.00,269.90,218.27,9.46;4,72.00,283.45,153.76,9.46">Moreover, combining similar labels with high coverage would result in an explosion in annotated labels.</s></p><p><s coords="4,82.91,297.00,207.36,9.46;4,71.66,310.55,98.67,9.46">The final set of selected emotions is listed in Table <ref type="table" coords="4,99.20,310.55,4.17,9.46" target="#tab_4">4</ref>, and Figure <ref type="figure" coords="4,161.98,310.55,4.17,9.46" target="#fig_0">1</ref>.</s><s coords="4,174.76,310.55,115.51,9.46;4,72.00,324.10,220.08,9.46;4,72.00,337.65,22.11,9.46">See Appendix B for more details on our multi-step taxonomy selection procedure.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3" coords="4,72.00,359.47,77.28,9.81">Annotation</head><p><s coords="4,71.49,377.37,176.05,9.46">We assigned three raters to each example.</s><s coords="4,250.76,377.37,39.50,9.46;4,72.00,390.92,218.27,9.46;4,72.00,404.47,220.18,9.46">For those examples where no raters agree on at least one emotion label, we assigned two additional raters.</s><s coords="4,71.61,418.02,213.95,9.46;4,285.55,415.98,3.99,6.91;4,72.00,438.37,59.09,9.81">All raters are native English speakers from India. 4   Instructions.</s><s coords="4,142.00,438.80,148.26,9.46;4,72.00,452.35,218.27,9.46;4,72.00,465.89,219.00,9.46;4,72.00,479.44,186.60,9.46">Raters were asked to identify the emotions expressed by the writer of the text, given pre-defined emotion definitions (see Appendix A) and a few example texts for each emotion.</s><s coords="4,261.97,479.44,28.29,9.46;4,71.61,492.99,218.66,9.46;4,72.00,506.54,218.27,9.46;4,72.00,520.09,220.17,9.46">Raters were free to select multiple emotions, but were asked to only select those ones for which they were reasonably confident that it is expressed in the text.</s><s coords="4,72.00,533.64,218.27,9.46;4,72.00,547.19,199.83,9.46">If raters were not certain about any emotion being expressed, they were asked to select Neutral.</s><s coords="4,275.70,547.19,14.58,9.46;4,72.00,560.74,218.27,9.46;4,72.00,574.29,218.27,9.46;4,72.00,587.84,157.91,9.46">We included a checkbox for raters to indicate if an example was particularly difficult to label, in which case they could select no emotions.</s><s coords="4,234.09,587.84,56.18,9.46;4,72.00,601.39,211.81,9.46">We removed all examples for which no emotion was selected.</s></p><p><s coords="4,72.00,621.74,90.93,9.81">The rater interface.</s><s coords="4,173.83,622.16,118.24,9.46;4,72.00,635.71,218.27,9.46;4,72.00,649.26,92.51,9.46">Reddit comments were presented with no additional metadata (such as the author or subreddit).</s><s coords="4,170.06,649.26,120.21,9.46;4,72.00,662.81,218.27,9.46;4,72.00,676.36,218.26,9.46;4,72.00,689.91,220.08,9.46;4,72.00,703.46,220.07,9.46;4,72.00,717.01,139.27,9.46">To help raters navigate the large space of emotion in our taxonomy, they were presented a table containing all emotion categories aggregated by sentiment (by the mapping in Figure <ref type="figure" coords="4,89.79,703.46,4.63,9.46" target="#fig_1">2</ref>) and whether that emotion is generally expressed towards something (e.g.</s><s coords="4,214.68,717.01,75.58,9.46;4,307.28,277.28,140.38,9.46">disapproval) or is more of an intrinsic feeling (e.g.</s><s coords="4,451.04,277.28,20.11,9.46">joy).</s><s coords="4,474.54,277.28,52.82,9.46;4,307.28,290.83,218.27,9.46;4,306.88,304.38,218.66,9.46;4,307.28,317.93,218.26,9.46;4,307.28,331.48,169.03,9.46">The instructions highlighted that this separation of categories was by no means clear-cut, but captured general tendencies, and we encouraged raters to ignore the categorization whenever they saw fit.</s><s coords="4,482.89,331.48,42.66,9.46;4,306.88,345.03,218.66,9.46;4,307.28,358.58,218.45,9.46;4,307.28,372.13,61.51,9.46">Emotions with a straightforward mapping onto emojis were shown with an emoji in the UI, to further ease their interpretation.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4" coords="4,307.28,395.47,88.67,10.75">Data Analysis</head><p><s coords="4,306.94,418.04,193.42,9.46">Table <ref type="table" coords="4,332.63,418.04,5.35,9.46" target="#tab_1">2</ref> shows summary statistics for the data.</s><s coords="4,503.56,418.04,21.98,9.46;4,307.28,431.59,218.27,9.46;4,307.28,445.14,220.08,9.46;4,307.28,458.69,45.53,9.46">Most of the examples (83%) have a single emotion label and have at least two raters agreeing on a single label (94%).</s><s coords="4,356.20,458.69,169.35,9.46;4,307.28,472.24,218.26,9.46;4,307.28,485.79,218.27,9.46;4,307.28,499.34,194.81,9.46">The Neutral category makes up 26% of all emotion labels -we exclude that category from the following analyses, since we do not consider it to be part of the semantic space of emotions.</s></p><p><s coords="4,318.19,513.10,209.26,9.46">Figure <ref type="figure" coords="4,348.37,513.10,5.35,9.46" target="#fig_0">1</ref> shows the distribution of emotion labels.</s><s coords="4,306.76,526.65,218.78,9.46;4,307.28,540.20,73.23,9.46">We can see a large disparity in terms of emotion frequencies (e.g.</s><s coords="4,383.88,540.01,143.47,9.64;4,307.28,553.56,220.07,9.64;4,307.28,567.29,220.18,9.46">admiration is 30 times more frequent than grief ), despite our emotion and sentiment balancing steps taken during data selection.</s><s coords="4,306.94,580.84,218.61,9.46;4,307.28,594.39,168.61,9.46">This is expected given the disparate frequencies of emotions in natural human expression.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1" coords="4,307.28,617.08,128.86,9.81">Interrater Correlation</head><p><s coords="4,306.76,635.39,218.78,9.46;4,307.28,648.94,213.78,9.46;4,521.06,646.90,3.99,6.91">We estimate rater agreement for each emotion via interrater correlation <ref type="bibr" coords="4,400.25,648.94,116.10,9.46" target="#b13">(Delgado and Tibau, 2019)</ref>. 5</s><s coords="4,307.28,662.14,218.27,9.81;4,307.28,675.69,218.27,9.81;5,72.00,382.89,217.97,9.81;5,72.00,396.78,24.24,9.46"> For each rater r ∈ R, we calculate the Spearman correlation between r's judgments and the mean of other raters' judgments, for all examples that r rated.</s><s coords="5,99.62,396.78,190.64,9.46;5,72.00,410.33,82.33,9.46">We then take the average of these rater-level correlation scores.</s><s coords="5,160.15,410.33,130.12,9.46;5,72.00,423.88,219.63,9.46;5,72.00,437.43,211.04,9.46">In Section 4.3, we show that each emotion has significant interrater correlation, after controlling for several potential confounds.</s></p><p><s coords="5,82.91,451.97,207.36,9.64;5,72.00,465.52,220.08,9.64;5,72.00,479.07,178.22,9.64">Figure <ref type="figure" coords="5,115.22,452.16,5.56,9.46" target="#fig_0">1</ref> shows that gratitude, admiration and amusement have the highest and grief and nervousness have the lowest interrater correlation.</s><s coords="5,253.44,479.26,36.83,9.46;5,72.00,492.81,218.27,9.46;5,72.00,506.35,121.25,9.46">Emotion frequency correlates with interrater agreement but the two are not equivalent.</s><s coords="5,199.45,506.35,90.82,9.46;5,72.00,519.90,219.63,9.46;5,72.00,533.27,220.08,9.64;5,72.00,546.82,217.11,9.64">Infrequent emotions can have relatively high interrater correlation (e.g., fear), and frequent emotions can have have relatively low interrater correlation (e.g., annoyance).</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2" coords="5,72.00,573.04,162.52,9.81">Correlation Among Emotions</head><p><s coords="5,71.66,593.27,220.41,9.46;5,72.00,606.82,200.40,9.46">To better understand the relationship between emotions in our data, we look at their correlations.</s><s coords="5,275.78,606.82,14.48,9.46;5,72.00,620.02,199.73,9.81">Let N be the number of examples in our dataset.</s><s coords="5,275.70,620.37,14.57,9.46;5,72.00,633.57,218.26,9.81;5,72.00,647.47,218.27,9.46;5,72.00,661.02,117.56,9.46">We obtain N dimensional vectors for each emotion by averaging raters' judgments for all examples labeled with that emotion.</s><s coords="5,194.46,661.02,95.82,9.46;5,72.00,674.57,220.18,9.46">We calculate Pearson correlation values between each pair of emotions.</s><s coords="5,71.66,688.12,218.61,9.46;5,72.00,701.67,119.55,9.46">The heatmap in Figure <ref type="figure" coords="5,173.79,688.12,5.48,9.46" target="#fig_1">2</ref> shows that emotions that are related in intensity (e.g.</s><s coords="5,194.93,701.48,96.70,9.64;5,72.00,715.03,218.27,9.64;5,72.00,728.77,129.02,9.46">annoyance and anger, joy and excitement, nervousness and fear) have a strong positive correlation.</s><s coords="5,206.68,728.77,84.95,9.46;5,72.00,742.32,220.08,9.46;5,72.00,755.86,78.33,9.46">On the other hand, emotions that have the opposite sentiment are negatively correlated.</s><s coords="5,318.19,436.74,209.17,9.46;5,307.28,450.29,197.68,9.46">We also perform hierarchical clustering to uncover the nested structure of our taxonomy.</s><s coords="5,510.97,450.29,14.58,9.46;5,307.28,463.83,218.27,9.46;5,307.28,477.38,220.17,9.46">We use correlation as a distance metric and ward as a linkage method, applied to the averaged ratings.</s><s coords="5,306.94,490.93,218.61,9.46;5,307.28,504.48,219.63,9.46;5,307.28,518.03,218.27,9.46;5,307.28,531.58,46.65,9.46">The dendrogram on the top of Figure <ref type="figure" coords="5,471.31,490.93,5.45,9.46" target="#fig_1">2</ref> shows that emotions that are related by intensity are neighbors, and that larger clusters map closely onto sentiment categories.</s><s coords="5,357.33,531.58,168.21,9.46;5,307.28,545.13,196.01,9.46">Interestingly, emotions that we labeled as "ambiguous" in terms of sentiment (e.g.</s><s coords="5,509.65,544.94,17.70,9.39;5,307.28,558.49,218.26,9.64;5,307.28,572.23,38.60,9.46">surprise) are closer to the positive than to the negative category.</s><s coords="5,349.20,572.23,176.35,9.46;5,307.28,585.78,218.27,9.46;5,307.28,599.33,218.60,9.46">This suggests that in our data, ambiguous emotions are more likely to occur in the context of positive sentiment than that of negative sentiment.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3" coords="5,307.28,626.55,215.95,9.81">Principal Preserved Component Analysis</head><p><s coords="5,306.94,647.47,218.60,9.46;5,307.28,661.02,218.65,9.46;5,307.28,674.57,218.99,9.46;5,306.92,688.12,149.86,9.46">To better understand agreement among raters and the latent structure of the emotion space, we apply Principal Preserved Component Analysis (PPCA) <ref type="bibr" coords="5,306.92,688.12,96.58,9.46" target="#b11">(Cowen et al., 2019b)</ref> to our data.</s><s coords="5,460.45,688.12,65.09,9.46;5,307.28,701.67,218.27,9.46;5,307.28,715.22,218.27,9.46;5,307.28,728.77,220.08,9.46;5,307.28,742.32,188.89,9.46">PPCA extracts linear combinations of attributes (here, emotion judgments), that maximally covary across two sets of data that measure the same attributes (here, randomly split judgments for each example).</s><s coords="5,501.87,742.32,25.04,9.46;5,307.28,755.86,218.27,9.46;6,101.45,149.65,28.31,10.18;6,129.77,147.70,40.80,6.99;6,174.09,149.65,117.99,9.81;6,101.46,163.20,84.00,9.81;6,78.12,178.36,6.98,7.77">Thus, PPCA allows us to uncover latent dimensions of J ∈ R n×|R|×|E| ← all ratings for the examples annotated by r 7:</s></p><formula xml:id="formula_0" coords="6,78.12,174.80,213.52,51.99">J −r ∈ R n×|R|−1×|E| ← all ratings in J, excluding r 8: J r ∈ R n×|E| ← all ratings by r 9:</formula><p><s coords="6,101.46,217.40,44.16,10.18;6,145.62,215.44,23.08,6.99;6,172.26,217.40,87.01,9.81;6,260.32,215.44,10.41,6.99;6,274.20,217.74,16.07,9.46;6,101.46,231.29,179.56,9.46;6,73.63,246.11,11.46,7.77">X, Y ∈ R n×|E| ← randomly split J −r and average ratings across raters for both sets 10:</s></p><formula xml:id="formula_1" coords="6,73.63,242.54,211.28,24.89">W ∈ R |E|×|E| ← result of P P CA(X, Y ) 11:</formula><p><s coords="6,101.45,257.97,82.75,9.88;6,184.20,256.09,3.76,6.99;6,191.19,258.01,9.06,9.60;6,200.25,257.97,84.85,11.29;6,73.63,274.36,11.46,7.77">for all components † w i∈{1,...,|E|} in W do 12:</s></p><formula xml:id="formula_2" coords="6,73.63,270.79,218.00,65.54">v r i ← projection ‡ of J r onto w i 13: v −r i ← projection ‡ of J −r onto w i 14: C r,i ← correlation between v r i and v −r i , partialing out v −r k ∀k ∈ {1, ..., i − 1} 15:</formula><p><s coords="6,101.46,326.87,33.36,9.81;6,73.63,340.41,50.27,9.81;6,73.63,354.04,182.90,9.81;6,73.63,367.59,217.91,9.81;6,72.00,381.37,3.76,6.99;6,76.26,383.67,147.13,9.46;6,72.00,394.92,3.76,6.99;6,76.26,397.22,161.00,9.46;6,72.00,433.93,207.75,9.46">end for 16: end for 17: C ← Wilcoxon signed rank test on C 18: C ← Bonferroni correction on C (α = 0.05) † in descending order of eigenvalue ‡ we demean vectors before projection emotion that have high agreement across raters.</s></p><p><s coords="6,82.91,448.02,208.73,9.46;6,72.00,461.57,218.27,9.46;6,72.00,475.12,218.54,9.46;6,71.61,488.67,105.07,9.46">Unlike Principal Component Analysis (PCA), PPCA examines the cross-covariance between datasets rather than the variancecovariance matrix within a single dataset.</s><s coords="6,183.30,488.67,106.97,9.46;6,72.00,502.22,220.08,9.46;6,72.00,515.42,79.00,10.18;6,151.00,513.46,25.51,6.99;6,177.02,515.42,113.44,9.81;6,72.00,528.96,219.63,9.81;6,72.00,542.68,218.27,9.64;6,72.00,556.06,114.93,9.81;6,187.79,554.11,4.93,6.99;6,194.39,556.06,28.42,9.57;6,225.24,554.11,4.93,6.99;6,231.85,556.06,12.62,9.57">We obtain the principal preserved components (PPCs) of two datasets (matrices) X, Y ∈ R N ×|E| , where N is the number of examples and |E| is the number of emotions, by calculating the eigenvectors of the symmetrized cross covariance matrix X T Y + Y T X.</s></p><p><s coords="6,72.00,578.76,157.89,9.81">Extracting significant dimensions.</s><s coords="6,240.79,579.19,49.47,9.46;6,72.00,592.74,220.07,9.46;6,72.00,606.29,218.27,9.46;6,72.00,619.83,59.51,9.46">We remove examples labeled as Neutral, and keep those examples that still have at least 3 ratings after this filtering step.</s><s coords="6,137.38,619.83,152.89,9.46;6,72.00,633.38,218.27,9.46;6,72.00,646.93,166.95,9.46">We then determine the number of significant dimensions using a leave-one-rater out analysis, as described by Algorithm 1.</s></p><p><s coords="6,82.91,661.02,209.17,9.46;6,72.00,674.57,21.32,9.46">We find that all 27 PPCs are highly significant.</s><s coords="6,97.93,674.57,192.35,9.46;6,72.00,688.12,218.27,9.46;6,72.00,701.32,218.27,9.81;6,72.00,715.22,83.67,9.46">Specifically, Bonferroni-corrected p-values are less than 1.5e-6 for all dimensions (corrected α = 0.0017), suggesting that the emotions were highly dissociable.</s><s coords="6,159.84,715.22,132.24,9.46;6,72.00,728.77,160.28,9.46">Such a high degree of significance for all dimensions is nontrivial.</s><s coords="6,235.62,728.77,56.01,9.46;6,72.00,742.32,218.45,9.46;6,72.00,755.86,220.11,9.46">For example, <ref type="bibr" coords="6,72.00,742.32,92.23,9.46" target="#b11">Cowen et al. (2019b)</ref> find that only 12 out of their 30 emotion categories are significantly dissociable.</s></p><p><s coords="6,307.28,66.24,81.60,9.81">t-SNE projection.</s><s coords="6,399.79,66.67,125.75,9.46;6,307.28,80.22,218.27,9.46;6,307.28,93.76,218.27,9.46;6,307.28,107.31,219.63,9.46;6,307.28,120.86,219.63,9.46;6,307.28,134.41,28.74,9.46">To better understand how the examples are organized in the emotion space, we apply t-SNE, a dimension reduction method that seeks to preserve distances between data points, using the scikit-learn package <ref type="bibr" coords="6,447.84,120.86,79.07,9.46;6,307.28,134.41,23.95,9.46" target="#b29">(Pedregosa et al., 2011)</ref>.</s><s coords="6,341.97,134.41,185.38,9.46;6,307.28,147.96,46.62,9.46;6,353.90,145.92,3.99,6.91;6,358.38,147.96,167.16,9.46;6,307.28,161.51,85.58,9.46">The dataset can be explored in our interactive plot 6 , where one can also look at the texts and the annotations.</s><s coords="6,396.19,161.51,129.35,9.46;6,307.28,175.06,220.08,9.46;6,307.28,188.61,218.27,9.46;6,307.28,202.16,37.87,9.46">The color of each data point is the weighted average of the RGB values representing those emotions that at least half of the raters selected.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4" coords="6,307.28,225.26,181.61,9.81">Linguistic Correlates of Emotions</head><p><s coords="6,306.76,243.82,219.16,9.46;6,307.28,257.37,218.27,9.46;6,307.28,270.91,218.27,9.46;6,307.28,284.46,220.17,9.46">We extract the lexical correlates of each emotion by calculating the log odds ratio, informative Dirichlet prior <ref type="bibr" coords="6,331.96,270.91,95.66,9.46" target="#b27">(Monroe et al., 2008)</ref> of all tokens for each emotion category contrasting to all other emotions.</s><s coords="6,307.28,297.66,218.45,9.81;6,307.28,311.56,220.08,9.46;6,307.28,325.11,168.04,9.46">Since the log odds are z-scored, all values greater than 3 indicate highly significant (&gt;3 std) association with the corresponding emotion.</s><s coords="6,478.70,325.11,46.84,9.46;6,307.28,338.66,180.81,9.46">We list the top 5 tokens for each category in Table <ref type="table" coords="6,479.88,338.66,4.10,9.46">3</ref>.</s><s coords="6,491.47,338.66,34.07,9.46;6,307.28,352.21,220.08,9.46;6,307.28,365.76,148.94,9.46">We find that those emotions that are highly significantly associated with certain tokens (e.g.</s><s coords="6,461.69,365.57,63.85,9.64;6,305.83,379.12,219.72,9.64;6,307.28,392.86,194.19,9.46">gratitude with "thanks", amusement with "lol") tend to have the highest interrater correlation (see Figure <ref type="figure" coords="6,489.42,392.86,4.02,9.46" target="#fig_0">1</ref>).</s><s coords="6,505.10,392.86,22.25,9.46;6,307.00,406.41,220.35,9.46;6,307.28,419.96,90.66,9.46">Conversely, emotions that have fewer significantly associated tokens (e.g.</s><s coords="6,401.38,419.77,124.17,9.64;6,307.28,433.51,152.45,9.46">grief and nervousness) tend to have low interrater correlation.</s><s coords="6,466.17,433.51,59.38,9.46;6,307.28,447.05,218.27,9.46;6,307.28,460.60,207.67,9.46">These results suggest certain emotions are more verbally implicit and may require more context to be interpreted.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5" coords="6,307.28,484.36,66.42,10.75">Modeling</head><p><s coords="6,306.76,507.24,218.78,9.46;6,307.28,520.79,103.33,9.46">We present a strong baseline emotion prediction model for GoEmotions.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1" coords="6,307.28,543.89,105.85,9.81">Data Preparation</head><p><s coords="6,306.94,562.45,218.61,9.46;6,307.28,576.00,220.18,9.46">To minimize the noise in our data, we filter out emotion labels selected by only a single annotator.</s></p><p><s coords="6,306.76,589.55,218.78,9.46;6,307.28,603.10,218.27,9.46;6,307.28,616.65,56.36,9.46">We keep examples with at least one label after this filtering is performed -this amounts to 93% of the original data.</s><s coords="6,366.98,616.65,158.56,9.46;6,306.92,630.20,174.95,9.46">We randomly split this data into train (80%), dev (10%) and test (10%) sets.</s><s coords="6,487.95,630.20,37.98,9.46;6,307.28,643.74,220.17,9.46">We only evaluate on the test set once the model is finalized.</s></p><p><s coords="6,318.19,657.62,207.36,9.46;6,307.28,671.17,220.08,9.46;6,307.28,684.72,119.09,9.46">Even though we filter our data for the baseline experiments, we see particular value in the 4K examples that lack agreement.</s><s coords="6,429.70,684.72,95.84,9.46;6,307.28,698.27,220.08,9.46;6,307.28,711.82,218.27,9.46;6,307.28,725.37,184.65,9.46">This subset of the data likely contains edge/difficult examples for the emotion domain (e.g., emotion-ambiguous text), and present challenges for further exploration.</s><s coords="6,495.30,725.37,30.24,9.46;7,146.44,73.02,8.52,5.91;7,180.35,73.02,27.84,5.91;7,229.75,73.02,23.30,5.91;7,274.38,73.02,25.19,5.91;7,321.29,73.02,103.12,5.91;7,440.84,73.02,21.79,5.91;7,479.51,73.02,37.69,5.91;7,79.40,81.49,78.40,5.91;7,183.37,81.49,21.79,5.91;7,226.75,81.49,29.29,5.91;7,274.76,81.49,24.43,5.91;7,325.61,81.49,29.75,5.91;7,375.34,81.49,90.07,5.91;7,485.97,81.49,24.78,5.91;7,80.48,89.96,78.78,5.91;7,180.60,89.96,27.35,5.91;7,227.10,89.96,76.83,5.91;7,323.53,89.96,33.91,5.91;7,389.55,89.96,19.51,5.91;7,435.54,89.96,32.39,5.91;7,486.14,89.96,24.43,5.91;7,85.22,98.44,26.71,5.91;7,131.47,98.44,26.71,5.91;7,183.19,98.44,22.16,5.91;7,231.07,98.44,20.65,5.91;7,272.68,98.44,28.60,5.91;7,329.03,98.44,22.93,5.91;7,373.44,98.44,51.73,5.91;7,440.28,98.44,22.92,5.91;7,485.19,98.44,26.33,5.91;7,79.91,106.91,83.39,5.91;7,178.64,106.91,31.25,5.91;7,230.32,106.91,22.16,5.91;7,274.57,106.91,24.81,5.91;7,327.89,106.91,25.20,5.91;7,376.10,106.91,142.06,5.91;7,89.92,115.36,17.31,6.13;7,129.11,115.36,31.44,6.13;7,180.82,115.36,26.90,6.13;7,236.85,115.36,9.09,6.13;7,276.74,115.36,20.47,6.13;7,317.76,115.36,45.46,6.13;7,393.43,115.36,11.74,6.13;7,444.92,115.36,13.64,6.13;7,485.66,115.36,25.39,6.13;7,85.59,124.10,25.95,5.91;7,128.49,124.10,32.67,5.91;7,178.83,124.10,30.88,5.91;7,226.56,124.10,138.65,5.91;7,384.06,124.10,30.49,5.91;7,441.04,124.10,21.40,5.91;7,481.78,124.10,33.15,5.91;7,87.15,132.57,22.85,5.91;7,131.69,132.57,26.26,5.91;7,180.15,132.57,28.23,5.91;7,228.99,132.57,24.81,5.91;7,272.73,132.57,28.50,5.91;7,325.24,132.57,30.50,5.91;7,384.81,132.57,28.98,5.91;7,443.12,132.57,17.24,5.91;7,485.19,132.57,26.33,5.91;7,83.93,141.04,29.29,5.91;7,133.78,141.04,22.09,5.91;7,183.75,141.04,21.02,5.91;7,227.32,141.04,28.15,5.91;7,272.90,141.04,28.16,5.91;7,321.54,141.04,37.90,5.91;7,385.57,141.04,27.46,5.91;7,485.97,141.04,24.78,5.91;7,86.16,149.52,24.81,5.91;7,133.49,149.52,22.68,5.91;7,182.61,149.52,23.30,5.91;7,224.10,149.52,141.87,5.91;7,382.92,149.52,32.77,5.91;7,486.04,149.52,24.64,5.91;7,80.29,157.99,130.56,5.91;7,230.32,157.99,22.16,5.91;7,274.38,157.99,25.19,5.91;7,318.61,157.99,43.75,5.91;7,379.88,157.99,38.83,5.91;7,487.47,157.99,21.79,5.91;7,92.77,166.44,11.61,6.13;7,131.19,166.44,27.28,6.13;7,186.50,166.44,15.54,6.13;7,233.88,166.44,15.03,6.13;7,269.42,166.44,35.11,6.13;7,328.62,166.44,23.74,6.13;7,388.18,166.44,21.98,6.13;7,436.39,166.44,30.95,6.13;7,486.46,166.44,23.81,6.13;7,86.27,175.18,24.61,5.91;7,131.66,175.18,26.33,5.91;7,179.78,175.18,28.98,5.91;7,230.70,175.18,21.40,5.91;7,271.42,175.18,31.12,5.91;7,326.94,175.18,27.09,5.91;7,388.09,175.18,22.16,5.91;7,436.24,175.18,31.25,5.91;7,485.31,175.18,26.09,5.91;7,84.56,183.65,79.88,5.91;7,182.43,183.65,23.68,5.91;7,225.72,183.65,76.66,5.91;7,327.75,183.65,25.46,5.91;7,385.44,183.65,27.47,5.91;7,434.53,183.65,82.86,5.91;7,81.60,192.12,33.94,5.91;7,132.42,192.12,169.43,5.91;7,322.58,192.12,35.80,5.91;7,385.63,192.12,27.09,5.91;7,436.43,192.12,78.66,5.91;7,84.94,200.60,27.26,5.91;7,128.82,200.60,32.02,5.91;7,190.29,200.60,7.95,5.91;7,229.37,200.60,24.05,5.91;7,271.54,200.60,30.88,5.91;7,322.40,200.60,36.18,5.91;7,382.98,200.60,32.39,5.91;7,432.64,200.60,83.40,5.91;7,88.66,209.07,19.82,5.91;7,134.69,209.07,20.27,5.91;7,269.68,209.07,34.60,5.91;7,329.40,209.07,22.17,5.91;7,385.63,209.07,27.09,5.91;7,436.61,209.07,30.50,5.91;7,485.76,209.07,25.20,5.91">That is  <ref type="formula" coords="7,146.44,73.02,5.68,5.91">66</ref>) agree ( <ref type="formula" coords="7,199.66,73.02,5.68,5.91">24</ref>) you ( <ref type="formula" coords="7,244.52,73.02,5.68,5.91">12</ref>) fuck ( <ref type="formula" coords="7,291.05,73.02,5.68,5.91">24</ref>) annoying ( <ref type="formula" coords="7,351.16,73.02,5.68,5.91">14</ref>) disappointing ( <ref type="formula" coords="7,415.88,73.02,5.68,5.91">11</ref>) not ( <ref type="formula" coords="7,454.11,73.02,5.68,5.91">16</ref>) confused ( <ref type="formula" coords="7,508.68,73.02,5.68,5.91">18</ref>) awesome ( <ref type="formula" coords="7,109.22,81.49,5.68,5.91">32</ref>) haha ( <ref type="formula" coords="7,149.28,81.49,5.68,5.91">32</ref>) not ( <ref type="formula" coords="7,196.64,81.49,5.68,5.91">13</ref>) worry ( <ref type="formula" coords="7,247.51,81.49,5.68,5.91">11</ref>) hate ( <ref type="formula" coords="7,290.67,81.49,5.68,5.91">18</ref>) stupid ( <ref type="formula" coords="7,346.84,81.49,5.68,5.91">13</ref>) disappointed (10) don't ( <ref type="formula" coords="7,456.89,81.49,5.68,5.91">14</ref>) why ( <ref type="formula" coords="7,502.23,81.49,5.68,5.91">11</ref>) amazing ( <ref type="formula" coords="7,108.14,89.96,5.68,5.91">30</ref>) funny ( <ref type="formula" coords="7,150.74,89.96,5.68,5.91">27</ref>) don't ( <ref type="formula" coords="7,199.42,89.96,5.68,5.91">12</ref>) careful ( <ref type="formula" coords="7,250.39,89.96,2.65,5.91">9</ref>) fucking ( <ref type="formula" coords="7,295.41,89.96,5.68,5.91">18</ref>) fucking ( <ref type="formula" coords="7,348.92,89.96,5.68,5.91">12</ref>) bad ( <ref type="formula" coords="7,403.76,89.96,2.65,5.91">9</ref>) disagree ( <ref type="formula" coords="7,462.63,89.96,2.65,5.91">9</ref>) sure ( <ref type="formula" coords="7,502.05,89.96,5.68,5.91">10</ref>) good ( <ref type="formula" coords="7,103.40,98.44,5.68,5.91">28</ref>) lmao ( <ref type="formula" coords="7,149.66,98.44,5.68,5.91">21</ref>) yes ( <ref type="formula" coords="7,196.83,98.44,5.68,5.91">12</ref>) stay ( <ref type="formula" coords="7,246.42,98.44,2.65,5.91">9</ref>) angry ( <ref type="formula" coords="7,292.75,98.44,5.68,5.91">11</ref>) shit ( <ref type="formula" coords="7,343.43,98.44,5.68,5.91">10</ref>) disappointment ( <ref type="formula" coords="7,419.86,98.44,2.65,5.91">7</ref>) nope ( <ref type="formula" coords="7,457.89,98.44,2.65,5.91">8</ref>) what ( <ref type="formula" coords="7,503.00,98.44,5.68,5.91">10</ref>) beautiful ( <ref type="formula" coords="7,108.71,106.91,5.68,5.91">23</ref>) hilarious ( <ref type="formula" coords="7,154.78,106.91,5.68,5.91">18</ref>) agreed ( <ref type="formula" coords="7,201.37,106.91,5.68,5.91">11</ref>) your ( <ref type="formula" coords="7,247.18,106.91,2.65,5.91">8</ref>) dare ( <ref type="formula" coords="7,290.86,106.91,5.68,5.91">10</ref>) dumb ( <ref type="formula" coords="7,347.78,106.91,2.65,5.91">9</ref>) unfortunately ( <ref type="formula" coords="7,417.21,106.91,2.65,5.91">7</ref>) doesn't ( <ref type="formula" coords="7,461.24,106.91,2.65,5.91">7</ref>) understand (8) desire excitement gratitude joy disgust embarrassment fear grief curiosity wish ( <ref type="formula" coords="7,103.02,124.10,5.68,5.91">29</ref>) excited ( <ref type="formula" coords="7,152.64,124.10,5.68,5.91">21</ref>) thanks ( <ref type="formula" coords="7,201.18,124.10,5.68,5.91">75</ref>) happy ( <ref type="formula" coords="7,247.71,124.10,5.68,5.91">32</ref>) disgusting ( <ref type="formula" coords="7,299.01,124.10,5.68,5.91">22</ref>) embarrassing ( <ref type="formula" coords="7,356.69,124.10,5.68,5.91">12</ref>) scared ( <ref type="formula" coords="7,406.02,124.10,5.68,5.91">16</ref>) died ( <ref type="formula" coords="7,457.14,124.10,2.65,5.91">6</ref>) curious ( <ref type="formula" coords="7,506.41,124.10,5.68,5.91">22</ref>) want ( <ref type="formula" coords="7,104.69,132.57,2.65,5.91">8</ref>) happy ( <ref type="formula" coords="7,152.65,132.57,2.65,5.91">8</ref>) thank ( <ref type="formula" coords="7,199.85,132.57,5.68,5.91">69</ref>) glad ( <ref type="formula" coords="7,245.28,132.57,5.68,5.91">27</ref>) awful ( <ref type="formula" coords="7,292.70,132.57,5.68,5.91">14</ref>) shame ( <ref type="formula" coords="7,347.21,132.57,5.68,5.91">11</ref>) afraid ( <ref type="formula" coords="7,405.27,132.57,5.68,5.91">16</ref>) rip ( <ref type="formula" coords="7,455.05,132.57,2.65,5.91">4</ref>) what ( <ref type="formula" coords="7,503.00,132.57,5.68,5.91">18</ref>) wanted ( <ref type="formula" coords="7,107.91,141.04,2.65,5.91">6</ref>) cake ( <ref type="formula" coords="7,150.57,141.04,2.65,5.91">8</ref>) for ( <ref type="formula" coords="7,196.25,141.04,5.68,5.91">24</ref>) enjoy ( <ref type="formula" coords="7,246.95,141.04,5.68,5.91">20</ref>) worst ( <ref type="formula" coords="7,292.53,141.04,5.68,5.91">13</ref>) awkward ( <ref type="formula" coords="7,350.91,141.04,5.68,5.91">10</ref>) scary ( <ref type="formula" coords="7,404.51,141.04,5.68,5.91">15</ref>) why ( <ref type="formula" coords="7,502.23,141.04,5.68,5.91">13</ref>) could ( <ref type="formula" coords="7,105.68,149.52,2.65,5.91">6</ref>) wow ( <ref type="formula" coords="7,150.87,149.52,2.65,5.91">8</ref>) you ( <ref type="formula" coords="7,197.39,149.52,5.68,5.91">18</ref>) enjoyed ( <ref type="formula" coords="7,250.17,149.52,5.68,5.91">12</ref>) worse ( <ref type="formula" coords="7,293.10,149.52,5.68,5.91">12</ref>) embarrassment ( <ref type="formula" coords="7,360.66,149.52,2.65,5.91">8</ref>) terrible ( <ref type="formula" coords="7,407.16,149.52,5.68,5.91">12</ref>) how ( <ref type="formula" coords="7,502.16,149.52,5.68,5.91">11</ref>) ambitious ( <ref type="formula" coords="7,111.55,157.99,2.65,5.91">4</ref>) interesting ( <ref type="formula" coords="7,158.76,157.99,2.65,5.91">7</ref>) sharing ( <ref type="formula" coords="7,202.32,157.99,5.68,5.91">17</ref>) fun ( <ref type="formula" coords="7,243.96,157.99,5.68,5.91">12</ref>) weird ( <ref type="formula" coords="7,294.27,157.99,2.65,5.91">9</ref>) embarrassed ( <ref type="formula" coords="7,357.06,157.99,2.65,5.91">7</ref>) terrifying ( <ref type="formula" coords="7,410.19,157.99,5.68,5.91">11</ref>) did (10) love optimism pride relief nervousness remorse sadness realization surprise love ( <ref type="formula" coords="7,102.35,175.18,5.68,5.91">76</ref>) hope ( <ref type="formula" coords="7,149.47,175.18,5.68,5.91">45</ref>) proud ( <ref type="formula" coords="7,200.24,175.18,5.68,5.91">14</ref>) glad ( <ref type="formula" coords="7,246.79,175.18,2.65,5.91">5</ref>) nervous ( <ref type="formula" coords="7,297.23,175.18,2.65,5.91">8</ref>) sorry ( <ref type="formula" coords="7,345.51,175.18,5.68,5.91">39</ref>) sad ( <ref type="formula" coords="7,401.73,175.18,5.68,5.91">31</ref>) realize ( <ref type="formula" coords="7,458.96,175.18,5.68,5.91">14</ref>) wow ( <ref type="formula" coords="7,502.88,175.18,5.68,5.91">23</ref>) loved ( <ref type="formula" coords="7,104.06,183.65,5.68,5.91">21</ref>) hopefully ( <ref type="formula" coords="7,155.91,183.65,5.68,5.91">19</ref>) pride ( <ref type="formula" coords="7,200.80,183.65,2.65,5.91">4</ref>) relieved ( <ref type="formula" coords="7,251.77,183.65,2.65,5.91">4</ref>) worried ( <ref type="formula" coords="7,297.08,183.65,2.65,5.91">8</ref>) regret ( <ref type="formula" coords="7,347.91,183.65,2.65,5.91">9</ref>) sadly ( <ref type="formula" coords="7,404.39,183.65,5.68,5.91">16</ref>) realized ( <ref type="formula" coords="7,460.67,183.65,5.68,5.91">12</ref>) surprised ( <ref type="formula" coords="7,508.87,183.65,5.68,5.91">21</ref>) favorite ( <ref type="formula" coords="7,107.02,192.12,5.68,5.91">13</ref>) luck (18) accomplishment relieving (4) anxiety ( <ref type="formula" coords="7,296.54,192.12,2.65,5.91">6</ref>) apologies ( <ref type="formula" coords="7,353.09,192.12,2.65,5.91">7</ref>) sorry ( <ref type="formula" coords="7,404.19,192.12,5.68,5.91">15</ref>) realised ( <ref type="formula" coords="7,462.00,192.12,2.65,5.91">7</ref>) wonder ( <ref type="formula" coords="7,506.57,192.12,5.68,5.91">15</ref>) loves ( <ref type="formula" coords="7,103.68,200.60,5.68,5.91">12</ref>) hoping ( <ref type="formula" coords="7,152.32,200.60,5.68,5.91">16</ref>) (4) relief ( <ref type="formula" coords="7,248.12,200.60,2.65,5.91">4</ref>) anxious ( <ref type="formula" coords="7,297.11,200.60,2.65,5.91">4</ref>) apologize ( <ref type="formula" coords="7,353.27,200.60,2.65,5.91">6</ref>) painful ( <ref type="formula" coords="7,406.85,200.60,5.68,5.91">10</ref>) realization ( <ref type="formula" coords="7,465.79,200.60,2.65,5.91">6</ref>) shocked ( <ref type="formula" coords="7,507.51,200.60,5.68,5.91">12</ref>) like ( <ref type="formula" coords="7,103.18,209.07,2.65,5.91">9</ref>) will ( <ref type="formula" coords="7,149.66,209.07,2.65,5.91">8</ref>) worrying ( <ref type="formula" coords="7,298.97,209.07,2.65,5.91">4</ref>) guilt ( <ref type="formula" coords="7,346.27,209.07,2.65,5.91">5</ref>) crying ( <ref type="formula" coords="7,407.42,209.07,2.65,5.91">9</ref>) thought ( <ref type="formula" coords="7,461.81,209.07,2.65,5.91">6</ref>) omg ( <ref type="formula" coords="7,502.43,209.07,5.68,5.91">11</ref>)</s></p><p><s coords="7,71.69,229.56,348.36,8.64">Table <ref type="table" coords="7,95.97,229.56,3.88,8.64">3</ref>: Top 5 words associated with each emotion ( positive , negative , ambiguous ).</s><s coords="7,423.11,229.24,102.44,8.96;7,72.00,241.52,371.61,8.64">The rounded z-scored log odds ratios in the parentheses, with the threshold set at 3, indicate significance of association.</s></p><p><s coords="7,71.61,275.37,220.48,9.46;7,72.00,288.92,55.15,9.46">why we release all 58K examples with all annotators' ratings.</s></p><p><s coords="7,72.00,309.51,94.55,9.81">Grouping emotions.</s><s coords="7,177.46,309.94,112.81,9.46;7,72.00,323.49,218.26,9.46;7,72.00,337.04,188.03,9.46">We create a hierarchical grouping of our taxonomy, and evaluate the model performance on each level of the hierarchy.</s><s coords="7,263.41,337.04,28.67,9.46;7,72.00,350.59,219.91,9.46;7,72.00,363.95,218.27,9.64;7,72.00,377.69,220.08,9.46;7,72.00,391.24,218.46,9.46;7,72.00,404.79,218.27,9.46;7,72.00,418.15,220.08,9.64;7,72.00,431.70,219.63,9.64;7,72.00,445.25,218.27,9.64;7,72.00,458.80,220.08,9.64;7,72.00,472.35,218.27,9.64;7,71.64,485.89,114.24,9.64">A sentiment level divides the labels into 4 categoriespositive, negative, ambiguous and Neutral -with the Neutral category intact, and the rest of the mapping as shown in Figure <ref type="figure" coords="7,175.68,391.24,4.01,9.46" target="#fig_1">2</ref>. The Ekman level further divides the taxonomy using the Neutral label and the following 6 groups: anger (maps to: anger, annoyance, disapproval), disgust (maps to: disgust), fear (maps to: fear, nervousness), joy (all positive emotions), sadness (maps to: sadness, disappointment, embarrassment, grief, remorse) and surprise (all ambiguous emotions).</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2" coords="7,72.00,508.15,115.94,9.81">Model Architecture</head><p><s coords="7,71.49,526.11,219.51,9.46;7,72.00,539.66,90.09,9.46">We use the BERT-base model <ref type="bibr" coords="7,203.37,526.11,87.62,9.46" target="#b14">(Devlin et al., 2019)</ref> for our experiments.</s><s coords="7,165.46,539.66,124.99,9.46;7,72.00,553.21,218.27,9.46;7,72.00,566.76,218.27,9.46;7,72.00,580.31,190.73,9.46">We add a dense output layer on top of the pretrained model for the purposes of finetuning, with a sigmoid cross entropy loss function to support multi-label classification.</s><s coords="7,265.98,580.31,24.29,9.46;7,72.00,593.86,220.11,9.46">As an additional baseline, we train a bidirectional LSTM.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3" coords="7,72.00,615.93,113.80,9.81">Parameter Settings</head><p><s coords="7,71.49,633.89,218.78,9.46;7,72.00,647.44,218.27,9.46;7,72.00,660.99,218.27,9.46;7,72.00,674.54,76.17,9.46">When finetuning the pre-trained BERT model, we keep most of the hyperparameters set by <ref type="bibr" coords="7,260.25,647.44,30.02,9.46;7,72.00,660.99,53.50,9.46" target="#b14">Devlin et al. (2019)</ref> intact and only change the batch size and learning rate.</s><s coords="7,151.54,674.54,138.72,9.46;7,71.62,688.09,218.65,9.46;7,72.00,701.64,200.60,9.46">We find that training for at least 4 epochs is necessary for learning the data, but training for more epochs results in overfitting.</s><s coords="7,276.00,701.64,14.27,9.46;7,72.00,715.19,218.27,9.46;7,72.00,728.73,176.01,9.46">We also find that a small batch size of 16 and learning rate of 5e-5 yields the best performance.</s></p><p><s coords="7,82.91,742.32,209.17,9.46;7,72.00,755.86,218.27,9.46;7,307.28,275.37,80.88,9.46">For the biLSTM, we set the hidden layer dimensionality to 256, the learning rate to 0.1, with a decay rate of 0.95.</s><s coords="7,391.54,275.37,116.38,9.46">We apply a dropout of 0.7.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4" coords="7,307.28,298.11,58.48,9.81">Results</head><p><s coords="7,306.94,316.46,218.61,9.46;7,307.28,330.01,218.27,9.46;7,307.28,343.56,95.47,9.46">Table <ref type="table" coords="7,334.77,316.46,5.56,9.46" target="#tab_4">4</ref> summarizes the performance of our best model, BERT, on the test set, which achieves an average F1-score of .</s><s coords="7,402.74,343.56,55.44,9.46"><ref type="bibr" coords="7,402.74,343.56,46.77,9.46">46 (std=.19</ref>).</s><s coords="7,461.79,343.56,65.56,9.46;7,307.28,357.11,218.27,9.46;7,307.28,370.47,218.27,9.64;7,306.92,384.02,83.18,9.64">The model obtains the best performance on emotions with overt lexical markers, such as gratitude (.86), amusement (.8) and love (.78).</s><s coords="7,394.13,384.21,131.41,9.46;7,307.28,397.57,218.27,9.64;7,306.92,411.31,201.42,9.46">The model obtains the lowest F1-score on grief (0), relief (.15) and realization (.21), which are the lowest frequency emotions.</s><s coords="7,511.57,411.31,13.97,9.46;7,307.28,424.86,218.27,9.46;7,307.28,438.41,218.27,9.46;7,307.28,451.77,219.63,9.64;7,307.28,465.32,220.44,9.64;7,307.28,479.05,196.32,9.46">We find that less frequent emotions tend to be confused by the model with more frequent emotions related in sentiment and intensity (e.g., grief with sadness, pride with admiration, nervousness with fear)see Appendix G for a more detailed analysis.</s></p><p><s coords="7,318.19,492.83,209.17,9.46;7,307.28,506.38,146.50,9.46">Table <ref type="table" coords="7,344.27,492.83,5.35,9.46" target="#tab_5">5</ref> and Table <ref type="table" coords="7,396.37,492.83,5.35,9.46" target="#tab_6">6</ref> show results for a sentimentgrouped model (F1-score = .69)</s><s coords="7,457.71,506.38,69.65,9.46;7,307.28,519.93,141.24,9.46">and an Ekmangrouped model (F1-score = .64),</s><s coords="7,451.25,519.93,54.02,9.46">respectively.</s><s coords="7,508.66,519.93,16.88,9.46;7,307.28,533.48,218.27,9.46;7,307.28,547.03,218.27,9.46;7,307.28,560.58,220.08,9.46;7,307.28,574.12,126.45,9.46">The significant performance increase in the transition from full to Ekman-level taxonomy indicates that this grouping mitigates confusion among innergroup lower-level categories.</s></p><p><s coords="7,318.19,587.90,207.73,9.46;7,306.88,601.45,218.66,9.46;7,307.28,615.00,220.08,9.46;7,307.28,628.55,218.27,9.46;7,307.28,642.10,30.00,9.46">The biLSTM model performs significantly worse than BERT, obtaining an average F1-score of .41 for the full taxonomy, .53 for an Ekmangrouped model and .6 for a sentiment-grouped model.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6" coords="7,307.28,665.50,180.25,10.75">Transfer Learning Experiments</head><p><s coords="7,306.76,688.12,220.59,9.46;7,307.28,701.67,218.27,9.46;7,307.28,715.22,197.17,9.46">We conduct transfer learning experiments on existing emotion benchmarks, in order to show our data generalizes across domains and taxonomies.</s><s coords="7,508.25,715.22,17.30,9.46;7,307.28,728.77,218.27,9.46;7,307.28,742.32,218.27,9.46;7,307.28,755.86,162.40,9.46">The goal is to demonstrate that given little labeled data in a target domain, one can utilize GoEmotions as baseline emotion understanding data.</s><s coords="8,307.28,550.30,77.57,9.46">ity and taxonomy.</s><s coords="8,388.23,550.30,137.70,9.46;8,307.28,563.84,218.27,9.46;8,307.28,577.39,130.09,9.46">In the interest of space, we only discuss three of these datasets here, chosen based on their diversity of domains.</s><s coords="8,440.75,577.39,86.16,9.46;8,306.88,590.94,220.47,9.46;8,307.28,604.49,204.21,9.46">In our experiments, we observe similar trends for the additional benchmarks, and all are included in the Appendix H.</s><s coords="8,318.19,619.21,209.17,9.46;8,307.28,632.76,218.27,9.46;8,306.76,646.31,218.78,9.46;8,307.28,659.85,218.27,9.46;8,307.28,673.40,138.88,9.46">The International Survey on Emotion Antecedents and Reactions (ISEAR) <ref type="bibr" coords="8,467.06,632.76,58.49,9.46;8,306.76,646.31,63.97,9.46" target="#b35">(Scherer and Wallbott, 1994</ref>) is a collection of personal reports on emotional events, written by 3000 people from different cultural backgrounds.</s><s coords="8,452.16,673.40,75.19,9.46;8,307.28,686.95,220.08,9.46;8,307.28,700.50,19.82,9.46">The dataset contains 8k sentences, each labeled with a single emotion.</s><s coords="8,330.48,700.32,196.43,9.64;8,307.28,713.87,104.53,9.64">The categories are anger, disgust, fear, guilt, joy, sadness and shame.</s></p><p><s coords="8,318.19,728.77,207.36,9.46;8,307.28,742.32,220.07,9.46;8,307.28,755.86,152.33,9.46">EmoInt <ref type="bibr" coords="8,354.20,728.77,112.97,9.46">(Mohammad et al., 2018)</ref> is part of the SemEval 2018 benchmark, and it contains crowdsourced annotations for 7k tweets.</s><s coords="8,463.14,755.86,62.41,9.46;9,72.00,66.48,218.27,9.64;9,72.00,80.03,20.59,9.39">The labels are intensity annotations for anger, joy, sadness, and fear.</s><s coords="9,95.97,80.22,196.10,9.46;9,72.00,93.76,131.24,9.46">We obtain binary annotations for these emotions by using .5 as the cutoff.</s></p><p><s coords="9,82.91,107.98,207.35,9.46;9,72.00,121.53,218.27,9.46;9,72.00,135.08,162.67,9.46">Emotion-Stimulus <ref type="bibr" coords="9,166.06,107.98,84.94,9.46" target="#b18">(Ghazi et al., 2015)</ref> contains annotations for 2.4k sentences generated based on FrameNet's emotion-directed frames.</s><s coords="9,238.06,135.08,54.02,9.46;9,72.00,148.44,218.27,9.64;9,72.00,161.99,56.97,9.64">Their taxonomy is anger, disgust, fear, joy, sadness, shame and surprise.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2" coords="9,72.00,186.43,117.58,9.81">Experimental Setup</head><p><s coords="9,72.00,205.23,79.45,9.81">Training set size.</s><s coords="9,162.36,205.65,127.91,9.46;9,72.00,219.20,218.27,9.46;9,72.00,232.75,219.17,9.46;9,71.64,246.30,160.66,9.46">We experiment with varying amount of training data from the target domain dataset, including 100, 200, 500, 1000, and 80% (named "max") of dataset examples.</s><s coords="9,235.69,246.30,54.57,9.46;9,71.18,259.85,219.08,9.46;9,72.00,273.40,164.36,9.46">We generate 10 random splits for each train set size, with the remaining examples held as a test set.</s></p><p><s coords="9,82.91,287.61,209.17,9.46;9,72.00,301.16,220.08,9.46;9,72.00,314.71,218.27,9.46;9,72.00,328.26,68.19,9.46">We report the results of the finetuning experiments detailed below for each data size, with confidence intervals based on repeated experiments using the splits.</s></p><p><s coords="9,72.00,351.04,53.49,9.81">Finetuning.</s><s coords="9,136.40,351.47,155.68,9.46;9,72.00,365.02,48.48,9.46">We compare three different finetuning setups.</s><s coords="9,126.66,365.02,163.62,9.46;9,72.00,378.57,147.69,9.46">In the BASELINE setup, we finetune BERT only on the target dataset.</s><s coords="9,224.84,378.57,65.15,9.46;9,72.00,392.12,218.27,9.46;9,72.00,405.67,218.27,9.46;9,72.00,419.22,218.27,9.46;9,72.00,432.76,192.84,9.46">In the FREEZE setup, we first finetune BERT on GoEmotions, then perform transfer learning by replacing the final dense layer, freezing all layers besides the last layer and finetuning on the target dataset.</s><s coords="9,272.97,432.76,17.30,9.46;9,72.27,446.31,218.00,9.46;9,72.00,459.86,177.06,9.46">The NOFREEZE setup is the same as FREEZE, except that we do not freeze the bottom layers.</s><s coords="9,253.00,459.86,37.28,9.46;9,72.00,473.41,220.08,9.46;9,72.00,486.96,165.25,9.46">We hold the batch size at 16, learning rate at 2e-5 and number of epochs at 3 for all experiments.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3" coords="9,72.00,511.22,58.48,9.81">Results</head><p><s coords="9,71.66,530.44,220.41,9.46;9,72.00,543.99,219.63,9.46;9,72.00,557.54,218.27,9.46;9,72.00,571.09,218.27,9.46;9,72.00,584.64,180.88,9.46">The results in Figure <ref type="figure" coords="9,162.77,530.44,5.35,9.46">3</ref> suggest that our dataset generalizes well to different domains and taxonomies, and that using a model using GoEmotions can help in cases when there is limited data from the target domain, or limited resources for labeling.</s></p><p><s coords="9,82.91,598.85,209.17,9.46;9,72.00,612.40,220.07,9.46;9,72.00,625.95,218.45,9.46;9,72.00,639.50,76.50,9.46">Given limited target domain data (100 or 200 examples), both FREEZE and NOFREEZE yield significantly higher performance than the BASELINE, for all three datasets.</s><s coords="9,151.90,639.50,138.37,9.46;9,72.00,653.05,220.08,9.46;9,72.00,666.60,218.00,9.46;9,72.00,680.14,146.07,9.46">Importantly, NOFREEZE results show significantly higher performance for all training set sizes, except for "max", where NOFREEZE and BASELINE perform similarly.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7" coords="9,72.00,705.06,75.07,10.75">Conclusion</head><p><s coords="9,71.49,728.77,220.59,9.46;9,72.00,742.32,218.27,9.46;9,72.00,755.86,88.33,9.46">We present GoEmotions, a large, manually annotated, carefully curated dataset for fine-grained emotion prediction.</s><s coords="9,167.37,755.86,122.90,9.46;9,307.28,66.67,220.08,9.46;9,307.28,80.22,124.32,9.46">We provide a detailed data analysis, demonstrating the reliability of the annotations for the full taxonomy.</s><s coords="9,434.97,80.22,92.39,9.46;9,307.28,93.76,218.27,9.46;9,307.00,107.31,141.84,9.46">We show the generalizability of the data across domains and taxonomies via transfer learning experiments.</s><s coords="9,452.01,107.31,73.53,9.46;9,307.28,120.86,219.63,9.46;9,307.28,134.41,220.07,9.46;9,307.28,147.96,25.04,9.46">We build a strong baseline by fine-tuning a BERT model, however, the results suggest much room for future improvement.</s><s coords="9,336.28,147.96,189.26,9.46;9,307.28,161.51,220.08,9.46;9,307.28,175.06,173.92,9.46">Future work can explore the cross-cultural robustness of emotion ratings, and extend the taxonomy to other languages and domains.</s></p><p><s coords="9,318.19,188.41,207.36,9.88;9,307.28,202.38,218.27,9.46;9,307.28,215.93,40.58,9.46">Data Disclaimer: We are aware that the dataset contains biases and is not representative of global diversity.</s><s coords="9,352.98,215.93,172.57,9.46;9,307.28,229.48,136.78,9.46">We are aware that the dataset contains potentially problematic content.</s><s coords="9,447.43,229.48,78.11,9.46;9,307.28,243.03,218.45,9.46;9,307.28,256.58,218.27,9.46;9,307.28,270.13,218.27,9.46;9,307.28,283.68,218.27,9.46;9,306.88,297.23,199.16,9.46">Potential biases in the data include: Inherent biases in Reddit and user base biases, the offensive/vulgar word lists used for data filtering, inherent or unconscious bias in assessment of offensive identity labels, annotators were all native English speakers from India.</s><s coords="9,511.32,297.23,14.22,9.46;9,307.28,310.78,218.27,9.46;9,307.28,324.33,85.22,9.46">All these likely affect labeling, precision, and recall for a trained model.</s><s coords="9,395.88,324.33,129.66,9.46;9,307.28,337.88,218.27,9.46;9,307.28,351.43,135.35,9.46">The emotion pilot model used for sentiment labeling, was trained on examples reviewed by the research team.</s><s coords="9,446.01,351.43,79.53,9.46;9,307.28,364.97,218.27,9.46;9,307.28,378.52,33.02,9.46">Anyone using this dataset should be aware of these limitations of the dataset.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="12,72.00,65.52,122.55,10.75;12,72.00,87.25,52.12,9.81">A Emotion Definitions admiration</head><p><s coords="12,141.98,87.68,148.47,9.46;12,71.61,101.23,78.90,9.46">Finding something impressive or worthy of respect.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="12,72.00,720.15,217.40,10.75">B Taxonomy Selection &amp; Data Collection</head><p><s coords="12,71.49,742.32,220.60,9.46;12,72.00,755.86,64.94,9.46">We selected our taxonomy through a careful multiround process.</s><s coords="12,140.62,755.86,151.45,9.46;12,307.28,66.67,218.27,9.46;12,307.28,80.22,218.27,9.46;12,307.28,93.76,218.27,9.46;12,307.28,107.31,121.35,9.46">In the first pilot round of data col-lection, we used emotions that were identified to be salient by <ref type="bibr" coords="12,351.78,80.22,113.34,9.46" target="#b9">Cowen and Keltner (2017)</ref>, making sure that our set includes Ekmans emotion categories, as used in previous NLP work.</s><s coords="12,432.03,107.31,93.51,9.46;12,307.28,120.86,218.27,9.46;12,307.28,134.41,220.07,9.46;12,307.28,147.96,23.47,9.46">In this round, we also included an open input box where annotators could suggest emotion(s) that were not among the options.</s><s coords="12,334.11,147.96,193.34,9.46">We annotated 3K examples in the first round.</s><s coords="12,306.76,161.51,218.78,9.46;12,307.28,175.06,132.83,9.46">We updated the taxonomy based on the results of this round (see details below).</s><s coords="12,443.48,175.06,82.06,9.46;12,307.28,188.61,218.27,9.46;12,306.88,202.16,218.66,9.46;12,307.28,215.71,45.65,9.46">In the second pilot round of data collection, we repeated this process with 2k new examples, once again updating the taxonomy.</s></p><p><s coords="12,318.19,234.27,207.36,9.46;12,307.28,247.82,218.27,9.46;12,306.88,261.37,218.66,9.46;12,307.28,274.92,218.45,9.46;12,307.28,288.47,218.27,9.46;12,307.28,302.02,43.99,9.46">While reviewing the results from the pilot rounds, we identified and removed emotions that were scarcely selected by annotators and/or had low interrater agreement due to being very similar to other emotions or being too difficult to detect from text.</s><s coords="12,355.59,301.83,171.32,9.64;12,307.28,315.38,178.16,9.64">These emotions were boredom, doubt, heartbroken, indifference and calmness.</s><s coords="12,489.94,315.57,35.60,9.46;12,307.28,329.11,220.07,9.46;12,307.28,342.66,218.27,9.46;12,307.28,356.21,218.27,9.46;12,307.28,369.76,84.79,9.46">We also identified and added those emotions to our taxonomy that were frequently suggested by raters and/or seemed to be represented in the data upon manual inspection.</s><s coords="12,397.81,369.58,129.10,9.64;12,307.28,383.13,220.08,9.64;12,307.28,396.67,28.69,9.39">These emotions were desire, disappointment, pride, realization, relief and remorse.</s><s coords="12,339.31,396.86,186.61,9.46;12,307.28,410.41,50.22,9.46">In this process, we also refined the category names (e.g.</s><s coords="12,360.89,410.22,164.65,9.64;12,307.28,423.96,195.39,9.46">replacing ecstasy with excitement), to ones that seemed interpretable to annotators.</s><s coords="12,506.05,423.96,19.49,9.46;12,307.28,437.51,218.27,9.46;12,305.74,451.06,45.19,9.46">This is how we arrived at the final set of 27 emotions + Neutral.</s><s coords="12,354.42,451.06,172.94,9.46;12,307.28,464.61,218.27,9.46;12,306.88,478.16,218.66,9.46;12,307.28,491.70,120.90,9.46">Our high interrater agreement in the final data can be partially explained by the fact that we took interpretability into consideration while constructing the taxonomy.</s><s coords="12,431.93,491.70,93.61,9.46;12,307.28,505.25,218.27,9.46;12,307.28,518.80,67.78,9.46">The dataset is we are releasing was labeled in the third round over the final taxonomy.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="12,307.28,558.79,135.95,10.75">C Cohen's Kappa Values</head><p><s coords="12,307.28,593.27,218.27,9.46;12,307.28,606.82,220.07,9.46;12,307.28,620.37,169.37,9.46">In Section 4.1, we measure agreement between raters via Spearman correlation, following considerations by <ref type="bibr" coords="12,358.39,620.37,113.69,9.46" target="#b13">Delgado and Tibau (2019)</ref>.</s><s coords="12,480.02,620.37,46.89,9.46;12,306.88,633.92,220.48,9.46;12,307.28,647.47,218.27,9.46;12,307.28,661.02,220.07,9.46;12,307.28,674.57,200.65,9.46">In Table <ref type="table" coords="12,518.69,620.37,4.11,9.46" target="#tab_7">7</ref>, we report the Cohen's kappa values for comparison, which we obtain by randomly sampling two ratings for each example and calculating the Cohen's kappa between these two sets of ratings.</s><s coords="12,511.30,674.57,14.24,9.46;12,307.28,688.12,219.63,9.46;12,307.28,701.67,112.70,9.46">We find that all Cohen's kappa values are greater than 0, showing rater agreement.</s><s coords="12,423.79,701.67,101.76,9.46;12,307.28,715.22,218.45,9.46;12,307.28,728.42,219.63,9.81;12,307.28,742.32,218.27,9.46;12,307.28,755.86,210.97,9.46">Moreover, the Cohen's kappa values correlate highly with the interrater correlation values (Pearson r = 0.85, p &lt; 0.001), providing corroborative evidence for the significant degree of interrater agreement for each emotion.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="13,72.00,527.60,181.44,10.75">D Sentiment of Reddit Subreddits</head><p><s coords="13,72.00,549.31,218.27,9.46;13,72.00,562.86,174.78,9.46">In Section 3, we describe how we obtain subreddits that are balanced in terms of sentiment.</s><s coords="13,250.20,562.86,40.07,9.46;13,72.00,576.40,220.07,9.46;13,72.00,589.77,219.63,9.64;13,72.00,603.50,196.86,9.46;13,72.00,617.05,219.63,9.46;13,72.00,630.60,48.62,9.46">Here, we note the distribution of sentiments across subreddits before we apply the filtering: neutral (M=28%, STD=11%), positive (M=41%, STD=11%), ative (M=19%, STD=7%), ambiguous (M=35%, STD=8%).</s><s coords="13,124.15,630.60,167.92,9.46;13,72.00,644.15,219.79,9.46;13,72.00,657.70,219.63,9.46;13,72.00,671.25,220.08,9.46;13,72.00,684.80,110.48,9.46">After filtering, the distribution of sentiments across our remaining subreddits became: neutral (M=24%, STD=5%), positive (M=35%, STD=6%), negative (M=27%, STD=4%), ambiguous (M=33%, STD=4%).</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="13,72.00,707.06,179.03,10.75">E BERT's Most Activated Layers</head><p><s coords="13,71.66,728.77,218.60,9.46;13,72.00,742.32,218.45,9.46;13,72.00,755.86,218.27,9.46;13,307.28,66.67,218.27,9.46;13,306.88,80.22,121.90,9.46">To better understand whether there are any layers in BERT that are particularly important for our task, we freeze BERT and calculate the center of gravity <ref type="bibr" coords="13,339.53,66.67,86.99,9.46" target="#b38">(Tenney et al., 2019)</ref> based on scalar mixing weights <ref type="bibr" coords="13,342.78,80.22,81.40,9.46" target="#b30">(Peters et al., 2018)</ref>.</s><s coords="13,432.14,80.22,93.41,9.46;13,307.28,93.76,218.26,9.46;13,307.28,107.31,133.72,9.46">We find that all layers are similarly important for our task, with center of gravity = 6.19 (see Figure <ref type="figure" coords="13,428.94,107.31,4.02,9.46" target="#fig_4">4</ref>).</s><s coords="13,446.77,107.31,78.78,9.46;13,306.88,120.86,218.66,9.46;13,307.28,134.41,218.26,9.46;13,307.28,147.96,101.45,9.46">This is consistent with <ref type="bibr" coords="13,328.43,120.86,83.55,9.46" target="#b38">Tenney et al. (2019)</ref>, who have also found that tasks involving high-level semantics tend to make use of all BERT layers.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="13,307.28,373.08,179.08,10.75;13,326.54,387.03,45.17,10.75">F Number of Emotion Labels Per Example</head><p><s coords="13,307.28,409.49,218.45,9.46;13,307.28,423.04,218.27,9.46;13,307.28,436.59,94.54,9.46">Figure <ref type="figure" coords="13,339.18,409.49,5.56,9.46" target="#fig_6">5</ref> shows the number of emotion labels per example before and after we filter for those labels that have agreement.</s><s coords="13,409.16,436.59,116.39,9.46;13,307.28,450.14,182.10,9.46">We use the filtered set of labels for training and testing our models.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="13,307.28,706.31,112.58,10.75">G Confusion Matrix</head><p><s coords="13,307.28,728.77,218.45,9.46;13,307.28,742.32,96.39,9.46">Figure <ref type="figure" coords="13,337.34,728.77,5.35,9.46" target="#fig_7">6</ref> shows the normalized confusion matrix for our model predictions.</s><s coords="13,407.01,742.32,120.35,9.46;13,307.28,755.86,218.54,9.46;14,72.00,66.67,218.27,9.46;14,72.00,80.22,218.27,9.46;14,72.00,93.76,107.17,9.46">Since GoEmotions is a multilabel dataset, we calculate the confusion matrix similarly as we would calculate a co-occurrence matrix: for each true label, we increase the count for each predicted label.</s><s coords="14,182.55,93.76,107.71,9.46;14,72.00,106.96,218.27,10.63;14,72.00,120.51,218.27,9.81;14,72.00,134.06,33.78,9.81">Specifically, we define a matrix M where M i,j denotes the raw confusion count between the true label i and the predicted label j.</s><s coords="14,113.71,134.23,176.56,9.64;14,72.00,147.78,218.27,9.64;14,72.00,161.16,219.63,10.63;14,72.00,174.71,220.18,10.77">For example, if the true labels are joy and admiration, and the predicted labels are joy and pride, then we increase the count for M joy,joy , M joy,pride , M admiration,joy and M admiration,pride .</s></p><p><s coords="14,72.00,188.61,218.27,9.46;14,72.00,202.16,218.54,9.46;14,72.00,215.71,218.27,9.46;14,72.00,229.26,80.00,9.46">In practice, since most of our examples only has a single label (see Figure <ref type="figure" coords="14,184.13,202.16,3.95,9.46" target="#fig_6">5</ref>), our confusion matrix is very similar to one calculated for a single-label classification task.</s></p><p><s coords="14,82.91,243.67,209.17,9.46;14,72.00,256.87,218.27,9.81;14,72.00,270.77,220.07,9.46;14,72.00,284.31,147.16,9.46">Given the disparate frequencies among the labels, we normalize M by dividing the counts in each row (representing counts for each true emotion label) by the sum of that row.</s><s coords="14,222.55,284.31,67.72,9.46;14,72.00,297.86,180.33,9.46">The heatmap in Figure <ref type="figure" coords="14,103.74,297.86,5.56,9.46" target="#fig_7">6</ref> shows these normalized counts.</s><s coords="14,255.70,297.86,34.57,9.46;14,72.00,311.41,218.27,9.46;14,72.00,324.78,218.27,9.64;14,72.00,338.33,218.27,9.64;14,72.00,351.87,24.19,9.39">We find that the model tends to confuse emotions that are related in sentiment and intensity (e.g., grief and sadness, pride and admiration, nervousness and fear).</s></p><p><s coords="14,82.91,366.47,207.36,9.46;14,72.00,380.02,218.27,9.46;14,72.00,393.57,200.69,9.46">We also perform hierarchical clustering over the normalized confusion matrix using correlation as a distance metric and ward as a linkage method.</s><s coords="14,276.08,393.57,14.19,9.46;14,72.00,407.12,218.27,9.46;14,72.00,420.67,218.27,9.46;14,72.00,434.22,218.26,9.46;14,72.00,447.77,111.48,9.46">We find that the model learns relatively similar clusters as the ones in Figure <ref type="figure" coords="14,169.15,420.67,4.17,9.46" target="#fig_1">2</ref>, even though the training data only includes a subset of the labels that have agreement (see Figure <ref type="figure" coords="14,171.67,447.77,3.94,9.46" target="#fig_6">5</ref>).</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="14,72.00,473.37,155.68,10.75">H Transfer Learning Results</head><p><s coords="14,72.00,497.57,218.27,9.46;14,72.00,511.12,218.27,9.46;14,72.00,524.67,220.18,9.46">Figure <ref type="figure" coords="14,104.59,497.57,5.56,9.46" target="#fig_8">7</ref> shows the results for all 9 datasets that are downloadable and have categorical emotions in the Unified Dataset <ref type="bibr" coords="14,170.12,524.67,117.37,9.46" target="#b2">(Bostan and Klinger, 2018)</ref>.</s><s coords="14,71.66,538.22,219.96,9.46;14,72.00,551.76,218.27,9.46;14,71.66,565.31,220.42,9.46;14,72.00,578.86,218.27,9.46;14,71.64,592.41,218.63,9.46;14,71.49,605.96,219.52,9.46;14,71.64,619.51,219.99,9.46;14,72.00,633.06,218.27,9.46;14,71.64,646.61,126.08,9.46">These datasets are DailyDialog <ref type="bibr" coords="14,216.70,538.22,70.14,9.46" target="#b20">(Li et al., 2017)</ref>, Emotion-Stimulus <ref type="bibr" coords="14,156.55,551.76,84.65,9.46" target="#b18">(Ghazi et al., 2015)</ref>, Affective Text <ref type="bibr" coords="14,95.43,565.31,153.45,9.46" target="#b37">(Strapparava and Mihalcea, 2007)</ref>, <ref type="bibr" coords="14,258.36,565.31,33.73,9.46;14,72.00,578.86,132.26,9.46">Crowd-Flower (CrowdFlower, 2016)</ref>, Electoral Tweets <ref type="bibr" coords="14,71.64,592.41,115.33,9.46" target="#b25">(Mohammad et al., 2015)</ref>, ISEAR <ref type="bibr" coords="14,232.43,592.41,57.84,9.46;14,71.49,605.96,64.55,9.46" target="#b35">(Scherer and Wallbott, 1994)</ref>, the Twitter Emotion Corpus (TEC) <ref type="bibr" coords="14,71.64,619.51,87.87,9.46" target="#b24">(Mohammad, 2012)</ref>, EmoInt <ref type="bibr" coords="14,205.21,619.51,86.41,9.46;14,72.00,633.06,25.96,9.46">(Mohammad et al., 2018)</ref> and the Stance Sentiment Emotion Corpus (SSEC) <ref type="bibr" coords="14,107.71,646.61,85.31,9.46" target="#b36">(Schuff et al., 2017)</ref>.</s></p><p><s coords="14,82.91,661.02,209.17,9.46;14,72.00,674.57,195.84,9.46">We describe the experimental setup in Section 6.2, which we use across all datasets.</s><s coords="14,275.70,674.57,14.58,9.46;14,72.00,688.12,218.27,9.46;14,72.00,701.67,220.07,9.46;14,72.00,715.22,40.66,9.46">We find that transfer learning helps in the case of all datasets, especially when there is limited training data.</s><s coords="14,123.20,715.22,168.88,9.46;14,72.00,728.77,218.27,9.46;14,72.00,742.32,218.27,9.46;14,72.00,755.52,218.27,9.81;14,307.28,66.67,220.08,9.46;14,307.28,80.22,218.27,9.46;14,307.55,93.76,219.90,9.46">Interestingly, in the case of Crowd-Flower, which is known to be noisy <ref type="bibr" coords="14,236.34,728.77,53.93,9.46;14,72.00,742.32,66.37,9.46" target="#b2">(Bostan and Klinger, 2018)</ref> and Electoral Tweets, which is a small dataset of ∼4k labeled examples and a large taxonomy of 36 emotions, FREEZE gives a significant boost of performance over the BASELINE and NOFREEZE for all training set sizes besides "max".</s></p><p><s coords="14,318.19,107.31,207.09,9.46;14,307.28,120.86,218.27,9.46;14,307.28,134.41,220.07,9.46;14,307.28,147.96,62.91,9.46">For the other datasets, we find that FREEZE tends to give a performance boost compared to the other setups only up to a couple of hundred training examples.</s><s coords="14,377.54,147.96,149.36,9.46;14,307.55,161.51,217.99,9.46;14,306.88,175.06,218.67,9.46;14,307.28,188.61,157.96,9.46">For 500-1000 training examples, NOFREEZE tends to outperform the BASELINE, but we can see that these two setups come closer when there is more training data available.</s><s coords="14,468.62,188.61,56.93,9.46;14,307.28,202.16,218.27,9.46;14,307.28,215.71,123.72,9.46">These results suggests that our dataset helps if there is limited data from the target domain.</s></p></div>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,72.00,323.12,219.92,8.64;5,72.00,335.07,219.92,8.64;5,72.00,347.03,216.12,8.64"><head>Figure 1 :</head><label>1</label><figDesc><div><p><s coords="5,72.00,323.12,219.92,8.64;5,72.00,335.07,219.92,8.64;5,72.00,347.03,34.86,8.64">Figure 1: Our emotion categories, ordered by the number of examples where at least one rater uses a particular label.</s><s coords="5,109.95,347.03,178.17,8.64">The color indicates the interrater correlation.</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,307.28,352.02,218.27,8.64;5,307.28,363.98,218.27,8.64;5,307.28,375.93,219.92,8.64;5,307.28,387.72,218.27,8.81;5,307.28,399.84,174.33,8.64"><head>Figure 2 :</head><label>2</label><figDesc><div><p><s coords="5,307.28,352.02,218.27,8.64;5,307.28,363.98,102.07,8.64">Figure 2: The heatmap shows the correlation between ratings for each emotion.</s><s coords="5,414.90,363.98,110.64,8.64;5,307.28,375.93,173.20,8.64">The dendrogram represents the a hierarchical clustering of the ratings.</s><s coords="5,486.20,375.93,41.00,8.64;5,307.28,387.72,218.27,8.81;5,307.28,399.84,174.33,8.64">The sentiment labeling was done a priori and it shows that the clusters closely map onto sentiment groups.</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,71.61,65.59,184.52,9.88;6,78.12,81.90,88.35,9.81;6,78.12,95.45,104.44,9.81;6,78.12,109.00,42.22,10.18;6,120.34,107.05,29.08,6.99;6,78.12,122.47,151.79,9.88;6,78.12,137.71,6.98,7.77;6,101.45,136.10,178.55,9.81;6,78.12,151.26,6.98,7.77"><head>Algorithm 1</head><label>1</label><figDesc><div><p><s coords="6,131.00,66.01,125.13,9.46;6,78.12,81.90,88.35,9.81;6,78.12,95.45,104.44,9.81;6,78.12,109.00,42.22,10.18;6,120.34,107.05,29.08,6.99;6,78.12,122.47,151.79,9.88;6,78.12,137.71,6.98,7.77;6,101.45,136.10,178.55,9.81;6,78.12,151.26,6.98,7.77">Leave-One-Rater-Out PPCA 1: R ← set of raters 2: E ← set of emotions 3: C ∈ R |R|×|E| 4: for all raters r ∈ {1, ..., |R|} do 5: n ← number of examples annotated by r 6:</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="12,72.00,114.35,53.32,9.81;12,141.71,114.78,148.56,9.46;12,72.00,128.33,51.20,9.46;12,72.00,141.45,26.66,9.81;12,115.01,141.88,177.07,9.46;12,72.00,155.43,29.40,9.46;12,72.00,168.55,49.70,9.81;12,138.06,168.98,94.10,9.46;12,72.00,182.10,41.41,9.81;12,128.58,182.52,163.51,9.46;12,72.00,196.07,16.67,9.46;12,72.00,209.20,218.26,9.88;12,72.00,222.75,44.86,9.81;12,133.22,223.17,154.71,9.46;12,72.00,236.30,220.08,9.88;12,72.00,250.27,25.16,9.46;12,72.00,263.39,218.45,9.88;12,71.61,277.37,146.36,9.46;12,72.00,290.49,218.65,9.88;12,72.00,304.47,220.18,9.46;12,72.00,317.59,54.75,9.81;12,143.37,318.02,148.70,9.46;12,72.00,331.57,56.97,9.46;12,72.00,344.69,32.74,9.81;12,120.05,345.11,170.22,9.46;12,72.00,358.66,165.63,9.46;12,72.00,371.79,72.71,9.81;12,161.07,372.21,129.38,9.46;12,72.00,385.76,61.22,9.46;12,72.00,398.89,50.28,9.81;12,138.64,399.31,153.44,9.46;12,72.00,412.86,35.44,9.46;12,72.00,425.98,18.77,9.81;12,107.14,426.41,105.32,9.46;12,72.00,439.53,43.03,9.81;12,131.39,439.96,160.68,9.46;12,72.00,453.51,32.42,9.46;12,72.00,466.63,220.08,9.88;12,72.00,480.61,52.11,9.46;12,72.00,493.73,14.54,9.81;12,102.90,494.16,158.75,9.46;12,72.00,507.28,18.57,9.81;12,108.51,507.70,181.76,9.46;12,72.00,521.25,41.23,9.46;12,72.00,534.38,190.35,9.88;12,72.00,547.93,43.64,9.81;12,134.19,548.35,156.08,9.46;12,72.00,561.90,168.15,9.46;12,72.00,575.02,218.27,9.88;12,72.00,589.00,218.27,9.46;12,71.61,602.55,139.68,9.46;12,72.00,615.67,188.29,9.88;12,72.00,629.22,218.27,9.88;12,72.00,643.20,106.05,9.46;12,72.00,656.32,145.36,9.88;12,72.00,669.87,35.16,9.81;12,123.52,670.30,104.47,9.46;12,72.00,683.42,38.07,9.81;12,126.60,683.84,165.48,9.46;12,72.00,697.39,77.40,9.46"><head></head><label></label><figDesc><div><p><s coords="12,72.00,114.35,53.32,9.81;12,141.71,114.78,148.56,9.46;12,72.00,128.33,51.20,9.46">amusement Finding something funny or being entertained.</s><s coords="12,72.00,141.45,26.66,9.81;12,115.01,141.88,177.07,9.46;12,72.00,155.43,29.40,9.46">anger A strong feeling of displeasure or antagonism.</s><s coords="12,72.00,168.55,49.70,9.81;12,138.06,168.98,94.10,9.46">annoyance Mild anger, irritation.</s><s coords="12,72.00,182.10,41.41,9.81;12,128.58,182.52,163.51,9.46;12,72.00,196.07,16.67,9.46">approval Having or expressing a favorable opinion.</s><s coords="12,72.00,209.20,218.26,9.88">caring Displaying kindness and concern for others.</s><s coords="12,72.00,222.75,44.86,9.81;12,133.22,223.17,154.71,9.46">confusion Lack of understanding, uncertainty.</s><s coords="12,72.00,236.30,220.08,9.88;12,72.00,250.27,25.16,9.46">curiosity A strong desire to know or learn something.</s><s coords="12,72.00,263.39,218.45,9.88;12,71.61,277.37,146.36,9.46">desire A strong feeling of wanting something or wishing for something to happen.</s><s coords="12,72.00,290.49,218.65,9.88;12,72.00,304.47,220.18,9.46">disappointment Sadness or displeasure caused by the nonfulfillment of one's hopes or expectations.</s><s coords="12,72.00,317.59,54.75,9.81;12,143.37,318.02,148.70,9.46;12,72.00,331.57,56.97,9.46">disapproval Having or expressing an unfavorable opinion.</s><s coords="12,72.00,344.69,32.74,9.81;12,120.05,345.11,170.22,9.46;12,72.00,358.66,165.63,9.46">disgust Revulsion or strong disapproval aroused by something unpleasant or offensive.</s><s coords="12,72.00,371.79,72.71,9.81;12,161.07,372.21,129.38,9.46;12,72.00,385.76,61.22,9.46">embarrassment Self-consciousness, shame, or awkwardness.</s><s coords="12,72.00,398.89,50.28,9.81;12,138.64,399.31,153.44,9.46;12,72.00,412.86,35.44,9.46">excitement Feeling of great enthusiasm and eagerness.</s><s coords="12,72.00,425.98,18.77,9.81;12,107.14,426.41,105.32,9.46">fear Being afraid or worried.</s><s coords="12,72.00,439.53,43.03,9.81;12,131.39,439.96,160.68,9.46;12,72.00,453.51,32.42,9.46">gratitude A feeling of thankfulness and appreciation.</s><s coords="12,72.00,466.63,220.08,9.88;12,72.00,480.61,52.11,9.46">grief Intense sorrow, especially caused by someone's death.</s><s coords="12,72.00,493.73,14.54,9.81;12,102.90,494.16,158.75,9.46">joy A feeling of pleasure and happiness.</s><s coords="12,72.00,507.28,18.57,9.81;12,108.51,507.70,181.76,9.46;12,72.00,521.25,41.23,9.46">love A strong positive emotion of regard and affection.</s><s coords="12,72.00,534.38,190.35,9.88">nervousness Apprehension, worry, anxiety.</s><s coords="12,72.00,547.93,43.64,9.81;12,134.19,548.35,156.08,9.46;12,72.00,561.90,168.15,9.46">optimism Hopefulness and confidence about the future or the success of something.</s><s coords="12,72.00,575.02,218.27,9.88;12,72.00,589.00,218.27,9.46;12,71.61,602.55,139.68,9.46">pride Pleasure or satisfaction due to ones own achievements or the achievements of those with whom one is closely associated.</s><s coords="12,72.00,615.67,188.29,9.88">realization Becoming aware of something.</s><s coords="12,72.00,629.22,218.27,9.88;12,72.00,643.20,106.05,9.46">relief Reassurance and relaxation following release from anxiety or distress.</s><s coords="12,72.00,656.32,145.36,9.88">remorse Regret or guilty feeling.</s><s coords="12,72.00,669.87,35.16,9.81;12,123.52,670.30,104.47,9.46">sadness Emotional pain, sorrow.</s><s coords="12,72.00,683.42,38.07,9.81;12,126.60,683.84,165.48,9.46;12,72.00,697.39,77.40,9.46">surprise Feeling astonished, startled by something unexpected.</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="13,307.28,325.06,218.27,8.64;13,307.28,337.02,88.54,8.64"><head>Figure 4 :</head><label>4</label><figDesc><div><p><s coords="13,307.28,325.06,218.27,8.64;13,307.28,337.02,88.54,8.64">Figure 4: Softmax weights of each BERT layer when trained on our dataset.</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="13,307.28,646.49,219.92,8.64;13,307.28,658.45,219.92,8.64;13,307.28,670.40,54.24,8.64"><head>Figure 5 :</head><label>5</label><figDesc><div><p><s coords="13,307.28,646.49,219.92,8.64;13,307.28,658.45,219.92,8.64;13,307.28,670.40,54.24,8.64">Figure 5: Number of emotion labels per example before and after filtering the labels chosen by only a single annotator.</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="15,72.00,388.03,453.55,8.64;15,72.00,399.99,288.07,8.64"><head>Figure 6 :</head><label>6</label><figDesc><div><p><s coords="15,72.00,388.03,282.41,8.64">Figure 6: A normalized confusion matrix for our model predictions.</s><s coords="15,361.02,388.03,164.53,8.64;15,72.00,399.99,288.07,8.64">The plot shows that the model confuses emotions with other emotions that are related in intensity and sentiment.</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8" coords="15,72.00,726.98,453.55,8.64"><head>Figure 7 :</head><label>7</label><figDesc><div><p><s coords="15,72.00,726.98,453.55,8.64">Figure7: Transfer learning results on 9 emotion benchmarks from the Unified Dataset<ref type="bibr" coords="15,414.77,726.98,106.49,8.64" target="#b2">(Bostan and Klinger, 2018)</ref>.</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="1,312.23,209.89,208.36,169.24"><head>Table 1 :</head><label>1</label><figDesc><div><p><s coords="1,356.80,370.49,154.12,8.64">Example annotations from our dataset.</s></p></div></figDesc><table coords="1,312.23,209.89,208.36,144.71"><row><cell>Sample Text</cell><cell>Label(s)</cell></row><row><cell>OMG, yep!!! That is the final ans-</cell><cell>gratitude,</cell></row><row><cell>wer. Thank you so much!</cell><cell>approval</cell></row><row><cell>I'm not even sure what it is, why do people hate it</cell><cell>confusion</cell></row><row><cell>Guilty of doing this tbph</cell><cell>remorse</cell></row><row><cell>This caught me off guard for real.</cell><cell>surprise,</cell></row><row><cell>I'm actually off my bed laughing</cell><cell>amusement</cell></row><row><cell>I tried to send this to a friend but [NAME] knocked it away.</cell><cell>disappointment</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,317.58,64.86,197.66,186.78"><head>Table 2 :</head><label>2</label><figDesc><div><p><s coords="4,356.39,243.01,154.96,8.64">Summary statistics of our labeled data.</s></p></div></figDesc><table coords="4,317.58,64.86,197.66,164.40"><row><cell>Number of examples</cell><cell>58,009</cell></row><row><cell>Number of emotions</cell><cell>27 + neutral</cell></row><row><cell>Number of unique raters</cell><cell>82</cell></row><row><cell>Number of raters / example</cell><cell>3 or 5</cell></row><row><cell>Marked unclear or difficult to label</cell><cell>1.6%</cell></row><row><cell></cell><cell>1: 83%</cell></row><row><cell>Number of labels per example</cell><cell>2: 15% 3: 2%</cell></row><row><cell></cell><cell>4+: .2%</cell></row><row><cell>Number of examples w/ 2+ raters agreeing on at least 1 label</cell><cell>54,263 (94%)</cell></row><row><cell>Number of examples w/ 3+ raters agreeing on at least 1 label</cell><cell>17,763 (31%)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="8,70.98,61.79,454.57,703.53"><head>Table 4 :</head><label>4</label><figDesc><div><p><s coords="8,111.82,179.73,326.39,8.64">Transfer learning results in terms of average F1-scores across emotion categories.</s><s coords="8,441.78,179.73,83.77,8.64;8,72.00,191.69,426.74,8.64">The bars indicate the 95% confidence intervals, which we obtain from 10 different runs on 10 different random splits of the data.</s><s coords="8,116.58,666.76,164.01,8.64">Results based on GoEmotions taxonomy.</s></p></div></figDesc><table coords="8,70.98,61.79,450.94,588.58"><row><cell>F-score</cell><cell>0.0 0.2 0.4 0.6</cell><cell cols="3">100 200 ISEAR (self-reported experiences) 500 1000 Training Set Size</cell><cell>max 0.2 0.4 0.6 0.8 1.0</cell><cell cols="3">100 200 Emotion-Stimulus (FrameNet-based sentences) 500 1000 max Training Set Size 0.2 0.4 0.6 0.8</cell><cell>100 200</cell><cell>500 Training Set Size EmoInt (tweets)</cell><cell>1000</cell><cell>max</cell></row><row><cell></cell><cell></cell><cell cols="2">Baseline</cell><cell cols="4">Transfer Learning w/ Freezing Layers</cell><cell cols="2">Transfer Learning w/o Freezing Layers</cell></row><row><cell cols="3">Figure 3: Emotion</cell><cell cols="5">Precision Recall F1</cell><cell>Sentiment</cell><cell>Precision Recall F1</cell></row><row><cell></cell><cell cols="2">admiration</cell><cell>0.53</cell><cell></cell><cell cols="2">0.83</cell><cell>0.65</cell><cell>ambiguous</cell><cell>0.54</cell><cell>0.66</cell><cell>0.60</cell></row><row><cell></cell><cell cols="2">amusement</cell><cell>0.70</cell><cell></cell><cell cols="2">0.94</cell><cell>0.80</cell><cell>negative</cell><cell>0.65</cell><cell>0.76</cell><cell>0.70</cell></row><row><cell></cell><cell cols="2">anger</cell><cell>0.36</cell><cell></cell><cell cols="2">0.66</cell><cell>0.47</cell><cell>neutral</cell><cell>0.64</cell><cell>0.69</cell><cell>0.67</cell></row><row><cell></cell><cell cols="2">annoyance approval caring</cell><cell>0.24 0.26 0.30</cell><cell></cell><cell cols="2">0.63 0.57 0.56</cell><cell>0.34 0.36 0.39</cell><cell cols="2">positive macro-average std</cell><cell>0.78 0.65 0.09</cell><cell>0.87 0.74 0.10</cell><cell>0.82 0.69 0.09</cell></row><row><cell></cell><cell cols="2">confusion</cell><cell>0.24</cell><cell></cell><cell cols="2">0.76</cell><cell>0.37</cell><cell></cell></row><row><cell></cell><cell cols="2">curiosity</cell><cell>0.40</cell><cell></cell><cell cols="2">0.84</cell><cell>0.54</cell><cell></cell></row><row><cell></cell><cell cols="2">desire</cell><cell>0.43</cell><cell></cell><cell cols="2">0.59</cell><cell>0.49</cell><cell></cell></row><row><cell></cell><cell cols="2">disappointment</cell><cell>0.19</cell><cell></cell><cell cols="2">0.52</cell><cell>0.28</cell><cell></cell></row><row><cell></cell><cell cols="2">disapproval</cell><cell>0.29</cell><cell></cell><cell cols="2">0.61</cell><cell>0.39</cell><cell></cell></row><row><cell></cell><cell cols="2">disgust</cell><cell>0.34</cell><cell></cell><cell cols="2">0.66</cell><cell>0.45</cell><cell></cell></row><row><cell></cell><cell cols="2">embarrassment</cell><cell>0.39</cell><cell></cell><cell cols="2">0.49</cell><cell>0.43</cell><cell></cell></row><row><cell></cell><cell cols="2">excitement</cell><cell>0.26</cell><cell></cell><cell cols="2">0.52</cell><cell>0.34</cell><cell></cell></row><row><cell></cell><cell cols="2">fear</cell><cell>0.46</cell><cell></cell><cell cols="2">0.85</cell><cell>0.60</cell><cell></cell></row><row><cell></cell><cell cols="2">gratitude</cell><cell>0.79</cell><cell></cell><cell cols="2">0.95</cell><cell>0.86</cell><cell></cell></row><row><cell></cell><cell cols="2">grief</cell><cell>0.00</cell><cell></cell><cell cols="2">0.00</cell><cell>0.00</cell><cell></cell></row><row><cell></cell><cell cols="2">joy</cell><cell>0.39</cell><cell></cell><cell cols="2">0.73</cell><cell>0.51</cell><cell></cell></row><row><cell></cell><cell cols="2">love</cell><cell>0.68</cell><cell></cell><cell cols="2">0.92</cell><cell>0.78</cell><cell></cell></row><row><cell></cell><cell cols="2">nervousness</cell><cell>0.28</cell><cell></cell><cell cols="2">0.48</cell><cell>0.35</cell><cell></cell></row><row><cell></cell><cell cols="2">neutral</cell><cell>0.56</cell><cell></cell><cell cols="2">0.84</cell><cell>0.68</cell><cell></cell></row><row><cell></cell><cell cols="2">optimism</cell><cell>0.41</cell><cell></cell><cell cols="2">0.69</cell><cell>0.51</cell><cell></cell></row><row><cell></cell><cell cols="2">pride</cell><cell>0.67</cell><cell></cell><cell cols="2">0.25</cell><cell>0.36</cell><cell></cell></row><row><cell></cell><cell cols="2">realization</cell><cell>0.16</cell><cell></cell><cell cols="2">0.29</cell><cell>0.21</cell><cell></cell></row><row><cell></cell><cell cols="2">relief</cell><cell>0.50</cell><cell></cell><cell cols="2">0.09</cell><cell>0.15</cell><cell></cell></row><row><cell></cell><cell cols="2">remorse</cell><cell>0.53</cell><cell></cell><cell cols="2">0.88</cell><cell>0.66</cell><cell></cell></row><row><cell></cell><cell cols="2">sadness</cell><cell>0.38</cell><cell></cell><cell cols="2">0.71</cell><cell>0.49</cell><cell></cell></row><row><cell></cell><cell cols="2">surprise</cell><cell>0.40</cell><cell></cell><cell cols="2">0.66</cell><cell>0.50</cell><cell></cell></row><row><cell></cell><cell cols="2">macro-average</cell><cell>0.40</cell><cell></cell><cell cols="2">0.63</cell><cell>0.46</cell><cell></cell></row><row><cell></cell><cell cols="2">std</cell><cell>0.18</cell><cell></cell><cell cols="2">0.24</cell><cell>0.19</cell><cell></cell></row></table><note><p><s coords="8,72.00,705.55,163.93,9.81;8,71.49,728.77,218.78,9.46;8,72.00,742.32,219.63,9.46;8,71.61,755.86,220.48,9.46">6.1 Emotion Benchmark DatasetsWe consider the nine benchmark datasets from Bostan and Klinger (2018)'s Unified Dataset, which vary in terms of their size, domain, qual-</s></p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="8,315.77,335.14,200.97,162.73"><head>Table 5 :</head><label>5</label><figDesc><div><p><s coords="8,350.99,335.14,165.75,8.64">Results based on sentiment-grouped data.</s></p></div></figDesc><table coords="8,318.00,365.19,196.82,132.69"><row><cell cols="4">Ekman Emotion Precision Recall F1</cell></row><row><cell>anger</cell><cell>0.50</cell><cell>0.65</cell><cell>0.57</cell></row><row><cell>disgust</cell><cell>0.52</cell><cell>0.53</cell><cell>0.53</cell></row><row><cell>fear</cell><cell>0.61</cell><cell>0.76</cell><cell>0.68</cell></row><row><cell>joy</cell><cell>0.77</cell><cell>0.88</cell><cell>0.82</cell></row><row><cell>neutral</cell><cell>0.66</cell><cell>0.67</cell><cell>0.66</cell></row><row><cell>sadness</cell><cell>0.56</cell><cell>0.62</cell><cell>0.59</cell></row><row><cell>surprise</cell><cell>0.53</cell><cell>0.70</cell><cell>0.61</cell></row><row><cell>macro-average</cell><cell>0.59</cell><cell>0.69</cell><cell>0.64</cell></row><row><cell>std</cell><cell>0.10</cell><cell>0.11</cell><cell>0.10</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="8,331.17,514.11,170.18,8.64"><head>Table 6 :</head><label>6</label><figDesc><div><p><s coords="8,366.38,514.11,134.96,8.64">Results using Ekman's taxonomy.</s></p></div></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="13,71.69,68.76,218.75,436.22"><head>Table 7 :</head><label>7</label><figDesc><div><p><s coords="13,106.21,484.38,184.23,8.64;13,72.00,496.34,121.46,8.64">Interrater agreement, as measured by interrater correlation and Cohen's kappa</s></p></div></figDesc><table coords="13,88.43,68.76,185.40,399.26"><row><cell>Emotion</cell><cell>Interrater Correlation</cell><cell>Cohen's kappa</cell></row><row><cell>admiration</cell><cell>0.535</cell><cell>0.468</cell></row><row><cell>amusement</cell><cell>0.482</cell><cell>0.474</cell></row><row><cell>anger</cell><cell>0.207</cell><cell>0.307</cell></row><row><cell>annoyance</cell><cell>0.193</cell><cell>0.192</cell></row><row><cell>approval</cell><cell>0.385</cell><cell>0.187</cell></row><row><cell>caring</cell><cell>0.237</cell><cell>0.252</cell></row><row><cell>confusion</cell><cell>0.217</cell><cell>0.270</cell></row><row><cell>curiosity</cell><cell>0.418</cell><cell>0.366</cell></row><row><cell>desire</cell><cell>0.177</cell><cell>0.251</cell></row><row><cell>disappointment</cell><cell>0.186</cell><cell>0.184</cell></row><row><cell>disapproval</cell><cell>0.274</cell><cell>0.234</cell></row><row><cell>disgust</cell><cell>0.192</cell><cell>0.241</cell></row><row><cell>embarrassment</cell><cell>0.177</cell><cell>0.218</cell></row><row><cell>excitement</cell><cell>0.193</cell><cell>0.222</cell></row><row><cell>fear</cell><cell>0.266</cell><cell>0.394</cell></row><row><cell>gratitude</cell><cell>0.645</cell><cell>0.749</cell></row><row><cell>grief</cell><cell>0.162</cell><cell>0.095</cell></row><row><cell>joy</cell><cell>0.296</cell><cell>0.301</cell></row><row><cell>love</cell><cell>0.446</cell><cell>0.555</cell></row><row><cell>nervousness</cell><cell>0.164</cell><cell>0.144</cell></row><row><cell>optimism</cell><cell>0.322</cell><cell>0.300</cell></row><row><cell>pride</cell><cell>0.163</cell><cell>0.148</cell></row><row><cell>realization</cell><cell>0.194</cell><cell>0.155</cell></row><row><cell>relief</cell><cell>0.172</cell><cell>0.185</cell></row><row><cell>remorse</cell><cell>0.178</cell><cell>0.358</cell></row><row><cell>sadness</cell><cell>0.346</cell><cell>0.336</cell></row><row><cell>surprise</cell><cell>0.275</cell><cell>0.331</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/dewarim/ reddit-data-tools 3 http://redditlist.com/nsfw</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4"><ref type="bibr" coords="4,88.14,737.20,76.66,7.77" target="#b11">Cowen et al. (2019b)</ref> find that emotion judgments in Indian and US English speakers largely occupy the same dimensions.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">We use correlations as opposed to Cohen's kappa<ref type="bibr" coords="4,498.59,697.35,28.07,7.77;4,306.60,707.32,20.65,7.77" target="#b5">(Cohen, 1960)</ref> because the former is a more interpretable metric and it is also more suitable for measuring agreement among a variable number of raters rating different examples. In Appendix C we report Cohen's kappa values as well, which correlate highly with the values obtained from interrater correlation (Pearson r = 0.85, p &lt; 0.001).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">https://nlp.stanford.edu/ ˜ddemszky/ goemotions/tsne.html</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="9,307.28,401.93,93.53,10.75">Acknowledgments</head><p><s coords="9,306.76,424.55,218.97,9.46;9,307.28,438.10,95.79,9.46">We thank the three anonymous reviewers for their constructive feedback.</s><s coords="9,406.45,438.10,119.37,9.46;9,307.28,451.65,149.55,9.46">We would also like to thank the annotators for their hard work.</s></p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct coords="9,307.28,507.68,220.01,8.64;9,318.19,518.64,207.36,8.64;9,318.19,529.43,207.36,8.81;9,318.19,540.39,207.36,8.58;9,317.86,551.35,208.93,8.58;9,318.19,562.48,62.53,8.64" xml:id="b0">
	<analytic>
		<title level="a" type="main">EmoNet: Fine-Grained Emotion Detection with Gated Recurrent Neural Networks</title>
		<author>
			<persName><forename type="first">Muhammad</forename><surname>Abdul-Mageed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lyle</forename><surname>Ungar</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/p17-1067</idno>
		<ptr type="open-access" target="https://www.aclweb.org/anthology/P17-1067.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</title>
				<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="718" to="728" />
		</imprint>
	</monogr>
	<note type="raw_reference">Muhammad Abdul-Mageed and Lyle Ungar. 2017. EmoNet: Fine-Grained Emotion Detection with Gated Recurrent Neural Networks. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 718-728.</note>
</biblStruct>

<biblStruct coords="9,307.28,583.31,220.01,8.64;9,318.19,594.27,207.36,8.64;9,318.19,605.06,209.01,8.81;9,318.19,616.02,207.36,8.58;9,318.19,626.98,207.36,8.58;9,317.91,637.93,208.88,8.81;9,318.19,649.06,209.01,8.64;9,318.19,660.02,79.15,8.64" xml:id="b1">
	<analytic>
		<title level="a" type="main">Emotions from text: Machine learning for text-based emotion prediction</title>
		<author>
			<persName coords=""><forename type="first">Cecilia</forename><surname>Ovesdotter Alm</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Richard</forename><surname>Sproat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing<address><addrLine>Vancouver, British Columbia, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="579" to="586" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
	<note type="raw_reference">Cecilia Ovesdotter Alm, Dan Roth, and Richard Sproat. 2005. Emotions from text: Machine learning for text-based emotion prediction. In Proceed- ings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 579-586, Vancouver, British Columbia, Canada. Association for Compu- tational Linguistics.</note>
</biblStruct>

<biblStruct coords="9,307.28,680.85,220.01,8.64;9,317.83,691.81,209.37,8.64;9,318.19,702.60,209.01,8.81;9,318.19,713.56,208.60,8.58;9,318.19,724.69,72.50,8.64" xml:id="b2">
	<analytic>
		<title level="a" type="main">An analysis of annotated corpora for emotion classification in text</title>
		<author>
			<persName coords=""><forename type="first">Laura-Ana-Maria</forename><surname>Bostan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Roman</forename><surname>Klinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
				<meeting>the 27th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2104" to="2119" />
		</imprint>
	</monogr>
	<note type="raw_reference">Laura-Ana-Maria Bostan and Roman Klinger. 2018. An analysis of annotated corpora for emotion clas- sification in text. In Proceedings of the 27th Inter- national Conference on Computational Linguistics, pages 2104-2119.</note>
</biblStruct>

<biblStruct coords="9,307.28,745.52,219.92,8.64;9,318.19,756.48,207.36,8.64;10,82.55,67.28,209.37,8.64;10,82.91,78.07,209.02,8.81;10,82.91,89.03,207.36,8.58;10,82.60,99.99,209.32,8.58;10,82.91,110.95,209.01,8.58;10,82.74,121.91,113.45,8.81" xml:id="b3">
	<analytic>
		<title level="a" type="main">Finding Microaggressions in the Wild: A Case for Locating Elusive Phenomena in Social Media Posts</title>
		<author>
			<persName coords=""><forename type="first">Luke</forename><surname>Breitfeller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Emily</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Jurgens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d19-1176</idno>
		<ptr type="open-access" target="https://www.aclweb.org/anthology/D19-1176.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1664" to="1674" />
		</imprint>
	</monogr>
	<note type="raw_reference">Luke Breitfeller, Emily Ahn, David Jurgens, and Yu- lia Tsvetkov. 2019. Finding microaggressions in the wild: A case for locating elusive phenomena in so- cial media posts. In Proceedings of the 2019 Con- ference on Empirical Methods in Natural Language Processing and the 9th International Joint Confer- ence on Natural Language Processing (EMNLP- IJCNLP), pages 1664-1674.</note>
</biblStruct>

<biblStruct coords="10,72.00,143.88,219.92,8.64;10,82.91,154.84,209.01,8.64;10,82.91,165.80,209.10,8.64;10,82.91,176.59,209.01,8.81;10,82.91,187.55,207.36,8.58;10,82.63,198.51,209.13,8.81;10,82.91,209.64,17.43,8.64" xml:id="b4">
	<analytic>
		<title level="a" type="main">EmoBank: Studying the Impact of Annotation Perspective and Representation Format on Dimensional Emotion Analysis</title>
		<author>
			<persName coords=""><forename type="first">Sven</forename><surname>Buechel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Udo</forename><surname>Hahn</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/e17-2092</idno>
		<ptr type="open-access" target="https://www.aclweb.org/anthology/E17-2092.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers</title>
				<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="578" to="585" />
		</imprint>
	</monogr>
	<note type="raw_reference">Sven Buechel and Udo Hahn. 2017. Emobank: Study- ing the impact of annotation perspective and repre- sentation format on dimensional emotion analysis. In Proceedings of the 15th Conference of the Euro- pean Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 578- 585.</note>
</biblStruct>

<biblStruct coords="10,72.00,231.44,218.44,8.64;10,82.91,242.23,209.01,8.81;10,82.91,253.19,92.88,8.81" xml:id="b5">
	<analytic>
		<title level="a" type="main">A Coefficient of Agreement for Nominal Scales</title>
		<author>
			<persName coords=""><forename type="first">Jacob</forename><surname>Cohen</surname></persName>
		</author>
		<idno type="DOI">10.1177/001316446002000104</idno>
		<idno type="ark">ark:/67375/M70-S3SN4S70-L</idno>
		<idno type="istexId">EF1B1E6AC276B6E182A83B9458693DD8D504BA96</idno>
		<ptr type="open-access" target="https://documentserver.uhasselt.be//bitstream/1942/28116/2/Main.pdf" />
	</analytic>
	<monogr>
		<title level="j">Educational and Psychological Measurement</title>
		<title level="j" type="abbrev">Educational and Psychological Measurement</title>
		<idno type="ISSN">0013-1644</idno>
		<idno type="ISSNe">1552-3888</idno>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="37" to="46" />
			<date type="published" when="1960-04">1960</date>
			<publisher>SAGE Publications</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Jacob Cohen. 1960. A coefficient of agreement for nominal scales. Educational and psychological mea- surement, 20(1):37-46.</note>
</biblStruct>

<biblStruct coords="10,72.00,275.17,218.44,8.64;10,82.91,286.13,207.36,8.64;10,82.91,297.09,209.01,8.64;10,82.91,307.88,191.82,8.81;10,82.60,318.84,115.30,8.81" xml:id="b6">
	<analytic>
		<title level="a" type="main">Mapping the Passions: Toward a High-Dimensional Taxonomy of Emotional Experience and Expression</title>
		<author>
			<persName coords=""><forename type="first">Alan</forename><surname>Cowen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Disa</forename><surname>Sauter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jessica</forename><forename type="middle">L</forename><surname>Tracy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dacher</forename><surname>Keltner</surname></persName>
		</author>
		<idno type="DOI">10.1177/1529100619850176</idno>
		<idno type="PMID">31313637</idno>
		<idno type="PMCID">PMC6675572</idno>
		<ptr type="open-access" target="https://journals.sagepub.com/doi/pdf/10.1177/1529100619850176" />
	</analytic>
	<monogr>
		<title level="j">Psychological Science in the Public Interest</title>
		<title level="j" type="abbrev">Psychol Sci Public Interest</title>
		<idno type="ISSN">1529-1006</idno>
		<idno type="ISSNe">1539-6053</idno>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="69" to="90" />
			<date type="published" when="2019-07">2019a</date>
			<publisher>SAGE Publications</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Alan Cowen, Disa Sauter, Jessica L Tracy, and Dacher Keltner. 2019a. Mapping the passions: Toward a high-dimensional taxonomy of emotional experi- ence and expression. Psychological Science in Public Interest, 20(1):69-90.</note>
</biblStruct>

<biblStruct coords="10,72.00,340.81,219.52,8.64;10,82.91,351.77,207.36,8.64;10,82.91,362.56,207.36,8.81;10,82.60,373.52,117.36,8.81" xml:id="b7">
	<analytic>
		<title level="a" type="main">Mapping 24 emotions conveyed by brief human vocalization.</title>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">S</forename><surname>Cowen</surname></persName>
			<idno type="ORCID">0000-0002-8381-5883</idno>
		</author>
		<author>
			<persName><forename type="first">Hillary</forename><forename type="middle">Anger</forename><surname>Elfenbein</surname></persName>
			<idno type="ORCID">0000-0002-3973-1447</idno>
		</author>
		<author>
			<persName><forename type="first">Petri</forename><surname>Laukka</surname></persName>
			<idno type="ORCID">0000-0001-8771-6818</idno>
		</author>
		<author>
			<persName coords=""><forename type="first">Dacher</forename><surname>Keltner</surname></persName>
		</author>
		<idno type="DOI">10.1037/amp0000399</idno>
		<idno type="PMID">30570267</idno>
		<idno type="PMCID">PMC6586540</idno>
		<ptr type="open-access" target="https://europepmc.org/articles/pmc6586540?pdf=render" />
	</analytic>
	<monogr>
		<title level="j">American Psychologist</title>
		<title level="j" type="abbrev">American Psychologist</title>
		<idno type="ISSN">0003-066X</idno>
		<idno type="ISSNe">1935-990X</idno>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="698" to="712" />
			<date type="published" when="2018">2018</date>
			<publisher>American Psychological Association (APA)</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Alan S Cowen, Hillary Anger Elfenbein, Petri Laukka, and Dacher Keltner. 2018. Mapping 24 emotions conveyed by brief human vocalization. American Psychologist, 74(6):698-712.</note>
</biblStruct>

<biblStruct coords="10,72.00,395.50,219.92,8.64;10,82.91,406.46,207.36,8.64;10,82.91,417.42,207.36,8.64;10,82.91,428.21,207.36,8.81;10,82.91,439.16,148.31,8.58" xml:id="b8">
	<analytic>
		<title level="a" type="main">What music makes us feel: At least 13 dimensions organize subjective experiences associated with music across different cultures</title>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">S</forename><surname>Cowen</surname></persName>
			<idno type="ORCID">0000-0002-8381-5883</idno>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Disa</forename><surname>Sauter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dacher</forename><surname>Keltner</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.1910704117</idno>
		<idno type="PMID">31907316</idno>
		<idno type="PMCID">PMC6995018</idno>
		<ptr type="open-access" target="https://www.pnas.org/content/pnas/117/4/1924.full.pdf" />
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<title level="j" type="abbrev">Proc. Natl. Acad. Sci. U.S.A.</title>
		<idno type="ISSN">0027-8424</idno>
		<idno type="ISSNe">1091-6490</idno>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1924" to="1934" />
			<date type="published" when="2020-01-06" />
			<publisher>Proceedings of the National Academy of Sciences</publisher>
		</imprint>
	</monogr>
	<note>in press</note>
	<note type="raw_reference">Alan S Cowen, Xia Fang, Disa Sauter, and Dacher Kelt- ner. in press. What music makes us feel: At least thirteen dimensions organize subjective experiences associated with music across cultures. Proceedings of the National Academy of Sciences.</note>
</biblStruct>

<biblStruct coords="10,72.00,461.14,218.27,8.64;10,82.91,472.10,207.71,8.64;10,82.91,482.89,207.36,8.81;10,82.30,493.85,182.33,8.81" xml:id="b9">
	<analytic>
		<title level="a" type="main">Self-report captures 27 distinct categories of emotion bridged by continuous gradients</title>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">S</forename><surname>Cowen</surname></persName>
			<idno type="ORCID">0000-0002-8381-5883</idno>
		</author>
		<author>
			<persName coords=""><forename type="first">Dacher</forename><surname>Keltner</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.1702247114</idno>
		<idno type="PMID">28874542</idno>
		<idno type="PMCID">PMC5617253</idno>
		<ptr type="open-access" target="https://www.pnas.org/content/pnas/114/38/E7900.full.pdf" />
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<title level="j" type="abbrev">Proc. Natl. Acad. Sci. U.S.A.</title>
		<idno type="ISSN">0027-8424</idno>
		<idno type="ISSNe">1091-6490</idno>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="issue">38</biblScope>
			<biblScope unit="page" from="E7900" to="E7909" />
			<date type="published" when="2017-09-05">2017</date>
			<publisher>Proceedings of the National Academy of Sciences</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Alan S Cowen and Dacher Keltner. 2017. Self-report captures 27 distinct categories of emotion bridged by continuous gradients. Proceedings of the National Academy of Sciences, 114(38):E7900-E7909.</note>
</biblStruct>

<biblStruct coords="10,72.00,515.83,218.27,8.64;10,82.91,526.78,207.71,8.64;10,82.91,537.57,189.69,8.81" xml:id="b10">
	<analytic>
		<title level="a" type="main">Supplemental Material for What the Face Displays: Mapping 28 Emotions Conveyed by Naturalistic Expression</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Alan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dacher</forename><surname>Cowen</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Keltner</surname></persName>
		</author>
		<idno type="DOI">10.1037/amp0000488.supp</idno>
	</analytic>
	<monogr>
		<title level="j">American Psychologist</title>
		<title level="j" type="abbrev">American Psychologist</title>
		<idno type="ISSN">0003-066X</idno>
		<idno type="ISSNe">1935-990X</idno>
		<imprint>
			<date type="published" when="2019-06-17">2019</date>
			<publisher>American Psychological Association (APA)</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Alan S Cowen and Dacher Keltner. 2019. What the face displays: Mapping 28 emotions conveyed by naturalistic expression. American Psychologist.</note>
</biblStruct>

<biblStruct coords="10,72.00,559.55,219.52,8.64;10,82.91,570.51,209.02,8.64;10,82.91,581.47,207.36,8.64;10,82.91,592.26,209.01,8.81;10,82.91,603.22,102.93,8.81" xml:id="b11">
	<analytic>
		<title level="a" type="main">The primacy of categories in the recognition of 12 emotions in speech prosody across two cultures</title>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">S</forename><surname>Cowen</surname></persName>
			<idno type="ORCID">0000-0002-8381-5883</idno>
		</author>
		<author>
			<persName><forename type="first">Petri</forename><surname>Laukka</surname></persName>
			<idno type="ORCID">0000-0001-8771-6818</idno>
		</author>
		<author>
			<persName><forename type="first">Hillary</forename><forename type="middle">Anger</forename><surname>Elfenbein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Runjing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dacher</forename><surname>Keltner</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41562-019-0533-6</idno>
		<idno type="PMID">30971794</idno>
		<idno type="PMCID">PMC6687085</idno>
		<ptr type="open-access" target="https://europepmc.org/articles/pmc6687085?pdf=render" />
	</analytic>
	<monogr>
		<title level="j">Nature Human Behaviour</title>
		<title level="j" type="abbrev">Nat Hum Behav</title>
		<idno type="ISSNe">2397-3374</idno>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="369" to="382" />
			<date type="published" when="2019-03-11">2019b</date>
			<publisher>Springer Science and Business Media LLC</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Alan S Cowen, Petri Laukka, Hillary Anger Elfenbein, Runjing Liu, and Dacher Keltner. 2019b. The pri- macy of categories in the recognition of 12 emotions in speech prosody across two cultures. Nature Hu- man Behaviour, 3(4):369.</note>
</biblStruct>

<biblStruct coords="10,72.00,625.19,56.79,8.64;10,144.85,625.19,22.42,8.64;10,214.26,625.19,77.67,8.64;10,82.91,636.15,195.22,8.64" xml:id="b12">
	<analytic>
		<title level="a" type="main">Chinese Text Sentiment Analysis Utilizing Emotion Degree Lexicon and Fuzzy Semantic Model</title>
		<author>
			<persName><forename type="first">Xing</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaojian</forename><surname>Zhuo</surname></persName>
		</author>
		<idno type="DOI">10.4018/978-1-4666-9840-6.ch048</idno>
		<ptr target="https://www.figure-eight.com/data/sentiment-analysis-emotion-text/" />
	</analytic>
	<monogr>
		<title level="m">Big Data</title>
				<imprint>
			<publisher>IGI Global</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1077" to="1090" />
		</imprint>
	</monogr>
	<note type="raw_reference">CrowdFlower. 2016. https://www.figure- eight.com/data/sentiment-analysis-emotion-text/.</note>
</biblStruct>

<biblStruct coords="10,72.00,657.96,220.01,8.64;10,82.44,668.92,209.48,8.64;10,82.91,679.71,197.27,8.81" xml:id="b13">
	<analytic>
		<title level="a" type="main">Why Cohen’s Kappa should be avoided as performance measure in classification</title>
		<author>
			<persName coords=""><forename type="first">Rosario</forename><surname>Delgado</surname></persName>
			<idno type="ORCID">0000-0003-1208-9236</idno>
		</author>
		<author>
			<persName coords=""><forename type="first">Xavier-Andoni</forename><surname>Tibau</surname></persName>
			<idno type="ORCID">0000-0002-7239-1421</idno>
		</author>
		<idno type="DOI">10.1371/journal.pone.0222916</idno>
		<idno type="PMID">31557204</idno>
		<idno type="PMCID">PMC6762152</idno>
		<ptr type="open-access" target="https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0222916&amp;type=printable" />
	</analytic>
	<monogr>
		<title level="j">PLOS ONE</title>
		<title level="j" type="abbrev">PLoS ONE</title>
		<idno type="ISSNe">1932-6203</idno>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">e0222916</biblScope>
			<date type="published" when="2019-09-26">2019</date>
			<publisher>Public Library of Science (PLoS)</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Rosario Delgado and Xavier-Andoni Tibau. 2019. Why cohens kappa should be avoided as perfor- mance measure in classification. PloS one, 14(9).</note>
</biblStruct>

<biblStruct coords="10,72.00,701.69,218.27,8.64;10,82.91,712.64,207.36,8.64;10,82.91,723.60,209.01,8.64;10,82.91,734.39,209.01,8.81;10,82.91,745.35,207.36,8.58;10,82.63,756.31,85.79,8.58" xml:id="b14">
	<analytic>
		<title level="a" type="main"></title>
		<author>
			<persName coords=""><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North</title>
				<meeting>the 2019 Conference of the North</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language understand- ing. In 17th Annual Conference of the North Amer- ican Chapter of the Association for Computational Linguistics (NAACL).</note>
</biblStruct>

<biblStruct coords="10,307.28,67.28,218.27,8.64;10,318.19,78.07,207.36,8.81;10,317.91,89.03,82.02,8.81" xml:id="b15">
	<analytic>
		<title level="a" type="main">6% of online adults are reddit users</title>
		<author>
			<persName coords=""><forename type="first">Maeve</forename><surname>Duggan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aaron</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pew Internet &amp; American Life Project</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Maeve Duggan and Aaron Smith. 2013. 6% of online adults are reddit users. Pew Internet &amp; American Life Project, 3:1-10.</note>
</biblStruct>

<biblStruct coords="10,307.28,110.84,219.92,8.81;10,318.19,121.80,139.47,8.81" xml:id="b16">
	<analytic>
		<title level="a" type="main">Are there basic emotions?</title>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Ekman</surname></persName>
		</author>
		<idno type="DOI">10.1037/0033-295x.99.3.550</idno>
		<idno type="PMID">1344638</idno>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<title level="j" type="abbrev">Psychological Review</title>
		<idno type="ISSN">0033-295X</idno>
		<idno type="ISSNe">1939-1471</idno>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="550" to="553" />
			<date type="published" when="1992">1992a</date>
			<publisher>American Psychological Association (APA)</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Paul Ekman. 1992a. Are there basic emotions? Psy- chological Review, 99(3):550-553.</note>
</biblStruct>

<biblStruct coords="10,307.28,143.77,220.01,8.64;10,317.86,154.56,156.26,8.81" xml:id="b17">
	<analytic>
		<title level="a" type="main">An argument for basic emotions</title>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Ekman</surname></persName>
		</author>
		<idno type="DOI">10.1080/02699939208411068</idno>
	</analytic>
	<monogr>
		<title level="j">Cognition and Emotion</title>
		<title level="j" type="abbrev">Cognition and Emotion</title>
		<idno type="ISSN">0269-9931</idno>
		<idno type="ISSNe">1464-0600</idno>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="169" to="200" />
			<date type="published" when="1992-05">1992b</date>
			<publisher>Informa UK Limited</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Paul Ekman. 1992b. An argument for basic emotions. Cognition &amp; Emotion, 6(3-4):169-200.</note>
</biblStruct>

<biblStruct coords="10,307.28,176.54,220.01,8.64;10,318.19,187.50,209.01,8.64;10,318.19,198.29,207.35,8.81;10,318.02,209.25,209.18,8.58;10,318.19,220.21,136.71,8.81" xml:id="b18">
	<analytic>
		<title level="a" type="main">Detecting Emotion Stimuli in Emotion-Bearing Sentences</title>
		<author>
			<persName coords=""><forename type="first">Diman</forename><surname>Ghazi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Diana</forename><surname>Inkpen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stan</forename><surname>Szpakowicz</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-18117-2_12</idno>
	</analytic>
	<monogr>
		<title level="m">Computational Linguistics and Intelligent Text Processing</title>
				<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="152" to="165" />
		</imprint>
	</monogr>
	<note type="raw_reference">Diman Ghazi, Diana Inkpen, and Stan Szpakowicz. 2015. Detecting Emotion Stimuli in Emotion- Bearing Sentences. In International Conference on Intelligent Text Processing and Computational Lin- guistics, pages 152-165. Springer.</note>
</biblStruct>

<biblStruct coords="10,307.28,242.18,218.27,8.64;10,318.19,253.14,207.36,8.64;10,318.19,263.93,207.36,8.81;10,318.02,274.89,209.18,8.58;10,318.19,285.85,208.61,8.81;10,317.83,296.98,209.37,8.64;10,318.19,307.94,16.33,8.64" xml:id="b19">
	<analytic>
		<title level="a" type="main">EmotionX challenge overview: Recognizing emotions in dialogues</title>
		<author>
			<persName coords=""><forename type="first">Chun</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lun-Wei</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Ku</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W18-3505</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth International Workshop on Natural Language Processing for Social Media</title>
				<meeting>the Sixth International Workshop on Natural Language Processing for Social Media<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="27" to="31" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
	<note type="raw_reference">Chao-Chun Hsu and Lun-Wei Ku. 2018. SocialNLP 2018 EmotionX challenge overview: Recognizing emotions in dialogues. In Proceedings of the Sixth International Workshop on Natural Language Pro- cessing for Social Media, pages 27-31, Melbourne, Australia. Association for Computational Linguis- tics.</note>
</biblStruct>

<biblStruct coords="10,307.28,329.74,218.27,8.64;10,318.19,340.70,207.71,8.64;10,318.19,351.49,207.35,8.81;10,318.19,362.45,75.27,8.58" xml:id="b20">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Yanran</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hui</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiaoyu</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ziqiang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shuzi</forename><surname>Niu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.03957</idno>
		<title level="m">Dailydialog: A manually labelled multi-turn dialogue dataset</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Yanran Li, Hui Su, Xiaoyu Shen, Wenjie Li, Ziqiang Cao, and Shuzi Niu. 2017. Dailydialog: A manually labelled multi-turn dialogue dataset. arXiv preprint arXiv:1710.03957.</note>
</biblStruct>

<biblStruct coords="10,307.28,384.43,219.92,8.64;10,318.19,395.39,209.02,8.64;10,318.19,406.18,190.84,8.81" xml:id="b21">
	<analytic>
		<title level="a" type="main">DENS: A Dataset for Multi-class Emotion Analysis</title>
		<author>
			<persName coords=""><forename type="first">Chen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Muhammad</forename><surname>Osama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anderson</forename><surname>De Andrade</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d19-1656</idno>
		<idno type="arXiv">arXiv:1910.11769</idno>
		<ptr type="open-access" target="https://www.aclweb.org/anthology/D19-1656.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">De Andrade. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Chen Liu, Muhammad Osama, and Anderson De An- drade. 2019. Dens: A dataset for multi-class emo- tion analysis. arXiv preprint arXiv:1910.11769.</note>
</biblStruct>

<biblStruct coords="10,307.28,428.15,219.92,8.64;10,318.19,439.11,207.36,8.64;10,318.19,449.90,207.36,8.81;10,317.77,460.86,209.43,8.58;10,318.19,471.82,200.60,8.81" xml:id="b22">
	<analytic>
		<title level="a" type="main">Obtaining Reliable Human Ratings of Valence, Arousal, and Dominance for 20,000 English Words</title>
		<author>
			<persName coords=""><forename type="first">Saif</forename><surname>Mohammad</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/p18-1017</idno>
		<ptr type="open-access" target="https://www.aclweb.org/anthology/P18-1017.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</title>
				<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="174" to="184" />
		</imprint>
	</monogr>
	<note type="raw_reference">Saif Mohammad. 2018. Obtaining reliable human rat- ings of valence, arousal, and dominance for 20,000 english words. In Proceedings of the 56th Annual Meeting of the Association for Computational Lin- guistics (Volume 1: Long Papers), pages 174-184.</note>
</biblStruct>

<biblStruct coords="10,307.28,493.80,218.27,8.64;10,318.19,504.76,209.01,8.64;10,318.19,515.55,207.36,8.81;10,317.63,526.50,209.57,8.58;10,318.19,537.46,209.01,8.81;10,318.19,548.59,152.21,8.64" xml:id="b23">
	<analytic>
		<title level="a" type="main">SemEval-2018 Task 1: Affect in Tweets</title>
		<author>
			<persName coords=""><forename type="first">Saif</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Felipe</forename><surname>Bravo-Marquez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mohammad</forename><surname>Salameh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Svetlana</forename><surname>Kiritchenko</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/s18-1001</idno>
		<ptr type="open-access" target="https://www.aclweb.org/anthology/S18-1001.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 12th International Workshop on Semantic Evaluation</title>
				<meeting>The 12th International Workshop on Semantic Evaluation<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="17" />
		</imprint>
	</monogr>
	<note type="raw_reference">Saif Mohammad, Felipe Bravo-Marquez, Mohammad Salameh, and Svetlana Kiritchenko. 2018. SemEval- 2018 task 1: Affect in tweets. In Proceedings of The 12th International Workshop on Semantic Eval- uation, pages 1-17, New Orleans, Louisiana. Asso- ciation for Computational Linguistics.</note>
</biblStruct>

<biblStruct coords="10,307.28,570.23,219.92,8.81;10,318.19,581.19,207.36,8.58;10,318.19,592.15,209.01,8.58;10,318.19,603.11,207.36,8.58;10,317.58,614.07,207.97,8.58;10,317.36,625.02,209.93,8.81;10,317.83,636.15,172.13,8.64" xml:id="b24">
	<analytic>
		<title level="a" type="main"># emotional tweets</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Saif</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Mohammad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Joint Conference on Lexical and Computational Semantics</title>
				<meeting>the First Joint Conference on Lexical and Computational Semantics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="246" to="255" />
		</imprint>
	</monogr>
	<note>Proceedings of the Sixth International Workshop on Semantic Evaluation</note>
	<note type="raw_reference">Saif M Mohammad. 2012. # emotional tweets. In Pro- ceedings of the First Joint Conference on Lexical and Computational Semantics-Volume 1: Proceed- ings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation, pages 246-255. Association for Computational Linguistics.</note>
</biblStruct>

<biblStruct coords="10,307.28,657.96,219.92,8.64;10,318.19,668.92,209.02,8.64;10,318.19,679.71,209.01,8.81;10,318.19,690.67,193.05,8.81" xml:id="b25">
	<analytic>
		<title level="a" type="main">Sentiment, emotion, purpose, and style in electoral tweets</title>
		<author>
			<persName><forename type="first">Saif</forename><forename type="middle">M</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Svetlana</forename><surname>Kiritchenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joel</forename><surname>Martin</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ipm.2014.09.003</idno>
	</analytic>
	<monogr>
		<title level="j">Information Processing &amp; Management</title>
		<title level="j" type="abbrev">Information Processing &amp; Management</title>
		<idno type="ISSN">0306-4573</idno>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="480" to="499" />
			<date type="published" when="2015-07">2015</date>
			<publisher>Elsevier BV</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Saif M Mohammad, Xiaodan Zhu, Svetlana Kir- itchenko, and Joel Martin. 2015. Sentiment, emo- tion, purpose, and style in electoral tweets. Informa- tion Processing &amp; Management, 51(4):480-499.</note>
</biblStruct>

<biblStruct coords="10,307.28,712.64,218.27,8.64;10,318.19,723.60,209.10,8.64;10,317.88,734.56,207.67,8.64;10,318.19,745.35,207.36,8.81;10,318.02,756.31,143.23,8.81" xml:id="b26">
	<analytic>
		<title level="a" type="main">The Impact of Toxic Language on the Health of Reddit Communities</title>
		<author>
			<persName coords=""><forename type="first">Shruthi</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Apala</forename><surname>Guha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fred</forename><surname>Popowich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ashley</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chris</forename><surname>Priebe</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-57351-9_6</idno>
	</analytic>
	<monogr>
		<title level="m">Advances in Artificial Intelligence</title>
				<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="51" to="56" />
		</imprint>
	</monogr>
	<note type="raw_reference">Shruthi Mohan, Apala Guha, Michael Harris, Fred Popowich, Ashley Schuster, and Chris Priebe. 2017. The impact of toxic language on the health of reddit communities. In Canadian Conference on Artificial Intelligence, pages 51-56. Springer.</note>
</biblStruct>

<biblStruct coords="11,72.00,67.28,218.27,8.64;11,82.91,78.24,209.01,8.64;11,82.91,89.20,209.02,8.64;11,82.91,99.99,197.62,8.81" xml:id="b27">
	<analytic>
		<title level="a" type="main">Fightin&apos; Words: Lexical Feature Selection and Evaluation for Identifying the Content of Political Conflict</title>
		<author>
			<persName coords=""><forename type="first">Burt</forename><forename type="middle">L</forename><surname>Monroe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">P</forename><surname>Colaresi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kevin</forename><forename type="middle">M</forename><surname>Quinn</surname></persName>
		</author>
		<idno type="DOI">10.1093/pan/mpn018</idno>
		<ptr type="open-access" target="https://www.cambridge.org/core/services/aop-cambridge-core/content/view/81B3703230D21620B81EB6E2266C7A66/S1047198700002291a.pdf/div-class-title-fightin-words-lexical-feature-selection-and-evaluation-for-identifying-the-content-of-political-conflict-div.pdf" />
	</analytic>
	<monogr>
		<title level="j">Political Analysis</title>
		<title level="j" type="abbrev">Polit. anal.</title>
		<idno type="ISSN">1047-1987</idno>
		<idno type="ISSNe">1476-4989</idno>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="372" to="403" />
			<date type="published" when="2008">2008</date>
			<publisher>Cambridge University Press (CUP)</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Burt L Monroe, Michael P Colaresi, and Kevin M Quinn. 2008. Fightin&apos;words: Lexical feature selec- tion and evaluation for identifying the content of po- litical conflict. Political Analysis, 16(4):372-403.</note>
</biblStruct>

<biblStruct coords="11,72.00,119.64,218.27,10.95;11,82.60,132.91,93.07,8.64;11,192.52,132.91,97.92,8.64;11,82.91,143.87,207.36,8.64;11,82.91,154.66,207.36,8.81;11,82.91,165.62,207.36,8.58;11,82.66,176.58,208.85,8.58;11,82.91,187.71,52.57,8.64" xml:id="b28">
	<analytic>
		<title level="a" type="main">Creating a Dataset for Multilingual Fine-grained Emotion-detection Using Gamification-based Annotation</title>
		<author>
			<persName coords=""><forename type="first">Emily</forename><surname>Öhman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kaisla</forename><surname>Kajava</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jörg</forename><surname>Tiedemann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Timo</forename><surname>Honkela</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/w18-6205</idno>
		<ptr type="open-access" target="https://www.aclweb.org/anthology/W18-6205.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</title>
				<meeting>the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="24" to="30" />
		</imprint>
	</monogr>
	<note type="raw_reference">Emily Öhman, Kaisla Kajava, Jörg Tiedemann, and Timo Honkela. 2018. Creating a dataset for multilingual fine-grained emotion-detection using gamification-based annotation. In Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 24-30.</note>
</biblStruct>

<biblStruct coords="11,72.00,209.51,219.52,8.64;11,82.91,220.46,208.61,8.64;11,82.91,231.42,208.61,8.64;11,82.91,242.38,209.01,8.64;11,82.91,253.34,207.36,8.64;11,82.91,264.13,208.60,8.81;11,82.16,275.26,60.05,8.64" xml:id="b29">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note type="raw_reference">F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duch- esnay. 2011. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825-2830.</note>
</biblStruct>

<biblStruct coords="11,72.00,297.05,218.27,8.64;11,82.91,308.01,207.36,8.64;11,82.91,318.97,209.02,8.64;11,82.91,329.76,207.36,8.81;11,82.58,340.72,209.34,8.58;11,82.91,351.68,129.24,8.58" xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep Contextualized Word Representations</title>
		<author>
			<persName coords=""><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n18-1202</idno>
		<ptr type="open-access" target="https://www.aclweb.org/anthology/N18-1202.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)</title>
				<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep Contextualized Word Rep- resentations. In 16th Annual Conference of the North American Chapter of the Association for Com- putational Linguistics (NAACL).</note>
</biblStruct>

<biblStruct coords="11,72.00,373.48,218.58,8.81;11,82.91,384.60,23.52,8.64" xml:id="b31">
	<monogr>
		<title level="m" type="main">Affective computing</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Rosalind</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rosalind</forename><forename type="middle">W</forename><surname>Picard</surname></persName>
		</author>
		<idno type="DOI">10.1037/e526112012-054</idno>
		<imprint>
			<date type="published" when="1997">1997</date>
			<publisher>American Psychological Association (APA)</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Rosalind W Picard. 1997. Affective Computing. MIT Press.</note>
</biblStruct>

<biblStruct coords="11,72.00,406.40,218.27,8.64;11,82.91,417.36,209.10,8.64;11,82.91,428.15,207.36,8.81;11,82.66,439.11,207.61,8.58;11,82.30,450.07,205.90,8.81" xml:id="b32">
	<analytic>
		<title level="a" type="main">Identifying Depression on Reddit: The Effect of Training Data</title>
		<author>
			<persName coords=""><forename type="first">Inna</forename><surname>Pirina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Çağrı</forename><surname>Çöltekin</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/w18-5903</idno>
		<ptr type="open-access" target="https://www.aclweb.org/anthology/W18-5903.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 EMNLP Workshop SMM4H: The 3rd Social Media Mining for Health Applications Workshop &amp; Shared Task</title>
				<meeting>the 2018 EMNLP Workshop SMM4H: The 3rd Social Media Mining for Health Applications Workshop &amp; Shared Task</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="9" to="12" />
		</imprint>
	</monogr>
	<note type="raw_reference">Inna Pirina and C ¸agrı C ¸öltekin. 2018. Identifying depression on reddit: The effect of training data. In Proceedings of the 2018 EMNLP Workshop SMM4H: The 3rd Social Media Mining for Health Applications Workshop &amp; Shared Task, pages 9-12.</note>
</biblStruct>

<biblStruct coords="11,72.00,472.03,218.62,8.64;11,82.91,482.82,207.36,8.81;11,82.91,493.95,59.25,8.64" xml:id="b33">
	<analytic>
		<title level="a" type="main">A GENERAL PSYCHOEVOLUTIONARY THEORY OF EMOTION</title>
		<author>
			<persName coords=""><forename type="first">Robert</forename><surname>Plutchik</surname></persName>
		</author>
		<idno type="DOI">10.1016/b978-0-12-558701-3.50007-7</idno>
	</analytic>
	<monogr>
		<title level="m">Theories of Emotion</title>
				<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="1980">1980</date>
			<biblScope unit="page" from="3" to="33" />
		</imprint>
	</monogr>
	<note type="raw_reference">Robert Plutchik. 1980. A general psychoevolutionary theory of emotion. In Theories of emotion, pages 3-33. Elsevier.</note>
</biblStruct>

<biblStruct coords="11,72.00,515.75,219.92,8.64;11,82.91,526.54,208.60,8.81;11,82.16,537.67,46.76,8.64" xml:id="b34">
	<analytic>
		<title level="a" type="main">Core affect and the psychological construction of emotion.</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">James</forename><forename type="middle">A</forename><surname>Russell</surname></persName>
		</author>
		<idno type="DOI">10.1037/0033-295x.110.1.145</idno>
		<idno type="PMID">12529060</idno>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<title level="j" type="abbrev">Psychological Review</title>
		<idno type="ISSN">0033-295X</idno>
		<idno type="ISSNe">1939-1471</idno>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="145" to="172" />
			<date type="published" when="2003">2003</date>
			<publisher>American Psychological Association (APA)</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">James A Russell. 2003. Core affect and the psycholog- ical construction of emotion. Psychological review, 110(1):145.</note>
</biblStruct>

<biblStruct coords="11,72.00,559.46,219.92,8.64;11,82.91,570.42,209.01,8.64;11,82.91,581.21,207.36,8.81;11,82.91,592.17,182.28,8.81" xml:id="b35">
	<analytic>
		<title level="a" type="main">Evidence for universality and cultural variation of differential emotion response patterning.</title>
		<author>
			<persName><forename type="first">Klaus</forename><forename type="middle">R</forename><surname>Scherer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Harald</forename><forename type="middle">G</forename><surname>Wallbott</surname></persName>
		</author>
		<idno type="DOI">10.1037/0022-3514.66.2.310</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Personality and Social Psychology</title>
		<title level="j" type="abbrev">Journal of Personality and Social Psychology</title>
		<idno type="ISSN">0022-3514</idno>
		<idno type="ISSNe">1939-1315</idno>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="310" to="328" />
			<date type="published" when="1994">1994</date>
			<publisher>American Psychological Association (APA)</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Klaus R Scherer and Harald G Wallbott. 1994. Evi- dence for Universality and Cultural Variation of Dif- ferential Emotion Response Patterning. Journal of personality and social psychology, 66(2):310.</note>
</biblStruct>

<biblStruct coords="11,72.00,614.14,219.92,8.64;11,82.91,625.09,208.60,8.64;11,82.91,636.05,207.36,8.64;11,82.91,646.84,209.01,8.81;11,82.91,657.80,209.02,8.58;11,82.91,668.76,209.01,8.58;11,82.91,679.72,106.54,8.81" xml:id="b36">
	<analytic>
		<title level="a" type="main">Annotation, Modelling and Analysis of Fine-Grained Emotions on a Stance and Sentiment Detection Corpus</title>
		<author>
			<persName coords=""><forename type="first">Hendrik</forename><surname>Schuff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeremy</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Julian</forename><surname>Mohme</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sebastian</forename><surname>Padó</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Roman</forename><surname>Klinger</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/w17-5203</idno>
		<ptr type="open-access" target="https://www.aclweb.org/anthology/W17-5203.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</title>
				<meeting>the 8th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="13" to="23" />
		</imprint>
	</monogr>
	<note type="raw_reference">Hendrik Schuff, Jeremy Barnes, Julian Mohme, Sebas- tian Padó, and Roman Klinger. 2017. Annotation, modelling and analysis of fine-grained emotions on a stance and sentiment detection corpus. In Pro- ceedings of the 8th Workshop on Computational Ap- proaches to Subjectivity, Sentiment and Social Me- dia Analysis, pages 13-23.</note>
</biblStruct>

<biblStruct coords="11,72.00,701.69,219.92,8.64;11,82.91,712.48,207.36,8.81;11,82.30,723.43,209.62,8.58;11,82.91,734.39,207.35,8.81;11,82.91,745.52,209.02,8.64;11,82.91,756.48,16.33,8.64" xml:id="b37">
	<analytic>
		<title level="a" type="main">SemEval-2007 task 14</title>
		<author>
			<persName coords=""><forename type="first">Carlo</forename><surname>Strapparava</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<idno type="DOI">10.3115/1621474.1621487</idno>
		<ptr type="open-access" target="http://dl.acm.org/ft_gateway.cfm?id=1621487&amp;type=pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th International Workshop on Semantic Evaluations - SemEval &apos;07</title>
				<meeting>the 4th International Workshop on Semantic Evaluations - SemEval &apos;07<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="70" to="74" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
	<note type="raw_reference">Carlo Strapparava and Rada Mihalcea. 2007. SemEval- 2007 task 14: Affective text. In Proceedings of the Fourth International Workshop on Semantic Evalua- tions (SemEval-2007), pages 70-74, Prague, Czech Republic. Association for Computational Linguis- tics.</note>
</biblStruct>

<biblStruct coords="11,307.28,67.28,220.01,8.64;11,318.19,78.24,207.36,8.64;11,317.58,89.03,171.60,8.58" xml:id="b38">
	<analytic>
		<title level="a" type="main">BERT Rediscovers the Classical NLP Pipeline</title>
		<author>
			<persName coords=""><forename type="first">Ian</forename><surname>Tenney</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ellie</forename><surname>Pavlick</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/p19-1452</idno>
		<ptr type="open-access" target="http://arxiv.org/pdf/1905.05950" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Ian Tenney, Dipanjan Das, and Ellie Pavlick. 2019. BERT Rediscovers the Classical NLP Pipeline. In Association for Computational Linguistics.</note>
</biblStruct>

<biblStruct coords="11,307.28,109.12,219.92,8.64;11,317.94,120.08,207.61,8.64;11,318.19,131.04,209.10,8.64;11,318.19,141.83,68.35,8.81" xml:id="b39">
	<analytic>
		<title level="a" type="main">Small and Practical BERT Models for Sequence Labeling</title>
		<author>
			<persName coords=""><forename type="first">Henry</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jason</forename><surname>Riesa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Naveen</forename><surname>Arivazhagan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Amelia</forename><surname>Archer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d19-1374</idno>
		<ptr type="open-access" target="https://www.aclweb.org/anthology/D19-1374.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Henry Tsai, Jason Riesa, Melvin Johnson, Naveen Ari- vazhagan, Xin Li, and Amelia Archer. 2019. Small and Practical BERT Models for Sequence Labeling. In EMNLP 2019.</note>
</biblStruct>

<biblStruct coords="11,307.28,161.93,219.52,8.64;11,318.19,172.89,207.36,8.64;11,318.19,183.68,207.36,8.81;11,318.02,194.63,207.53,8.58;11,318.19,205.59,209.02,8.58;11,318.19,216.55,155.79,8.81" xml:id="b40">
	<analytic>
		<title level="a" type="main">Harnessing Twitter &amp;#x0022;Big Data&amp;#x0022; for Automatic Emotion Identification</title>
		<author>
			<persName coords=""><forename type="first">Wenbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Krishnaprasad</forename><surname>Thirunarayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amit</forename><forename type="middle">P</forename><surname>Sheth</surname></persName>
		</author>
		<idno type="DOI">10.1109/socialcom-passat.2012.119</idno>
	</analytic>
	<monogr>
		<title level="m">2012 International Conference on Privacy, Security, Risk and Trust and 2012 International Confernece on Social Computing</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012-09">2012</date>
			<biblScope unit="page" from="587" to="592" />
		</imprint>
	</monogr>
	<note type="raw_reference">Wenbo Wang, Lu Chen, Krishnaprasad Thirunarayan, and Amit P Sheth. 2012. Harnessing twitter &apos;big data&apos; for automatic emotion identification. In 2012 International Conference on Privacy, Security, Risk and Trust and 2012 International Confernece on So- cial Computing, pages 587-592. IEEE.</note>
</biblStruct>

<biblStruct coords="11,307.28,236.65,219.92,8.64;11,318.19,247.61,129.58,8.64" xml:id="b41">
	<monogr>
		<title level="m" type="main">Norman: Worlds first psychopath ai</title>
		<author>
			<persName coords=""><forename type="first">-</forename><forename type="middle">M</forename><surname>Cebrian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Yanardag</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Rahwan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Cebrian-M. Yanardag, P. and I. Rahwan. 2018. Nor- man: Worlds first psychopath ai.</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
