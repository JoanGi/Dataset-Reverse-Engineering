[
  {
    "question": "What is GoEmotions dataset?",
    "answers": [
      [
        "Score: 0.95",
        "We conduct transfer learning experiments on existing emotion benchmarks, in order to show our data generalizes across domains and taxonomies. The goal is to demonstrate that given little labeled data in a target domain, one can utilize GoEmotions as baseline emotion understanding data. ity and taxonomy. In the interest of space, we only discuss three of these datasets here, chosen based on their diversity of domains. In our experiments, we observe similar trends for the additional benchmarks, and all are included in the Appendix H. The International Survey on Emotion Antecedents and Reactions (ISEAR) (Scherer and Wallbott, 1994 ) is a collection of personal reports on emotional events, written by 3000 people from different cultural backgrounds. The dataset contains 8k sentences, each labeled with a single emotion. The categories are anger, disgust, fear, guilt, joy, sadness and shame."
      ],
      [
        "Score: 0.92",
        "description: ### Context GoEmotions is a corpus of 58k carefully curated comments extracted from Reddit, with human annotations to 27 emotion categories or Neutral.  - Number of examples: 58,009. - Number of labels: 27 + Neutral. - Maximum sequence length in training and evaluation datasets: 30.   ### Content On top of the raw data, we also include a version filtered based on reter-agreement, which contains a train/test/validation split:  - Size of training dataset: 43,410. - Size of test dataset: 5,427. - Size of validation dataset: 5,426. The emotion categories are: admiration, amusement, anger, annoyance, approval, caring, confusion, curiosity, desire, disappointment, disapproval, disgust, embarrassment, excitement, fear, gratitude, grief, joy, love, nervousness, optimism, pride, realization, relief, remorse, sadness, surprise.  `analyze_data.py` and `extract_words.py` have already been run and the output files are stored in `plots` and `tables` respectively. What's inside is more than just rows and columns. Make it easy for others to get started by describing how you acquired the data and what time period it represents, too. Original repository from where the data is taken: [https://github.com/google-research/google-research/tree/master/goemotions](https://github.com/google-research/google-research/tree/master/goemotions) However, some of the scripts were giving errors which have been rectified and updated in this repository: [https://github.com/DebarshiChanda/google-research/tree/master/goemotions](https://github.com/DebarshiChanda/google-research/tree/master/goemotions)  ### Acknowledgements The entire Credit for creating this dataset goes to Google Research Team.   ### Inspiration Detect emotion from the text using Multi-label Classification"
      ],
      [
        "Score: 0.92",
        "wget -P data/full_dataset/ https://storage.googleapis.com/gresearch/goemotions/data/full_dataset/goemotions_1.csv wget -P data/full_dataset/ https://storage.googleapis.com/gresearch/goemotions/data/full_dataset/goemotions_2.csv wget -P data/full_dataset/ https://storage.googleapis.com/gresearch/goemotions/data/full_dataset/goemotions_3.csv"
      ],
      [
        "Score: 0.91",
        "We present GoEmotions, a large, manually annotated, carefully curated dataset for fine-grained emotion prediction. We provide a detailed data analysis, demonstrating the reliability of the annotations for the full taxonomy. We show the generalizability of the data across domains and taxonomies via transfer learning experiments. We build a strong baseline by fine-tuning a BERT model, however, the results suggest much room for future improvement. Future work can explore the cross-cultural robustness of emotion ratings, and extend the taxonomy to other languages and domains."
      ],
      [
        "Score: 0.90",
        "The results in Figure 3 suggest that our dataset generalizes well to different domains and taxonomies, and that using a model using GoEmotions can help in cases when there is limited data from the target domain, or limited resources for labeling."
      ]
    ]
  },
  {
    "question": "What is GoEmotions about?",
    "answers": [
      [
        "Score: 0.87",
        "GoEmotions GoEmotions is a corpus of 58k carefully curated comments extracted from Reddit, with human annotations to 27 emotion categories or Neutral."
      ],
      [
        "Score: 0.46",
        "We present GoEmotions, a large, manually annotated, carefully curated dataset for fine-grained emotion prediction. We provide a detailed data analysis, demonstrating the reliability of the annotations for the full taxonomy. We show the generalizability of the data across domains and taxonomies via transfer learning experiments. We build a strong baseline by fine-tuning a BERT model, however, the results suggest much room for future improvement. Future work can explore the cross-cultural robustness of emotion ratings, and extend the taxonomy to other languages and domains."
      ],
      [
        "Score: 0.41",
        "description: ### Context GoEmotions is a corpus of 58k carefully curated comments extracted from Reddit, with human annotations to 27 emotion categories or Neutral.  - Number of examples: 58,009. - Number of labels: 27 + Neutral. - Maximum sequence length in training and evaluation datasets: 30.   ### Content On top of the raw data, we also include a version filtered based on reter-agreement, which contains a train/test/validation split:  - Size of training dataset: 43,410. - Size of test dataset: 5,427. - Size of validation dataset: 5,426. The emotion categories are: admiration, amusement, anger, annoyance, approval, caring, confusion, curiosity, desire, disappointment, disapproval, disgust, embarrassment, excitement, fear, gratitude, grief, joy, love, nervousness, optimism, pride, realization, relief, remorse, sadness, surprise.  `analyze_data.py` and `extract_words.py` have already been run and the output files are stored in `plots` and `tables` respectively. What's inside is more than just rows and columns. Make it easy for others to get started by describing how you acquired the data and what time period it represents, too. Original repository from where the data is taken: [https://github.com/google-research/google-research/tree/master/goemotions](https://github.com/google-research/google-research/tree/master/goemotions) However, some of the scripts were giving errors which have been rectified and updated in this repository: [https://github.com/DebarshiChanda/google-research/tree/master/goemotions](https://github.com/DebarshiChanda/google-research/tree/master/goemotions)  ### Acknowledgements The entire Credit for creating this dataset goes to Google Research Team.   ### Inspiration Detect emotion from the text using Multi-label Classification"
      ],
      [
        "Score: 0.41",
        "Figure 6 shows the normalized confusion matrix for our model predictions. Since GoEmotions is a multilabel dataset, we calculate the confusion matrix similarly as we would calculate a co-occurrence matrix: for each true label, we increase the count for each predicted label. Specifically, we define a matrix M where M i,j denotes the raw confusion count between the true label i and the predicted label j. For example, if the true labels are joy and admiration, and the predicted labels are joy and pride, then we increase the count for M joy,joy , M joy,pride , M admiration,joy and M admiration,pride ."
      ],
      [
        "Score: 0.35",
        "We provide a strong baseline for modeling finegrained emotion classification over GoEmotions. By fine-tuning a BERT-base model (Devlin et al., 2019) , we achieve an average F1-score of .46 over our taxonomy, .64 over an Ekman-style grouping into six coarse categories and .69 over a sentiment grouping. These results leave much room for improvement, showcasing this task is not yet fully addressed by current state-of-the-art NLU models."
      ]
    ]
  },
  {
    "question": "Which are the purposes GoEmotions fills?",
    "answers": [
      [
        "Score: 0.67",
        "GoEmotions GoEmotions is a corpus of 58k carefully curated comments extracted from Reddit, with human annotations to 27 emotion categories or Neutral."
      ],
      [
        "Score: 0.45",
        "We present GoEmotions, a large, manually annotated, carefully curated dataset for fine-grained emotion prediction. We provide a detailed data analysis, demonstrating the reliability of the annotations for the full taxonomy. We show the generalizability of the data across domains and taxonomies via transfer learning experiments. We build a strong baseline by fine-tuning a BERT model, however, the results suggest much room for future improvement. Future work can explore the cross-cultural robustness of emotion ratings, and extend the taxonomy to other languages and domains."
      ],
      [
        "Score: 0.33",
        "The results in Figure 3 suggest that our dataset generalizes well to different domains and taxonomies, and that using a model using GoEmotions can help in cases when there is limited data from the target domain, or limited resources for labeling."
      ],
      [
        "Score: 0.32",
        "We provide a strong baseline for modeling finegrained emotion classification over GoEmotions. By fine-tuning a BERT-base model (Devlin et al., 2019) , we achieve an average F1-score of .46 over our taxonomy, .64 over an Ekman-style grouping into six coarse categories and .69 over a sentiment grouping. These results leave much room for improvement, showcasing this task is not yet fully addressed by current state-of-the-art NLU models."
      ],
      [
        "Score: 0.31",
        "description: ### Context GoEmotions is a corpus of 58k carefully curated comments extracted from Reddit, with human annotations to 27 emotion categories or Neutral.  - Number of examples: 58,009. - Number of labels: 27 + Neutral. - Maximum sequence length in training and evaluation datasets: 30.   ### Content On top of the raw data, we also include a version filtered based on reter-agreement, which contains a train/test/validation split:  - Size of training dataset: 43,410. - Size of test dataset: 5,427. - Size of validation dataset: 5,426. The emotion categories are: admiration, amusement, anger, annoyance, approval, caring, confusion, curiosity, desire, disappointment, disapproval, disgust, embarrassment, excitement, fear, gratitude, grief, joy, love, nervousness, optimism, pride, realization, relief, remorse, sadness, surprise.  `analyze_data.py` and `extract_words.py` have already been run and the output files are stored in `plots` and `tables` respectively. What's inside is more than just rows and columns. Make it easy for others to get started by describing how you acquired the data and what time period it represents, too. Original repository from where the data is taken: [https://github.com/google-research/google-research/tree/master/goemotions](https://github.com/google-research/google-research/tree/master/goemotions) However, some of the scripts were giving errors which have been rectified and updated in this repository: [https://github.com/DebarshiChanda/google-research/tree/master/goemotions](https://github.com/DebarshiChanda/google-research/tree/master/goemotions)  ### Acknowledgements The entire Credit for creating this dataset goes to Google Research Team.   ### Inspiration Detect emotion from the text using Multi-label Classification"
      ]
    ]
  },
  {
    "question": "Which gaps GoEmotions fills?",
    "answers": [
      [
        "Score: 0.90",
        "We present a strong baseline emotion prediction model for GoEmotions."
      ],
      [
        "Score: 0.61",
        "We provide a strong baseline for modeling finegrained emotion classification over GoEmotions. By fine-tuning a BERT-base model (Devlin et al., 2019) , we achieve an average F1-score of .46 over our taxonomy, .64 over an Ekman-style grouping into six coarse categories and .69 over a sentiment grouping. These results leave much room for improvement, showcasing this task is not yet fully addressed by current state-of-the-art NLU models."
      ],
      [
        "Score: 0.54",
        "We present GoEmotions, a large, manually annotated, carefully curated dataset for fine-grained emotion prediction. We provide a detailed data analysis, demonstrating the reliability of the annotations for the full taxonomy. We show the generalizability of the data across domains and taxonomies via transfer learning experiments. We build a strong baseline by fine-tuning a BERT model, however, the results suggest much room for future improvement. Future work can explore the cross-cultural robustness of emotion ratings, and extend the taxonomy to other languages and domains."
      ],
      [
        "Score: 0.45",
        "GoEmotions GoEmotions is a corpus of 58k carefully curated comments extracted from Reddit, with human annotations to 27 emotion categories or Neutral."
      ],
      [
        "Score: 0.18",
        "The results in Figure 3 suggest that our dataset generalizes well to different domains and taxonomies, and that using a model using GoEmotions can help in cases when there is limited data from the target domain, or limited resources for labeling."
      ]
    ]
  },
  {
    "question": "What demonstrate GoEmotions dataset?",
    "answers": [
      [
        "Score: 0.98",
        "We conduct transfer learning experiments on existing emotion benchmarks, in order to show our data generalizes across domains and taxonomies. The goal is to demonstrate that given little labeled data in a target domain, one can utilize GoEmotions as baseline emotion understanding data. ity and taxonomy. In the interest of space, we only discuss three of these datasets here, chosen based on their diversity of domains. In our experiments, we observe similar trends for the additional benchmarks, and all are included in the Appendix H. The International Survey on Emotion Antecedents and Reactions (ISEAR) (Scherer and Wallbott, 1994 ) is a collection of personal reports on emotional events, written by 3000 people from different cultural backgrounds. The dataset contains 8k sentences, each labeled with a single emotion. The categories are anger, disgust, fear, guilt, joy, sadness and shame."
      ],
      [
        "Score: 0.97",
        "We present GoEmotions, a large, manually annotated, carefully curated dataset for fine-grained emotion prediction. We provide a detailed data analysis, demonstrating the reliability of the annotations for the full taxonomy. We show the generalizability of the data across domains and taxonomies via transfer learning experiments. We build a strong baseline by fine-tuning a BERT model, however, the results suggest much room for future improvement. Future work can explore the cross-cultural robustness of emotion ratings, and extend the taxonomy to other languages and domains."
      ],
      [
        "Score: 0.95",
        "The results in Figure 3 suggest that our dataset generalizes well to different domains and taxonomies, and that using a model using GoEmotions can help in cases when there is limited data from the target domain, or limited resources for labeling."
      ],
      [
        "Score: 0.94",
        "The emotion categories are: admiration, amusement, anger, annoyance, approval, caring, confusion, curiosity, desire, disappointment, disapproval, disgust, embarrassment, excitement, fear, gratitude, grief, joy, love, nervousness, optimism, pride, realization, relief, remorse, sadness, surprise. This directory includes the data and code for data analysis scripts. We also include code for our baseline model, which involves fine-tuning a pre-trained BERT-base model. For more details on the design and content of the dataset, please see our paper. Refer to our GoEmotions Model Card for recommended uses of models built with this data, as well as considerations and limitations relating to the GoEmotions data. Requirements See requirements.txt Setup Download the pre-trained BERT model from here and unzip them inside the bert directory. In the paper, we use the cased base model. Data Our raw dataset can be retrieved by running:"
      ],
      [
        "Score: 0.94",
        "To this end, we compiled GoEmotions, the largest human annotated dataset of 58k carefully selected Reddit comments, labeled for 27 emotion categories or Neutral, with comments extracted from popular English subreddits. Table 1 shows an illustrative sample of our collected data. We design our emotion taxonomy considering related work in psychology and coverage in our data. In contrast to Ekman's taxonomy, which includes only one positive emotion (joy), our taxonomy includes a large number of positive, negative, and ambiguous emotion categories, making it suitable for downstream conversation understanding tasks that require a subtle understanding of emotion expression, such as the analysis of customer feedback or the enhancement of chatbots."
      ]
    ]
  },
  {
    "question": "Which experiments has been conducted?",
    "answers": [
      [
        "Score: 0.45",
        "We report the results of the finetuning experiments detailed below for each data size, with confidence intervals based on repeated experiments using the splits."
      ],
      [
        "Score: 0.22",
        "We conduct transfer learning experiments on existing emotion benchmarks, in order to show our data generalizes across domains and taxonomies. The goal is to demonstrate that given little labeled data in a target domain, one can utilize GoEmotions as baseline emotion understanding data. ity and taxonomy. In the interest of space, we only discuss three of these datasets here, chosen based on their diversity of domains. In our experiments, we observe similar trends for the additional benchmarks, and all are included in the Appendix H. The International Survey on Emotion Antecedents and Reactions (ISEAR) (Scherer and Wallbott, 1994 ) is a collection of personal reports on emotional events, written by 3000 people from different cultural backgrounds. The dataset contains 8k sentences, each labeled with a single emotion. The categories are anger, disgust, fear, guilt, joy, sadness and shame."
      ],
      [
        "Score: 0.09",
        "Recent advances in psychology have offered new conceptual and methodological approaches to capturing the more complex \"semantic space\" of emotion (Cowen et al., 2019a ) by studying the distribution of emotion responses to a diverse array of stimuli via computational techniques. Studies guided by these principles have identified 27 distinct varieties of emotional experience conveyed by short videos (Cowen and Keltner, 2017) , 13 by music (Cowen et al., in press) , 28 by facial expression (Cowen and Keltner, 2019) , 12 by speech prosody (Cowen et al., 2019b) , and 24 by nonverbal vocalization (Cowen et al., 2018) . In this work, we build on these methods and findings to devise our granular taxonomy for text-based emotion recognition and study the dimensionality of language-based emotion space."
      ],
      [
        "Score: 0.09",
        "We present GoEmotions, a large, manually annotated, carefully curated dataset for fine-grained emotion prediction. We provide a detailed data analysis, demonstrating the reliability of the annotations for the full taxonomy. We show the generalizability of the data across domains and taxonomies via transfer learning experiments. We build a strong baseline by fine-tuning a BERT model, however, the results suggest much room for future improvement. Future work can explore the cross-cultural robustness of emotion ratings, and extend the taxonomy to other languages and domains."
      ],
      [
        "Score: 0.07",
        "We conduct transfer learning experiments with existing emotion benchmarks to show that our data can generalize to different taxonomies and domains, such as tweets and personal narratives. Our experiments demonstrate that given limited resources to label additional emotion classification data for specialized domains, our data can provide baseline emotion understanding and contribute to increasing model accuracy for the target domain."
      ]
    ]
  },
  {
    "question": "Is there recommended uses for dataset?",
    "answers": [
      [
        "Score: 0.93",
        "We describe the experimental setup in Section 6.2, which we use across all datasets. We find that transfer learning helps in the case of all datasets, especially when there is limited training data. Interestingly, in the case of Crowd-Flower, which is known to be noisy (Bostan and Klinger, 2018) and Electoral Tweets, which is a small dataset of \u223c4k labeled examples and a large taxonomy of 36 emotions, FREEZE gives a significant boost of performance over the BASELINE and NOFREEZE for all training set sizes besides \"max\"."
      ],
      [
        "Score: 0.84",
        "The results in Figure 3 suggest that our dataset generalizes well to different domains and taxonomies, and that using a model using GoEmotions can help in cases when there is limited data from the target domain, or limited resources for labeling."
      ],
      [
        "Score: 0.81",
        "Data Disclaimer: We are aware that the dataset contains biases and is not representative of global diversity. We are aware that the dataset contains potentially problematic content. Potential biases in the data include: Inherent biases in Reddit and user base biases, the offensive/vulgar word lists used for data filtering, inherent or unconscious bias in assessment of offensive identity labels, annotators were all native English speakers from India. All these likely affect labeling, precision, and recall for a trained model. The emotion pilot model used for sentiment labeling, was trained on examples reviewed by the research team. Anyone using this dataset should be aware of these limitations of the dataset."
      ],
      [
        "Score: 0.80",
        "For the other datasets, we find that FREEZE tends to give a performance boost compared to the other setups only up to a couple of hundred training examples. For 500-1000 training examples, NOFREEZE tends to outperform the BASELINE, but we can see that these two setups come closer when there is more training data available. These results suggests that our dataset helps if there is limited data from the target domain."
      ],
      [
        "Score: 0.80",
        "We are aware that the dataset contains biases and is not representative of global diversity. We are aware that the dataset contains potentially problematic content. Potential biases in the data include: Inherent biases in Reddit and user base biases, the offensive/vulgar word lists used for data filtering, inherent or unconscious bias in assessment of offensive identity labels, annotators were all native English speakers from India. All these likely affect labelling, precision, and recall for a trained model. The emotion pilot model used for sentiment labeling, was trained on examples reviewed by the research team. Anyone using this dataset should be aware of these limitations of the dataset."
      ]
    ]
  },
  {
    "question": "Is there non-recommended uses for GoEmotions dataset?",
    "answers": [
      [
        "Score: 0.91",
        "The emotion categories are: admiration, amusement, anger, annoyance, approval, caring, confusion, curiosity, desire, disappointment, disapproval, disgust, embarrassment, excitement, fear, gratitude, grief, joy, love, nervousness, optimism, pride, realization, relief, remorse, sadness, surprise. This directory includes the data and code for data analysis scripts. We also include code for our baseline model, which involves fine-tuning a pre-trained BERT-base model. For more details on the design and content of the dataset, please see our paper. Refer to our GoEmotions Model Card for recommended uses of models built with this data, as well as considerations and limitations relating to the GoEmotions data. Requirements See requirements.txt Setup Download the pre-trained BERT model from here and unzip them inside the bert directory. In the paper, we use the cased base model. Data Our raw dataset can be retrieved by running:"
      ],
      [
        "Score: 0.89",
        "The results in Figure 3 suggest that our dataset generalizes well to different domains and taxonomies, and that using a model using GoEmotions can help in cases when there is limited data from the target domain, or limited resources for labeling."
      ],
      [
        "Score: 0.78",
        "We conduct transfer learning experiments on existing emotion benchmarks, in order to show our data generalizes across domains and taxonomies. The goal is to demonstrate that given little labeled data in a target domain, one can utilize GoEmotions as baseline emotion understanding data. ity and taxonomy. In the interest of space, we only discuss three of these datasets here, chosen based on their diversity of domains. In our experiments, we observe similar trends for the additional benchmarks, and all are included in the Appendix H. The International Survey on Emotion Antecedents and Reactions (ISEAR) (Scherer and Wallbott, 1994 ) is a collection of personal reports on emotional events, written by 3000 people from different cultural backgrounds. The dataset contains 8k sentences, each labeled with a single emotion. The categories are anger, disgust, fear, guilt, joy, sadness and shame."
      ],
      [
        "Score: 0.56",
        "We present GoEmotions, a large, manually annotated, carefully curated dataset for fine-grained emotion prediction. We provide a detailed data analysis, demonstrating the reliability of the annotations for the full taxonomy. We show the generalizability of the data across domains and taxonomies via transfer learning experiments. We build a strong baseline by fine-tuning a BERT model, however, the results suggest much room for future improvement. Future work can explore the cross-cultural robustness of emotion ratings, and extend the taxonomy to other languages and domains."
      ],
      [
        "Score: 0.52",
        "For the other datasets, we find that FREEZE tends to give a performance boost compared to the other setups only up to a couple of hundred training examples. For 500-1000 training examples, NOFREEZE tends to outperform the BASELINE, but we can see that these two setups come closer when there is more training data available. These results suggests that our dataset helps if there is limited data from the target domain."
      ]
    ]
  },
  {
    "question": "Has the dataset been used before?",
    "answers": [
      [
        "Score: 0.90",
        "We describe the experimental setup in Section 6.2, which we use across all datasets. We find that transfer learning helps in the case of all datasets, especially when there is limited training data. Interestingly, in the case of Crowd-Flower, which is known to be noisy (Bostan and Klinger, 2018) and Electoral Tweets, which is a small dataset of \u223c4k labeled examples and a large taxonomy of 36 emotions, FREEZE gives a significant boost of performance over the BASELINE and NOFREEZE for all training set sizes besides \"max\"."
      ],
      [
        "Score: 0.90",
        "Data Disclaimer: We are aware that the dataset contains biases and is not representative of global diversity. We are aware that the dataset contains potentially problematic content. Potential biases in the data include: Inherent biases in Reddit and user base biases, the offensive/vulgar word lists used for data filtering, inherent or unconscious bias in assessment of offensive identity labels, annotators were all native English speakers from India. All these likely affect labeling, precision, and recall for a trained model. The emotion pilot model used for sentiment labeling, was trained on examples reviewed by the research team. Anyone using this dataset should be aware of these limitations of the dataset."
      ],
      [
        "Score: 0.90",
        "We are aware that the dataset contains biases and is not representative of global diversity. We are aware that the dataset contains potentially problematic content. Potential biases in the data include: Inherent biases in Reddit and user base biases, the offensive/vulgar word lists used for data filtering, inherent or unconscious bias in assessment of offensive identity labels, annotators were all native English speakers from India. All these likely affect labelling, precision, and recall for a trained model. The emotion pilot model used for sentiment labeling, was trained on examples reviewed by the research team. Anyone using this dataset should be aware of these limitations of the dataset."
      ],
      [
        "Score: 0.86",
        "Ever since Affective Text (Strapparava and Mihalcea, 2007) , the first benchmark for emotion recognition was introduced, the field has seen several emotion datasets that vary in size, domain and taxonomy (cf. Bostan and Klinger, 2018) . The majority of emotion datasets are constructed manually, but tend to be relatively small. The largest manually labeled dataset is CrowdFlower (2016), with 39k labeled examples, which were found by Bostan and Klinger (2018) to be noisy in comparison with other emotion datasets. Other datasets are automatically weakly-labeled, based on emotion-related hashtags on Twitter (Wang et al., 2012; Abdul-Mageed and Ungar, 2017) . We build our dataset manually, making it the largest human annotated dataset, with multiple annotations per example for quality assurance."
      ],
      [
        "Score: 0.77",
        "Figure 7 shows the results for all 9 datasets that are downloadable and have categorical emotions in the Unified Dataset (Bostan and Klinger, 2018) . These datasets are DailyDialog (Li et al., 2017) , Emotion-Stimulus (Ghazi et al., 2015) , Affective Text (Strapparava and Mihalcea, 2007) , Crowd-Flower (CrowdFlower, 2016) , Electoral Tweets (Mohammad et al., 2015) , ISEAR (Scherer and Wallbott, 1994) , the Twitter Emotion Corpus (TEC) (Mohammad, 2012) , EmoInt (Mohammad et al., 2018) and the Stance Sentiment Emotion Corpus (SSEC) (Schuff et al., 2017) ."
      ]
    ]
  },
  {
    "question": "Is there any distribution policy for the dataset?",
    "answers": [
      [
        "Score: 0.40",
        "We are aware that the dataset contains biases and is not representative of global diversity. We are aware that the dataset contains potentially problematic content. Potential biases in the data include: Inherent biases in Reddit and user base biases, the offensive/vulgar word lists used for data filtering, inherent or unconscious bias in assessment of offensive identity labels, annotators were all native English speakers from India. All these likely affect labelling, precision, and recall for a trained model. The emotion pilot model used for sentiment labeling, was trained on examples reviewed by the research team. Anyone using this dataset should be aware of these limitations of the dataset."
      ],
      [
        "Score: 0.33",
        "For the other datasets, we find that FREEZE tends to give a performance boost compared to the other setups only up to a couple of hundred training examples. For 500-1000 training examples, NOFREEZE tends to outperform the BASELINE, but we can see that these two setups come closer when there is more training data available. These results suggests that our dataset helps if there is limited data from the target domain."
      ],
      [
        "Score: 0.32",
        "Data Disclaimer: We are aware that the dataset contains biases and is not representative of global diversity. We are aware that the dataset contains potentially problematic content. Potential biases in the data include: Inherent biases in Reddit and user base biases, the offensive/vulgar word lists used for data filtering, inherent or unconscious bias in assessment of offensive identity labels, annotators were all native English speakers from India. All these likely affect labeling, precision, and recall for a trained model. The emotion pilot model used for sentiment labeling, was trained on examples reviewed by the research team. Anyone using this dataset should be aware of these limitations of the dataset."
      ],
      [
        "Score: 0.30",
        "The results in Figure 3 suggest that our dataset generalizes well to different domains and taxonomies, and that using a model using GoEmotions can help in cases when there is limited data from the target domain, or limited resources for labeling."
      ],
      [
        "Score: 0.25",
        "One of the main aspects distinguishing our dataset is its emotion taxonomy. The vast majority of existing datasets contain annotations for minor variations of the 6 basic emotion categories (joy, anger, fear, sadness, disgust, and surprise) proposed by Ekman (1992a) and/or along affective dimensions (valence and arousal) that underpin the circumplex model of affect (Russell, 2003; Buechel and Hahn, 2017) ."
      ]
    ]
  },
  {
    "question": "How the dataset is distributed?",
    "answers": [
      [
        "Score: 0.66",
        "Our dataset is composed of 58K Reddit comments, labeled for one or more of 27 emotion(s) or Neutral."
      ],
      [
        "Score: 0.60",
        "In the past decade, NLP researchers made available several datasets for language-based emotion classification for a variety of domains and applications, including for news headlines (Strapparava and Mihalcea, 2007) , tweets (CrowdFlower, 2016; Mohammad et al., 2018) , and narrative sequences (Liu et al., 2019) , to name just a few. However, existing available datasets are (1) mostly small, containing up to several thousand instances, and (2) cover a limited emotion taxonomy, with coarse clas- sification into Ekman (Ekman, 1992b) or Plutchik (Plutchik, 1980) emotions."
      ],
      [
        "Score: 0.54",
        "Size of training dataset: 43,410. Size of test dataset: 5,427. Size of validation dataset: 5,426."
      ],
      [
        "Score: 0.53",
        "We describe the experimental setup in Section 6.2, which we use across all datasets. We find that transfer learning helps in the case of all datasets, especially when there is limited training data. Interestingly, in the case of Crowd-Flower, which is known to be noisy (Bostan and Klinger, 2018) and Electoral Tweets, which is a small dataset of \u223c4k labeled examples and a large taxonomy of 36 emotions, FREEZE gives a significant boost of performance over the BASELINE and NOFREEZE for all training set sizes besides \"max\"."
      ],
      [
        "Score: 0.52",
        "Training set size. We experiment with varying amount of training data from the target domain dataset, including 100, 200, 500, 1000, and 80% (named \"max\") of dataset examples. We generate 10 random splits for each train set size, with the remaining examples held as a test set."
      ]
    ]
  },
  {
    "question": "Who are the funders dataset?",
    "answers": [
      [
        "Score: 0.59",
        "Figure 7 shows the results for all 9 datasets that are downloadable and have categorical emotions in the Unified Dataset (Bostan and Klinger, 2018) . These datasets are DailyDialog (Li et al., 2017) , Emotion-Stimulus (Ghazi et al., 2015) , Affective Text (Strapparava and Mihalcea, 2007) , Crowd-Flower (CrowdFlower, 2016) , Electoral Tweets (Mohammad et al., 2015) , ISEAR (Scherer and Wallbott, 1994) , the Twitter Emotion Corpus (TEC) (Mohammad, 2012) , EmoInt (Mohammad et al., 2018) and the Stance Sentiment Emotion Corpus (SSEC) (Schuff et al., 2017) ."
      ],
      [
        "Score: 0.43",
        "Several existing datasets come from the domain of Twitter, given its informal language and expressive content, such as emojis and hashtags. Other datasets annotate news headlines (Strapparava and Mihalcea, 2007) , dialogs (Li et al., 2017) , fairytales (Alm et al., 2005) , movie subtitles ( \u00d6hman et al., 2018) , sentences based on FrameNet (Ghazi et al., 2015) , or self-reported experiences (Scherer and Wallbott, 1994) among other domains. We are the first to build on Reddit comments for emotion prediction."
      ],
      [
        "Score: 0.29",
        "In the past decade, NLP researchers made available several datasets for language-based emotion classification for a variety of domains and applications, including for news headlines (Strapparava and Mihalcea, 2007) , tweets (CrowdFlower, 2016; Mohammad et al., 2018) , and narrative sequences (Liu et al., 2019) , to name just a few. However, existing available datasets are (1) mostly small, containing up to several thousand instances, and (2) cover a limited emotion taxonomy, with coarse clas- sification into Ekman (Ekman, 1992b) or Plutchik (Plutchik, 1980) emotions."
      ],
      [
        "Score: 0.22",
        "We describe the experimental setup in Section 6.2, which we use across all datasets. We find that transfer learning helps in the case of all datasets, especially when there is limited training data. Interestingly, in the case of Crowd-Flower, which is known to be noisy (Bostan and Klinger, 2018) and Electoral Tweets, which is a small dataset of \u223c4k labeled examples and a large taxonomy of 36 emotions, FREEZE gives a significant boost of performance over the BASELINE and NOFREEZE for all training set sizes besides \"max\"."
      ],
      [
        "Score: 0.18",
        "Ever since Affective Text (Strapparava and Mihalcea, 2007) , the first benchmark for emotion recognition was introduced, the field has seen several emotion datasets that vary in size, domain and taxonomy (cf. Bostan and Klinger, 2018) . The majority of emotion datasets are constructed manually, but tend to be relatively small. The largest manually labeled dataset is CrowdFlower (2016), with 39k labeled examples, which were found by Bostan and Klinger (2018) to be noisy in comparison with other emotion datasets. Other datasets are automatically weakly-labeled, based on emotion-related hashtags on Twitter (Wang et al., 2012; Abdul-Mageed and Ungar, 2017) . We build our dataset manually, making it the largest human annotated dataset, with multiple annotations per example for quality assurance."
      ]
    ]
  },
  {
    "question": "Who is the maintainer of the GoEmotions dataset?",
    "answers": [
      [
        "Score: 0.95",
        "We conduct transfer learning experiments on existing emotion benchmarks, in order to show our data generalizes across domains and taxonomies. The goal is to demonstrate that given little labeled data in a target domain, one can utilize GoEmotions as baseline emotion understanding data. ity and taxonomy. In the interest of space, we only discuss three of these datasets here, chosen based on their diversity of domains. In our experiments, we observe similar trends for the additional benchmarks, and all are included in the Appendix H. The International Survey on Emotion Antecedents and Reactions (ISEAR) (Scherer and Wallbott, 1994 ) is a collection of personal reports on emotional events, written by 3000 people from different cultural backgrounds. The dataset contains 8k sentences, each labeled with a single emotion. The categories are anger, disgust, fear, guilt, joy, sadness and shame."
      ],
      [
        "Score: 0.85",
        "We present GoEmotions, a large, manually annotated, carefully curated dataset for fine-grained emotion prediction. We provide a detailed data analysis, demonstrating the reliability of the annotations for the full taxonomy. We show the generalizability of the data across domains and taxonomies via transfer learning experiments. We build a strong baseline by fine-tuning a BERT model, however, the results suggest much room for future improvement. Future work can explore the cross-cultural robustness of emotion ratings, and extend the taxonomy to other languages and domains."
      ],
      [
        "Score: 0.82",
        "The results in Figure 3 suggest that our dataset generalizes well to different domains and taxonomies, and that using a model using GoEmotions can help in cases when there is limited data from the target domain, or limited resources for labeling."
      ],
      [
        "Score: 0.76",
        "The emotion categories are: admiration, amusement, anger, annoyance, approval, caring, confusion, curiosity, desire, disappointment, disapproval, disgust, embarrassment, excitement, fear, gratitude, grief, joy, love, nervousness, optimism, pride, realization, relief, remorse, sadness, surprise. This directory includes the data and code for data analysis scripts. We also include code for our baseline model, which involves fine-tuning a pre-trained BERT-base model. For more details on the design and content of the dataset, please see our paper. Refer to our GoEmotions Model Card for recommended uses of models built with this data, as well as considerations and limitations relating to the GoEmotions data. Requirements See requirements.txt Setup Download the pre-trained BERT model from here and unzip them inside the bert directory. In the paper, we use the cased base model. Data Our raw dataset can be retrieved by running:"
      ],
      [
        "Score: 0.73",
        "wget -P data/full_dataset/ https://storage.googleapis.com/gresearch/goemotions/data/full_dataset/goemotions_1.csv wget -P data/full_dataset/ https://storage.googleapis.com/gresearch/goemotions/data/full_dataset/goemotions_2.csv wget -P data/full_dataset/ https://storage.googleapis.com/gresearch/goemotions/data/full_dataset/goemotions_3.csv"
      ]
    ]
  },
  {
    "question": "Is there any erratum?",
    "answers": [
      [
        "Score: 0.00",
        "While reviewing the results from the pilot rounds, we identified and removed emotions that were scarcely selected by annotators and/or had low interrater agreement due to being very similar to other emotions or being too difficult to detect from text. These emotions were boredom, doubt, heartbroken, indifference and calmness. We also identified and added those emotions to our taxonomy that were frequently suggested by raters and/or seemed to be represented in the data upon manual inspection. These emotions were desire, disappointment, pride, realization, relief and remorse. In this process, we also refined the category names (e.g. replacing ecstasy with excitement), to ones that seemed interpretable to annotators. This is how we arrived at the final set of 27 emotions + Neutral. Our high interrater agreement in the final data can be partially explained by the fact that we took interpretability into consideration while constructing the taxonomy. The dataset is we are releasing was labeled in the third round over the final taxonomy."
      ],
      [
        "Score: 0.00",
        "We find that all 27 PPCs are highly significant. Specifically, Bonferroni-corrected p-values are less than 1.5e-6 for all dimensions (corrected \u03b1 = 0.0017), suggesting that the emotions were highly dissociable. Such a high degree of significance for all dimensions is nontrivial. For example, Cowen et al. (2019b) find that only 12 out of their 30 emotion categories are significantly dissociable."
      ],
      [
        "Score: 0.00",
        "Even though we filter our data for the baseline experiments, we see particular value in the 4K examples that lack agreement. This subset of the data likely contains edge/difficult examples for the emotion domain (e.g., emotion-ambiguous text), and present challenges for further exploration. That is 66 ) agree ( 24 ) you ( 12 ) fuck ( 24 ) annoying ( 14 ) disappointing ( 11 ) not ( 16 ) confused ( 18 ) awesome ( 32 ) haha ( 32 ) not ( 13 ) worry ( 11 ) hate ( 18 ) stupid ( 13 ) disappointed (10) don't ( 14 ) why ( 11 ) amazing ( 30 ) funny ( 27 ) don't ( 12 ) careful ( 9 ) fucking ( 18 ) fucking ( 12 ) bad ( 9 ) disagree ( 9 ) sure ( 10 ) good ( 28 ) lmao ( 21 ) yes ( 12 ) stay ( 9 ) angry ( 11 ) shit ( 10 ) disappointment ( 7 ) nope ( 8 ) what ( 10 ) beautiful ( 23 ) hilarious ( 18 ) agreed ( 11 ) your ( 8 ) dare ( 10 ) dumb ( 9 ) unfortunately ( 7 ) doesn't ( 7 ) understand (8) desire excitement gratitude joy disgust embarrassment fear grief curiosity wish ( 29 ) excited ( 21 ) thanks ( 75 ) happy ( 32 ) disgusting ( 22 ) embarrassing ( 12 ) scared ( 16 ) died ( 6 ) curious ( 22 ) want ( 8 ) happy ( 8 ) thank ( 69 ) glad ( 27 ) awful ( 14 ) shame ( 11 ) afraid ( 16 ) rip ( 4 ) what ( 18 ) wanted ( 6 ) cake ( 8 ) for ( 24 ) enjoy ( 20 ) worst ( 13 ) awkward ( 10 ) scary ( 15 ) why ( 13 ) could ( 6 ) wow ( 8 ) you ( 18 ) enjoyed ( 12 ) worse ( 12 ) embarrassment ( 8 ) terrible ( 12 ) how ( 11 ) ambitious ( 4 ) interesting ( 7 ) sharing ( 17 ) fun ( 12 ) weird ( 9 ) embarrassed ( 7 ) terrifying ( 11 ) did (10) love optimism pride relief nervousness remorse sadness realization surprise love ( 76 ) hope ( 45 ) proud ( 14 ) glad ( 5 ) nervous ( 8 ) sorry ( 39 ) sad ( 31 ) realize ( 14 ) wow ( 23 ) loved ( 21 ) hopefully ( 19 ) pride ( 4 ) relieved ( 4 ) worried ( 8 ) regret ( 9 ) sadly ( 16 ) realized ( 12 ) surprised ( 21 ) favorite ( 13 ) luck (18) accomplishment relieving (4) anxiety ( 6 ) apologies ( 7 ) sorry ( 15 ) realised ( 7 ) wonder ( 15 ) loves ( 12 ) hoping ( 16 ) (4) relief ( 4 ) anxious ( 4 ) apologize ( 6 ) painful ( 10 ) realization ( 6 ) shocked ( 12 ) like ( 9 ) will ( 8 ) worrying ( 4 ) guilt ( 5 ) crying ( 9 ) thought ( 6 ) omg ( 11 )"
      ],
      [
        "Score: 0.00",
        "Finding something impressive or worthy of respect."
      ],
      [
        "Score: 0.00",
        "The results in Figure 3 suggest that our dataset generalizes well to different domains and taxonomies, and that using a model using GoEmotions can help in cases when there is limited data from the target domain, or limited resources for labeling."
      ]
    ]
  },
  {
    "question": "Which are the updating policies of GoEmotions dataset?",
    "answers": [
      [
        "Score: 0.90",
        "We conduct transfer learning experiments on existing emotion benchmarks, in order to show our data generalizes across domains and taxonomies. The goal is to demonstrate that given little labeled data in a target domain, one can utilize GoEmotions as baseline emotion understanding data. ity and taxonomy. In the interest of space, we only discuss three of these datasets here, chosen based on their diversity of domains. In our experiments, we observe similar trends for the additional benchmarks, and all are included in the Appendix H. The International Survey on Emotion Antecedents and Reactions (ISEAR) (Scherer and Wallbott, 1994 ) is a collection of personal reports on emotional events, written by 3000 people from different cultural backgrounds. The dataset contains 8k sentences, each labeled with a single emotion. The categories are anger, disgust, fear, guilt, joy, sadness and shame."
      ],
      [
        "Score: 0.85",
        "The emotion categories are: admiration, amusement, anger, annoyance, approval, caring, confusion, curiosity, desire, disappointment, disapproval, disgust, embarrassment, excitement, fear, gratitude, grief, joy, love, nervousness, optimism, pride, realization, relief, remorse, sadness, surprise. This directory includes the data and code for data analysis scripts. We also include code for our baseline model, which involves fine-tuning a pre-trained BERT-base model. For more details on the design and content of the dataset, please see our paper. Refer to our GoEmotions Model Card for recommended uses of models built with this data, as well as considerations and limitations relating to the GoEmotions data. Requirements See requirements.txt Setup Download the pre-trained BERT model from here and unzip them inside the bert directory. In the paper, we use the cased base model. Data Our raw dataset can be retrieved by running:"
      ],
      [
        "Score: 0.73",
        "We present GoEmotions, a large, manually annotated, carefully curated dataset for fine-grained emotion prediction. We provide a detailed data analysis, demonstrating the reliability of the annotations for the full taxonomy. We show the generalizability of the data across domains and taxonomies via transfer learning experiments. We build a strong baseline by fine-tuning a BERT model, however, the results suggest much room for future improvement. Future work can explore the cross-cultural robustness of emotion ratings, and extend the taxonomy to other languages and domains."
      ],
      [
        "Score: 0.73",
        "The results in Figure 3 suggest that our dataset generalizes well to different domains and taxonomies, and that using a model using GoEmotions can help in cases when there is limited data from the target domain, or limited resources for labeling."
      ],
      [
        "Score: 0.70",
        "To this end, we compiled GoEmotions, the largest human annotated dataset of 58k carefully selected Reddit comments, labeled for 27 emotion categories or Neutral, with comments extracted from popular English subreddits. Table 1 shows an illustrative sample of our collected data. We design our emotion taxonomy considering related work in psychology and coverage in our data. In contrast to Ekman's taxonomy, which includes only one positive emotion (joy), our taxonomy includes a large number of positive, negative, and ambiguous emotion categories, making it suitable for downstream conversation understanding tasks that require a subtle understanding of emotion expression, such as the analysis of customer feedback or the enhancement of chatbots."
      ]
    ]
  },
  {
    "question": "Which are the contributing guidelines?",
    "answers": [
      [
        "Score: 0.00",
        "The results in Figure 3 suggest that our dataset generalizes well to different domains and taxonomies, and that using a model using GoEmotions can help in cases when there is limited data from the target domain, or limited resources for labeling."
      ],
      [
        "Score: 0.00",
        "Reddit is known for a demographic bias leaning towards young male users (Duggan and Smith, 2013) , which is not reflective of a globally diverse population. The platform also introduces a skew towards toxic, offensive language (Mohan et al., 2017) . Thus, Reddit content has been used to study depression (Pirina and C \u00b8\u00f6ltekin, 2018) , microaggressions (Breitfeller et al., 2019) , and Yanardag and Rahwan (2018) have shown the effect of using biased Reddit data by training a \"psychopath\" bot. To address these concerns, and enable building broadly representative emotion models using GoEmotions, we take a series of data curation measures to ensure our data does not reinforce general, nor emotion-specific, language biases. We identify harmful comments using pre-defined lists containing offensive/adult, vulgar (mildly offensive profanity), identity, and religion terms (included as supplementary material). These are used for data filtering and masking, as described below. Lists were internally compiled and we believe they are comprehensive and widely useful for dataset curation, however, they may not be complete."
      ],
      [
        "Score: 0.00",
        "On top of the raw data, we also include a version filtered based on reter-agreement, which contains a train/test/validation split:"
      ],
      [
        "Score: 0.00",
        "To better understand agreement among raters and the latent structure of the emotion space, we apply Principal Preserved Component Analysis (PPCA) (Cowen et al., 2019b) to our data. PPCA extracts linear combinations of attributes (here, emotion judgments), that maximally covary across two sets of data that measure the same attributes (here, randomly split judgments for each example). Thus, PPCA allows us to uncover latent dimensions of J \u2208 R n\u00d7|R|\u00d7|E| \u2190 all ratings for the examples annotated by r 7:"
      ],
      [
        "Score: 0.00",
        "Recently, Bostan and Klinger (2018) have aggregated 14 popular emotion classification corpora under a unified framework that allows direct comparison of the existing resources. Importantly, their analysis suggests annotation quality gaps in the largest manually annotated emotion classification dataset, CrowdFlower (2016), containing 40K tweets labeled for one of 13 emotions. While their work enables such comparative evaluations, it highlights the need for a large-scale, consistently labeled emotion dataset over a fine-grained taxonomy, with demonstrated high-quality annotations."
      ]
    ]
  },
  {
    "question": "Which are the composition of GoEmotions dataset?",
    "answers": [
      [
        "Score: 0.90",
        "The emotion categories are: admiration, amusement, anger, annoyance, approval, caring, confusion, curiosity, desire, disappointment, disapproval, disgust, embarrassment, excitement, fear, gratitude, grief, joy, love, nervousness, optimism, pride, realization, relief, remorse, sadness, surprise. This directory includes the data and code for data analysis scripts. We also include code for our baseline model, which involves fine-tuning a pre-trained BERT-base model. For more details on the design and content of the dataset, please see our paper. Refer to our GoEmotions Model Card for recommended uses of models built with this data, as well as considerations and limitations relating to the GoEmotions data. Requirements See requirements.txt Setup Download the pre-trained BERT model from here and unzip them inside the bert directory. In the paper, we use the cased base model. Data Our raw dataset can be retrieved by running:"
      ],
      [
        "Score: 0.89",
        "We present GoEmotions, a large, manually annotated, carefully curated dataset for fine-grained emotion prediction. We provide a detailed data analysis, demonstrating the reliability of the annotations for the full taxonomy. We show the generalizability of the data across domains and taxonomies via transfer learning experiments. We build a strong baseline by fine-tuning a BERT model, however, the results suggest much room for future improvement. Future work can explore the cross-cultural robustness of emotion ratings, and extend the taxonomy to other languages and domains."
      ],
      [
        "Score: 0.87",
        "We conduct transfer learning experiments on existing emotion benchmarks, in order to show our data generalizes across domains and taxonomies. The goal is to demonstrate that given little labeled data in a target domain, one can utilize GoEmotions as baseline emotion understanding data. ity and taxonomy. In the interest of space, we only discuss three of these datasets here, chosen based on their diversity of domains. In our experiments, we observe similar trends for the additional benchmarks, and all are included in the Appendix H. The International Survey on Emotion Antecedents and Reactions (ISEAR) (Scherer and Wallbott, 1994 ) is a collection of personal reports on emotional events, written by 3000 people from different cultural backgrounds. The dataset contains 8k sentences, each labeled with a single emotion. The categories are anger, disgust, fear, guilt, joy, sadness and shame."
      ],
      [
        "Score: 0.87",
        "Our dataset is composed of 58K Reddit comments, labeled for one or more of 27 emotion(s) or Neutral."
      ],
      [
        "Score: 0.86",
        "description: ### Context GoEmotions is a corpus of 58k carefully curated comments extracted from Reddit, with human annotations to 27 emotion categories or Neutral.  - Number of examples: 58,009. - Number of labels: 27 + Neutral. - Maximum sequence length in training and evaluation datasets: 30.   ### Content On top of the raw data, we also include a version filtered based on reter-agreement, which contains a train/test/validation split:  - Size of training dataset: 43,410. - Size of test dataset: 5,427. - Size of validation dataset: 5,426. The emotion categories are: admiration, amusement, anger, annoyance, approval, caring, confusion, curiosity, desire, disappointment, disapproval, disgust, embarrassment, excitement, fear, gratitude, grief, joy, love, nervousness, optimism, pride, realization, relief, remorse, sadness, surprise.  `analyze_data.py` and `extract_words.py` have already been run and the output files are stored in `plots` and `tables` respectively. What's inside is more than just rows and columns. Make it easy for others to get started by describing how you acquired the data and what time period it represents, too. Original repository from where the data is taken: [https://github.com/google-research/google-research/tree/master/goemotions](https://github.com/google-research/google-research/tree/master/goemotions) However, some of the scripts were giving errors which have been rectified and updated in this repository: [https://github.com/DebarshiChanda/google-research/tree/master/goemotions](https://github.com/DebarshiChanda/google-research/tree/master/goemotions)  ### Acknowledgements The entire Credit for creating this dataset goes to Google Research Team.   ### Inspiration Detect emotion from the text using Multi-label Classification"
      ]
    ]
  },
  {
    "question": "Which type of data GoEmotions is composed?",
    "answers": [
      [
        "Score: 0.83",
        "The emotion categories are: admiration, amusement, anger, annoyance, approval, caring, confusion, curiosity, desire, disappointment, disapproval, disgust, embarrassment, excitement, fear, gratitude, grief, joy, love, nervousness, optimism, pride, realization, relief, remorse, sadness, surprise. This directory includes the data and code for data analysis scripts. We also include code for our baseline model, which involves fine-tuning a pre-trained BERT-base model. For more details on the design and content of the dataset, please see our paper. Refer to our GoEmotions Model Card for recommended uses of models built with this data, as well as considerations and limitations relating to the GoEmotions data. Requirements See requirements.txt Setup Download the pre-trained BERT model from here and unzip them inside the bert directory. In the paper, we use the cased base model. Data Our raw dataset can be retrieved by running:"
      ],
      [
        "Score: 0.82",
        "To this end, we compiled GoEmotions, the largest human annotated dataset of 58k carefully selected Reddit comments, labeled for 27 emotion categories or Neutral, with comments extracted from popular English subreddits. Table 1 shows an illustrative sample of our collected data. We design our emotion taxonomy considering related work in psychology and coverage in our data. In contrast to Ekman's taxonomy, which includes only one positive emotion (joy), our taxonomy includes a large number of positive, negative, and ambiguous emotion categories, making it suitable for downstream conversation understanding tasks that require a subtle understanding of emotion expression, such as the analysis of customer feedback or the enhancement of chatbots."
      ],
      [
        "Score: 0.78",
        "We present GoEmotions, a large, manually annotated, carefully curated dataset for fine-grained emotion prediction. We provide a detailed data analysis, demonstrating the reliability of the annotations for the full taxonomy. We show the generalizability of the data across domains and taxonomies via transfer learning experiments. We build a strong baseline by fine-tuning a BERT model, however, the results suggest much room for future improvement. Future work can explore the cross-cultural robustness of emotion ratings, and extend the taxonomy to other languages and domains."
      ],
      [
        "Score: 0.78",
        "We conduct transfer learning experiments on existing emotion benchmarks, in order to show our data generalizes across domains and taxonomies. The goal is to demonstrate that given little labeled data in a target domain, one can utilize GoEmotions as baseline emotion understanding data. ity and taxonomy. In the interest of space, we only discuss three of these datasets here, chosen based on their diversity of domains. In our experiments, we observe similar trends for the additional benchmarks, and all are included in the Appendix H. The International Survey on Emotion Antecedents and Reactions (ISEAR) (Scherer and Wallbott, 1994 ) is a collection of personal reports on emotional events, written by 3000 people from different cultural backgrounds. The dataset contains 8k sentences, each labeled with a single emotion. The categories are anger, disgust, fear, guilt, joy, sadness and shame."
      ],
      [
        "Score: 0.78",
        "description: ### Context GoEmotions is a corpus of 58k carefully curated comments extracted from Reddit, with human annotations to 27 emotion categories or Neutral.  - Number of examples: 58,009. - Number of labels: 27 + Neutral. - Maximum sequence length in training and evaluation datasets: 30.   ### Content On top of the raw data, we also include a version filtered based on reter-agreement, which contains a train/test/validation split:  - Size of training dataset: 43,410. - Size of test dataset: 5,427. - Size of validation dataset: 5,426. The emotion categories are: admiration, amusement, anger, annoyance, approval, caring, confusion, curiosity, desire, disappointment, disapproval, disgust, embarrassment, excitement, fear, gratitude, grief, joy, love, nervousness, optimism, pride, realization, relief, remorse, sadness, surprise.  `analyze_data.py` and `extract_words.py` have already been run and the output files are stored in `plots` and `tables` respectively. What's inside is more than just rows and columns. Make it easy for others to get started by describing how you acquired the data and what time period it represents, too. Original repository from where the data is taken: [https://github.com/google-research/google-research/tree/master/goemotions](https://github.com/google-research/google-research/tree/master/goemotions) However, some of the scripts were giving errors which have been rectified and updated in this repository: [https://github.com/DebarshiChanda/google-research/tree/master/goemotions](https://github.com/DebarshiChanda/google-research/tree/master/goemotions)  ### Acknowledgements The entire Credit for creating this dataset goes to Google Research Team.   ### Inspiration Detect emotion from the text using Multi-label Classification"
      ]
    ]
  },
  {
    "question": "How has been GoEmotions data collected?",
    "answers": [
      [
        "Score: 0.96",
        "To this end, we compiled GoEmotions, the largest human annotated dataset of 58k carefully selected Reddit comments, labeled for 27 emotion categories or Neutral, with comments extracted from popular English subreddits. Table 1 shows an illustrative sample of our collected data. We design our emotion taxonomy considering related work in psychology and coverage in our data. In contrast to Ekman's taxonomy, which includes only one positive emotion (joy), our taxonomy includes a large number of positive, negative, and ambiguous emotion categories, making it suitable for downstream conversation understanding tasks that require a subtle understanding of emotion expression, such as the analysis of customer feedback or the enhancement of chatbots."
      ],
      [
        "Score: 0.90",
        "We conduct transfer learning experiments on existing emotion benchmarks, in order to show our data generalizes across domains and taxonomies. The goal is to demonstrate that given little labeled data in a target domain, one can utilize GoEmotions as baseline emotion understanding data. ity and taxonomy. In the interest of space, we only discuss three of these datasets here, chosen based on their diversity of domains. In our experiments, we observe similar trends for the additional benchmarks, and all are included in the Appendix H. The International Survey on Emotion Antecedents and Reactions (ISEAR) (Scherer and Wallbott, 1994 ) is a collection of personal reports on emotional events, written by 3000 people from different cultural backgrounds. The dataset contains 8k sentences, each labeled with a single emotion. The categories are anger, disgust, fear, guilt, joy, sadness and shame."
      ],
      [
        "Score: 0.89",
        "We present GoEmotions, a large, manually annotated, carefully curated dataset for fine-grained emotion prediction. We provide a detailed data analysis, demonstrating the reliability of the annotations for the full taxonomy. We show the generalizability of the data across domains and taxonomies via transfer learning experiments. We build a strong baseline by fine-tuning a BERT model, however, the results suggest much room for future improvement. Future work can explore the cross-cultural robustness of emotion ratings, and extend the taxonomy to other languages and domains."
      ],
      [
        "Score: 0.85",
        "The results in Figure 3 suggest that our dataset generalizes well to different domains and taxonomies, and that using a model using GoEmotions can help in cases when there is limited data from the target domain, or limited resources for labeling."
      ],
      [
        "Score: 0.83",
        "description: ### Context GoEmotions is a corpus of 58k carefully curated comments extracted from Reddit, with human annotations to 27 emotion categories or Neutral.  - Number of examples: 58,009. - Number of labels: 27 + Neutral. - Maximum sequence length in training and evaluation datasets: 30.   ### Content On top of the raw data, we also include a version filtered based on reter-agreement, which contains a train/test/validation split:  - Size of training dataset: 43,410. - Size of test dataset: 5,427. - Size of validation dataset: 5,426. The emotion categories are: admiration, amusement, anger, annoyance, approval, caring, confusion, curiosity, desire, disappointment, disapproval, disgust, embarrassment, excitement, fear, gratitude, grief, joy, love, nervousness, optimism, pride, realization, relief, remorse, sadness, surprise.  `analyze_data.py` and `extract_words.py` have already been run and the output files are stored in `plots` and `tables` respectively. What's inside is more than just rows and columns. Make it easy for others to get started by describing how you acquired the data and what time period it represents, too. Original repository from where the data is taken: [https://github.com/google-research/google-research/tree/master/goemotions](https://github.com/google-research/google-research/tree/master/goemotions) However, some of the scripts were giving errors which have been rectified and updated in this repository: [https://github.com/DebarshiChanda/google-research/tree/master/goemotions](https://github.com/DebarshiChanda/google-research/tree/master/goemotions)  ### Acknowledgements The entire Credit for creating this dataset goes to Google Research Team.   ### Inspiration Detect emotion from the text using Multi-label Classification"
      ]
    ]
  },
  {
    "question": "How has been GoEmotions data gathered?",
    "answers": [
      [
        "Score: 0.85",
        "We conduct transfer learning experiments on existing emotion benchmarks, in order to show our data generalizes across domains and taxonomies. The goal is to demonstrate that given little labeled data in a target domain, one can utilize GoEmotions as baseline emotion understanding data. ity and taxonomy. In the interest of space, we only discuss three of these datasets here, chosen based on their diversity of domains. In our experiments, we observe similar trends for the additional benchmarks, and all are included in the Appendix H. The International Survey on Emotion Antecedents and Reactions (ISEAR) (Scherer and Wallbott, 1994 ) is a collection of personal reports on emotional events, written by 3000 people from different cultural backgrounds. The dataset contains 8k sentences, each labeled with a single emotion. The categories are anger, disgust, fear, guilt, joy, sadness and shame."
      ],
      [
        "Score: 0.85",
        "The results in Figure 3 suggest that our dataset generalizes well to different domains and taxonomies, and that using a model using GoEmotions can help in cases when there is limited data from the target domain, or limited resources for labeling."
      ],
      [
        "Score: 0.83",
        "We present GoEmotions, a large, manually annotated, carefully curated dataset for fine-grained emotion prediction. We provide a detailed data analysis, demonstrating the reliability of the annotations for the full taxonomy. We show the generalizability of the data across domains and taxonomies via transfer learning experiments. We build a strong baseline by fine-tuning a BERT model, however, the results suggest much room for future improvement. Future work can explore the cross-cultural robustness of emotion ratings, and extend the taxonomy to other languages and domains."
      ],
      [
        "Score: 0.73",
        "description: ### Context GoEmotions is a corpus of 58k carefully curated comments extracted from Reddit, with human annotations to 27 emotion categories or Neutral.  - Number of examples: 58,009. - Number of labels: 27 + Neutral. - Maximum sequence length in training and evaluation datasets: 30.   ### Content On top of the raw data, we also include a version filtered based on reter-agreement, which contains a train/test/validation split:  - Size of training dataset: 43,410. - Size of test dataset: 5,427. - Size of validation dataset: 5,426. The emotion categories are: admiration, amusement, anger, annoyance, approval, caring, confusion, curiosity, desire, disappointment, disapproval, disgust, embarrassment, excitement, fear, gratitude, grief, joy, love, nervousness, optimism, pride, realization, relief, remorse, sadness, surprise.  `analyze_data.py` and `extract_words.py` have already been run and the output files are stored in `plots` and `tables` respectively. What's inside is more than just rows and columns. Make it easy for others to get started by describing how you acquired the data and what time period it represents, too. Original repository from where the data is taken: [https://github.com/google-research/google-research/tree/master/goemotions](https://github.com/google-research/google-research/tree/master/goemotions) However, some of the scripts were giving errors which have been rectified and updated in this repository: [https://github.com/DebarshiChanda/google-research/tree/master/goemotions](https://github.com/DebarshiChanda/google-research/tree/master/goemotions)  ### Acknowledgements The entire Credit for creating this dataset goes to Google Research Team.   ### Inspiration Detect emotion from the text using Multi-label Classification"
      ],
      [
        "Score: 0.69",
        "The emotion categories are: admiration, amusement, anger, annoyance, approval, caring, confusion, curiosity, desire, disappointment, disapproval, disgust, embarrassment, excitement, fear, gratitude, grief, joy, love, nervousness, optimism, pride, realization, relief, remorse, sadness, surprise. This directory includes the data and code for data analysis scripts. We also include code for our baseline model, which involves fine-tuning a pre-trained BERT-base model. For more details on the design and content of the dataset, please see our paper. Refer to our GoEmotions Model Card for recommended uses of models built with this data, as well as considerations and limitations relating to the GoEmotions data. Requirements See requirements.txt Setup Download the pre-trained BERT model from here and unzip them inside the bert directory. In the paper, we use the cased base model. Data Our raw dataset can be retrieved by running:"
      ]
    ]
  },
  {
    "question": "How has been GoEmotions data labeled?",
    "answers": [
      [
        "Score: 0.95",
        "We conduct transfer learning experiments on existing emotion benchmarks, in order to show our data generalizes across domains and taxonomies. The goal is to demonstrate that given little labeled data in a target domain, one can utilize GoEmotions as baseline emotion understanding data. ity and taxonomy. In the interest of space, we only discuss three of these datasets here, chosen based on their diversity of domains. In our experiments, we observe similar trends for the additional benchmarks, and all are included in the Appendix H. The International Survey on Emotion Antecedents and Reactions (ISEAR) (Scherer and Wallbott, 1994 ) is a collection of personal reports on emotional events, written by 3000 people from different cultural backgrounds. The dataset contains 8k sentences, each labeled with a single emotion. The categories are anger, disgust, fear, guilt, joy, sadness and shame."
      ],
      [
        "Score: 0.92",
        "To this end, we compiled GoEmotions, the largest human annotated dataset of 58k carefully selected Reddit comments, labeled for 27 emotion categories or Neutral, with comments extracted from popular English subreddits. Table 1 shows an illustrative sample of our collected data. We design our emotion taxonomy considering related work in psychology and coverage in our data. In contrast to Ekman's taxonomy, which includes only one positive emotion (joy), our taxonomy includes a large number of positive, negative, and ambiguous emotion categories, making it suitable for downstream conversation understanding tasks that require a subtle understanding of emotion expression, such as the analysis of customer feedback or the enhancement of chatbots."
      ],
      [
        "Score: 0.89",
        "The results in Figure 3 suggest that our dataset generalizes well to different domains and taxonomies, and that using a model using GoEmotions can help in cases when there is limited data from the target domain, or limited resources for labeling."
      ],
      [
        "Score: 0.83",
        "We present GoEmotions, a large, manually annotated, carefully curated dataset for fine-grained emotion prediction. We provide a detailed data analysis, demonstrating the reliability of the annotations for the full taxonomy. We show the generalizability of the data across domains and taxonomies via transfer learning experiments. We build a strong baseline by fine-tuning a BERT model, however, the results suggest much room for future improvement. Future work can explore the cross-cultural robustness of emotion ratings, and extend the taxonomy to other languages and domains."
      ],
      [
        "Score: 0.77",
        "description: ### Context GoEmotions is a corpus of 58k carefully curated comments extracted from Reddit, with human annotations to 27 emotion categories or Neutral.  - Number of examples: 58,009. - Number of labels: 27 + Neutral. - Maximum sequence length in training and evaluation datasets: 30.   ### Content On top of the raw data, we also include a version filtered based on reter-agreement, which contains a train/test/validation split:  - Size of training dataset: 43,410. - Size of test dataset: 5,427. - Size of validation dataset: 5,426. The emotion categories are: admiration, amusement, anger, annoyance, approval, caring, confusion, curiosity, desire, disappointment, disapproval, disgust, embarrassment, excitement, fear, gratitude, grief, joy, love, nervousness, optimism, pride, realization, relief, remorse, sadness, surprise.  `analyze_data.py` and `extract_words.py` have already been run and the output files are stored in `plots` and `tables` respectively. What's inside is more than just rows and columns. Make it easy for others to get started by describing how you acquired the data and what time period it represents, too. Original repository from where the data is taken: [https://github.com/google-research/google-research/tree/master/goemotions](https://github.com/google-research/google-research/tree/master/goemotions) However, some of the scripts were giving errors which have been rectified and updated in this repository: [https://github.com/DebarshiChanda/google-research/tree/master/goemotions](https://github.com/DebarshiChanda/google-research/tree/master/goemotions)  ### Acknowledgements The entire Credit for creating this dataset goes to Google Research Team.   ### Inspiration Detect emotion from the text using Multi-label Classification"
      ]
    ]
  },
  {
    "question": "How has been GoEmotions data selected?",
    "answers": [
      [
        "Score: 0.93",
        "To this end, we compiled GoEmotions, the largest human annotated dataset of 58k carefully selected Reddit comments, labeled for 27 emotion categories or Neutral, with comments extracted from popular English subreddits. Table 1 shows an illustrative sample of our collected data. We design our emotion taxonomy considering related work in psychology and coverage in our data. In contrast to Ekman's taxonomy, which includes only one positive emotion (joy), our taxonomy includes a large number of positive, negative, and ambiguous emotion categories, making it suitable for downstream conversation understanding tasks that require a subtle understanding of emotion expression, such as the analysis of customer feedback or the enhancement of chatbots."
      ],
      [
        "Score: 0.92",
        "We conduct transfer learning experiments on existing emotion benchmarks, in order to show our data generalizes across domains and taxonomies. The goal is to demonstrate that given little labeled data in a target domain, one can utilize GoEmotions as baseline emotion understanding data. ity and taxonomy. In the interest of space, we only discuss three of these datasets here, chosen based on their diversity of domains. In our experiments, we observe similar trends for the additional benchmarks, and all are included in the Appendix H. The International Survey on Emotion Antecedents and Reactions (ISEAR) (Scherer and Wallbott, 1994 ) is a collection of personal reports on emotional events, written by 3000 people from different cultural backgrounds. The dataset contains 8k sentences, each labeled with a single emotion. The categories are anger, disgust, fear, guilt, joy, sadness and shame."
      ],
      [
        "Score: 0.89",
        "We present GoEmotions, a large, manually annotated, carefully curated dataset for fine-grained emotion prediction. We provide a detailed data analysis, demonstrating the reliability of the annotations for the full taxonomy. We show the generalizability of the data across domains and taxonomies via transfer learning experiments. We build a strong baseline by fine-tuning a BERT model, however, the results suggest much room for future improvement. Future work can explore the cross-cultural robustness of emotion ratings, and extend the taxonomy to other languages and domains."
      ],
      [
        "Score: 0.87",
        "The results in Figure 3 suggest that our dataset generalizes well to different domains and taxonomies, and that using a model using GoEmotions can help in cases when there is limited data from the target domain, or limited resources for labeling."
      ],
      [
        "Score: 0.74",
        "description: ### Context GoEmotions is a corpus of 58k carefully curated comments extracted from Reddit, with human annotations to 27 emotion categories or Neutral.  - Number of examples: 58,009. - Number of labels: 27 + Neutral. - Maximum sequence length in training and evaluation datasets: 30.   ### Content On top of the raw data, we also include a version filtered based on reter-agreement, which contains a train/test/validation split:  - Size of training dataset: 43,410. - Size of test dataset: 5,427. - Size of validation dataset: 5,426. The emotion categories are: admiration, amusement, anger, annoyance, approval, caring, confusion, curiosity, desire, disappointment, disapproval, disgust, embarrassment, excitement, fear, gratitude, grief, joy, love, nervousness, optimism, pride, realization, relief, remorse, sadness, surprise.  `analyze_data.py` and `extract_words.py` have already been run and the output files are stored in `plots` and `tables` respectively. What's inside is more than just rows and columns. Make it easy for others to get started by describing how you acquired the data and what time period it represents, too. Original repository from where the data is taken: [https://github.com/google-research/google-research/tree/master/goemotions](https://github.com/google-research/google-research/tree/master/goemotions) However, some of the scripts were giving errors which have been rectified and updated in this repository: [https://github.com/DebarshiChanda/google-research/tree/master/goemotions](https://github.com/DebarshiChanda/google-research/tree/master/goemotions)  ### Acknowledgements The entire Credit for creating this dataset goes to Google Research Team.   ### Inspiration Detect emotion from the text using Multi-label Classification"
      ]
    ]
  },
  {
    "question": "How was the process for selecting GoEmotions data?",
    "answers": [
      [
        "Score: 0.82",
        "We present GoEmotions, a large, manually annotated, carefully curated dataset for fine-grained emotion prediction. We provide a detailed data analysis, demonstrating the reliability of the annotations for the full taxonomy. We show the generalizability of the data across domains and taxonomies via transfer learning experiments. We build a strong baseline by fine-tuning a BERT model, however, the results suggest much room for future improvement. Future work can explore the cross-cultural robustness of emotion ratings, and extend the taxonomy to other languages and domains."
      ],
      [
        "Score: 0.77",
        "We conduct transfer learning experiments on existing emotion benchmarks, in order to show our data generalizes across domains and taxonomies. The goal is to demonstrate that given little labeled data in a target domain, one can utilize GoEmotions as baseline emotion understanding data. ity and taxonomy. In the interest of space, we only discuss three of these datasets here, chosen based on their diversity of domains. In our experiments, we observe similar trends for the additional benchmarks, and all are included in the Appendix H. The International Survey on Emotion Antecedents and Reactions (ISEAR) (Scherer and Wallbott, 1994 ) is a collection of personal reports on emotional events, written by 3000 people from different cultural backgrounds. The dataset contains 8k sentences, each labeled with a single emotion. The categories are anger, disgust, fear, guilt, joy, sadness and shame."
      ],
      [
        "Score: 0.69",
        "The results in Figure 3 suggest that our dataset generalizes well to different domains and taxonomies, and that using a model using GoEmotions can help in cases when there is limited data from the target domain, or limited resources for labeling."
      ],
      [
        "Score: 0.45",
        "To this end, we compiled GoEmotions, the largest human annotated dataset of 58k carefully selected Reddit comments, labeled for 27 emotion categories or Neutral, with comments extracted from popular English subreddits. Table 1 shows an illustrative sample of our collected data. We design our emotion taxonomy considering related work in psychology and coverage in our data. In contrast to Ekman's taxonomy, which includes only one positive emotion (joy), our taxonomy includes a large number of positive, negative, and ambiguous emotion categories, making it suitable for downstream conversation understanding tasks that require a subtle understanding of emotion expression, such as the analysis of customer feedback or the enhancement of chatbots."
      ],
      [
        "Score: 0.39",
        "description: ### Context GoEmotions is a corpus of 58k carefully curated comments extracted from Reddit, with human annotations to 27 emotion categories or Neutral.  - Number of examples: 58,009. - Number of labels: 27 + Neutral. - Maximum sequence length in training and evaluation datasets: 30.   ### Content On top of the raw data, we also include a version filtered based on reter-agreement, which contains a train/test/validation split:  - Size of training dataset: 43,410. - Size of test dataset: 5,427. - Size of validation dataset: 5,426. The emotion categories are: admiration, amusement, anger, annoyance, approval, caring, confusion, curiosity, desire, disappointment, disapproval, disgust, embarrassment, excitement, fear, gratitude, grief, joy, love, nervousness, optimism, pride, realization, relief, remorse, sadness, surprise.  `analyze_data.py` and `extract_words.py` have already been run and the output files are stored in `plots` and `tables` respectively. What's inside is more than just rows and columns. Make it easy for others to get started by describing how you acquired the data and what time period it represents, too. Original repository from where the data is taken: [https://github.com/google-research/google-research/tree/master/goemotions](https://github.com/google-research/google-research/tree/master/goemotions) However, some of the scripts were giving errors which have been rectified and updated in this repository: [https://github.com/DebarshiChanda/google-research/tree/master/goemotions](https://github.com/DebarshiChanda/google-research/tree/master/goemotions)  ### Acknowledgements The entire Credit for creating this dataset goes to Google Research Team.   ### Inspiration Detect emotion from the text using Multi-label Classification"
      ]
    ]
  },
  {
    "question": "Is there any bias in GoEmotions dataset?",
    "answers": [
      [
        "Score: 0.93",
        "The results in Figure 3 suggest that our dataset generalizes well to different domains and taxonomies, and that using a model using GoEmotions can help in cases when there is limited data from the target domain, or limited resources for labeling."
      ],
      [
        "Score: 0.92",
        "Reddit is known for a demographic bias leaning towards young male users (Duggan and Smith, 2013) , which is not reflective of a globally diverse population. The platform also introduces a skew towards toxic, offensive language (Mohan et al., 2017) . Thus, Reddit content has been used to study depression (Pirina and C \u00b8\u00f6ltekin, 2018) , microaggressions (Breitfeller et al., 2019) , and Yanardag and Rahwan (2018) have shown the effect of using biased Reddit data by training a \"psychopath\" bot. To address these concerns, and enable building broadly representative emotion models using GoEmotions, we take a series of data curation measures to ensure our data does not reinforce general, nor emotion-specific, language biases. We identify harmful comments using pre-defined lists containing offensive/adult, vulgar (mildly offensive profanity), identity, and religion terms (included as supplementary material). These are used for data filtering and masking, as described below. Lists were internally compiled and we believe they are comprehensive and widely useful for dataset curation, however, they may not be complete."
      ],
      [
        "Score: 0.92",
        "Data Disclaimer: We are aware that the dataset contains biases and is not representative of global diversity. We are aware that the dataset contains potentially problematic content. Potential biases in the data include: Inherent biases in Reddit and user base biases, the offensive/vulgar word lists used for data filtering, inherent or unconscious bias in assessment of offensive identity labels, annotators were all native English speakers from India. All these likely affect labeling, precision, and recall for a trained model. The emotion pilot model used for sentiment labeling, was trained on examples reviewed by the research team. Anyone using this dataset should be aware of these limitations of the dataset."
      ],
      [
        "Score: 0.91",
        "We are aware that the dataset contains biases and is not representative of global diversity. We are aware that the dataset contains potentially problematic content. Potential biases in the data include: Inherent biases in Reddit and user base biases, the offensive/vulgar word lists used for data filtering, inherent or unconscious bias in assessment of offensive identity labels, annotators were all native English speakers from India. All these likely affect labelling, precision, and recall for a trained model. The emotion pilot model used for sentiment labeling, was trained on examples reviewed by the research team. Anyone using this dataset should be aware of these limitations of the dataset."
      ],
      [
        "Score: 0.80",
        "We conduct transfer learning experiments on existing emotion benchmarks, in order to show our data generalizes across domains and taxonomies. The goal is to demonstrate that given little labeled data in a target domain, one can utilize GoEmotions as baseline emotion understanding data. ity and taxonomy. In the interest of space, we only discuss three of these datasets here, chosen based on their diversity of domains. In our experiments, we observe similar trends for the additional benchmarks, and all are included in the Appendix H. The International Survey on Emotion Antecedents and Reactions (ISEAR) (Scherer and Wallbott, 1994 ) is a collection of personal reports on emotional events, written by 3000 people from different cultural backgrounds. The dataset contains 8k sentences, each labeled with a single emotion. The categories are anger, disgust, fear, guilt, joy, sadness and shame."
      ]
    ]
  },
  {
    "question": "Is there any privacy issues in GoEmotions dataset?",
    "answers": [
      [
        "Score: 0.88",
        "The results in Figure 3 suggest that our dataset generalizes well to different domains and taxonomies, and that using a model using GoEmotions can help in cases when there is limited data from the target domain, or limited resources for labeling."
      ],
      [
        "Score: 0.85",
        "We conduct transfer learning experiments on existing emotion benchmarks, in order to show our data generalizes across domains and taxonomies. The goal is to demonstrate that given little labeled data in a target domain, one can utilize GoEmotions as baseline emotion understanding data. ity and taxonomy. In the interest of space, we only discuss three of these datasets here, chosen based on their diversity of domains. In our experiments, we observe similar trends for the additional benchmarks, and all are included in the Appendix H. The International Survey on Emotion Antecedents and Reactions (ISEAR) (Scherer and Wallbott, 1994 ) is a collection of personal reports on emotional events, written by 3000 people from different cultural backgrounds. The dataset contains 8k sentences, each labeled with a single emotion. The categories are anger, disgust, fear, guilt, joy, sadness and shame."
      ],
      [
        "Score: 0.78",
        "We present GoEmotions, a large, manually annotated, carefully curated dataset for fine-grained emotion prediction. We provide a detailed data analysis, demonstrating the reliability of the annotations for the full taxonomy. We show the generalizability of the data across domains and taxonomies via transfer learning experiments. We build a strong baseline by fine-tuning a BERT model, however, the results suggest much room for future improvement. Future work can explore the cross-cultural robustness of emotion ratings, and extend the taxonomy to other languages and domains."
      ],
      [
        "Score: 0.77",
        "The emotion categories are: admiration, amusement, anger, annoyance, approval, caring, confusion, curiosity, desire, disappointment, disapproval, disgust, embarrassment, excitement, fear, gratitude, grief, joy, love, nervousness, optimism, pride, realization, relief, remorse, sadness, surprise. This directory includes the data and code for data analysis scripts. We also include code for our baseline model, which involves fine-tuning a pre-trained BERT-base model. For more details on the design and content of the dataset, please see our paper. Refer to our GoEmotions Model Card for recommended uses of models built with this data, as well as considerations and limitations relating to the GoEmotions data. Requirements See requirements.txt Setup Download the pre-trained BERT model from here and unzip them inside the bert directory. In the paper, we use the cased base model. Data Our raw dataset can be retrieved by running:"
      ],
      [
        "Score: 0.67",
        "To this end, we compiled GoEmotions, the largest human annotated dataset of 58k carefully selected Reddit comments, labeled for 27 emotion categories or Neutral, with comments extracted from popular English subreddits. Table 1 shows an illustrative sample of our collected data. We design our emotion taxonomy considering related work in psychology and coverage in our data. In contrast to Ekman's taxonomy, which includes only one positive emotion (joy), our taxonomy includes a large number of positive, negative, and ambiguous emotion categories, making it suitable for downstream conversation understanding tasks that require a subtle understanding of emotion expression, such as the analysis of customer feedback or the enhancement of chatbots."
      ]
    ]
  },
  {
    "question": "Is there any social concern in GoEmotions dataset?Are there potential harmful data in GoEmotions dataset?",
    "answers": [
      [
        "Score: 0.94",
        "We conduct transfer learning experiments on existing emotion benchmarks, in order to show our data generalizes across domains and taxonomies. The goal is to demonstrate that given little labeled data in a target domain, one can utilize GoEmotions as baseline emotion understanding data. ity and taxonomy. In the interest of space, we only discuss three of these datasets here, chosen based on their diversity of domains. In our experiments, we observe similar trends for the additional benchmarks, and all are included in the Appendix H. The International Survey on Emotion Antecedents and Reactions (ISEAR) (Scherer and Wallbott, 1994 ) is a collection of personal reports on emotional events, written by 3000 people from different cultural backgrounds. The dataset contains 8k sentences, each labeled with a single emotion. The categories are anger, disgust, fear, guilt, joy, sadness and shame."
      ],
      [
        "Score: 0.93",
        "The results in Figure 3 suggest that our dataset generalizes well to different domains and taxonomies, and that using a model using GoEmotions can help in cases when there is limited data from the target domain, or limited resources for labeling."
      ],
      [
        "Score: 0.90",
        "The emotion categories are: admiration, amusement, anger, annoyance, approval, caring, confusion, curiosity, desire, disappointment, disapproval, disgust, embarrassment, excitement, fear, gratitude, grief, joy, love, nervousness, optimism, pride, realization, relief, remorse, sadness, surprise. This directory includes the data and code for data analysis scripts. We also include code for our baseline model, which involves fine-tuning a pre-trained BERT-base model. For more details on the design and content of the dataset, please see our paper. Refer to our GoEmotions Model Card for recommended uses of models built with this data, as well as considerations and limitations relating to the GoEmotions data. Requirements See requirements.txt Setup Download the pre-trained BERT model from here and unzip them inside the bert directory. In the paper, we use the cased base model. Data Our raw dataset can be retrieved by running:"
      ],
      [
        "Score: 0.88",
        "We present GoEmotions, a large, manually annotated, carefully curated dataset for fine-grained emotion prediction. We provide a detailed data analysis, demonstrating the reliability of the annotations for the full taxonomy. We show the generalizability of the data across domains and taxonomies via transfer learning experiments. We build a strong baseline by fine-tuning a BERT model, however, the results suggest much room for future improvement. Future work can explore the cross-cultural robustness of emotion ratings, and extend the taxonomy to other languages and domains."
      ],
      [
        "Score: 0.87",
        "To this end, we compiled GoEmotions, the largest human annotated dataset of 58k carefully selected Reddit comments, labeled for 27 emotion categories or Neutral, with comments extracted from popular English subreddits. Table 1 shows an illustrative sample of our collected data. We design our emotion taxonomy considering related work in psychology and coverage in our data. In contrast to Ekman's taxonomy, which includes only one positive emotion (joy), our taxonomy includes a large number of positive, negative, and ambiguous emotion categories, making it suitable for downstream conversation understanding tasks that require a subtle understanding of emotion expression, such as the analysis of customer feedback or the enhancement of chatbots."
      ]
    ]
  },
  {
    "question": "What is the attribute df_index about?",
    "answers": [
      [
        "Score: 0.00",
        "To better understand agreement among raters and the latent structure of the emotion space, we apply Principal Preserved Component Analysis (PPCA) (Cowen et al., 2019b) to our data. PPCA extracts linear combinations of attributes (here, emotion judgments), that maximally covary across two sets of data that measure the same attributes (here, randomly split judgments for each example). Thus, PPCA allows us to uncover latent dimensions of J \u2208 R n\u00d7|R|\u00d7|E| \u2190 all ratings for the examples annotated by r 7:"
      ],
      [
        "Score: 0.00",
        "We find that all 27 PPCs are highly significant. Specifically, Bonferroni-corrected p-values are less than 1.5e-6 for all dimensions (corrected \u03b1 = 0.0017), suggesting that the emotions were highly dissociable. Such a high degree of significance for all dimensions is nontrivial. For example, Cowen et al. (2019b) find that only 12 out of their 30 emotion categories are significantly dissociable."
      ],
      [
        "Score: 0.00",
        "We conduct transfer learning experiments on existing emotion benchmarks, in order to show our data generalizes across domains and taxonomies. The goal is to demonstrate that given little labeled data in a target domain, one can utilize GoEmotions as baseline emotion understanding data. ity and taxonomy. In the interest of space, we only discuss three of these datasets here, chosen based on their diversity of domains. In our experiments, we observe similar trends for the additional benchmarks, and all are included in the Appendix H. The International Survey on Emotion Antecedents and Reactions (ISEAR) (Scherer and Wallbott, 1994 ) is a collection of personal reports on emotional events, written by 3000 people from different cultural backgrounds. The dataset contains 8k sentences, each labeled with a single emotion. The categories are anger, disgust, fear, guilt, joy, sadness and shame."
      ],
      [
        "Score: 0.00",
        "Table 4 summarizes the performance of our best model, BERT, on the test set, which achieves an average F1-score of . 46 (std=.19 ). The model obtains the best performance on emotions with overt lexical markers, such as gratitude (.86), amusement (.8) and love (.78). The model obtains the lowest F1-score on grief (0), relief (.15) and realization (.21), which are the lowest frequency emotions. We find that less frequent emotions tend to be confused by the model with more frequent emotions related in sentiment and intensity (e.g., grief with sadness, pride with admiration, nervousness with fear)see Appendix G for a more detailed analysis."
      ],
      [
        "Score: 0.00",
        "X, Y \u2208 R n\u00d7|E| \u2190 randomly split J \u2212r and average ratings across raters for both sets 10:"
      ]
    ]
  },
  {
    "question": "A description of the attribute df_index?",
    "answers": [
      [
        "Score: 0.00",
        "To better understand agreement among raters and the latent structure of the emotion space, we apply Principal Preserved Component Analysis (PPCA) (Cowen et al., 2019b) to our data. PPCA extracts linear combinations of attributes (here, emotion judgments), that maximally covary across two sets of data that measure the same attributes (here, randomly split judgments for each example). Thus, PPCA allows us to uncover latent dimensions of J \u2208 R n\u00d7|R|\u00d7|E| \u2190 all ratings for the examples annotated by r 7:"
      ],
      [
        "Score: 0.00",
        "We find that all 27 PPCs are highly significant. Specifically, Bonferroni-corrected p-values are less than 1.5e-6 for all dimensions (corrected \u03b1 = 0.0017), suggesting that the emotions were highly dissociable. Such a high degree of significance for all dimensions is nontrivial. For example, Cowen et al. (2019b) find that only 12 out of their 30 emotion categories are significantly dissociable."
      ],
      [
        "Score: 0.00",
        "While reviewing the results from the pilot rounds, we identified and removed emotions that were scarcely selected by annotators and/or had low interrater agreement due to being very similar to other emotions or being too difficult to detect from text. These emotions were boredom, doubt, heartbroken, indifference and calmness. We also identified and added those emotions to our taxonomy that were frequently suggested by raters and/or seemed to be represented in the data upon manual inspection. These emotions were desire, disappointment, pride, realization, relief and remorse. In this process, we also refined the category names (e.g. replacing ecstasy with excitement), to ones that seemed interpretable to annotators. This is how we arrived at the final set of 27 emotions + Neutral. Our high interrater agreement in the final data can be partially explained by the fact that we took interpretability into consideration while constructing the taxonomy. The dataset is we are releasing was labeled in the third round over the final taxonomy."
      ],
      [
        "Score: 0.00",
        "We present GoEmotions, a large, manually annotated, carefully curated dataset for fine-grained emotion prediction. We provide a detailed data analysis, demonstrating the reliability of the annotations for the full taxonomy. We show the generalizability of the data across domains and taxonomies via transfer learning experiments. We build a strong baseline by fine-tuning a BERT model, however, the results suggest much room for future improvement. Future work can explore the cross-cultural robustness of emotion ratings, and extend the taxonomy to other languages and domains."
      ],
      [
        "Score: 0.00",
        "Table 4 summarizes the performance of our best model, BERT, on the test set, which achieves an average F1-score of . 46 (std=.19 ). The model obtains the best performance on emotions with overt lexical markers, such as gratitude (.86), amusement (.8) and love (.78). The model obtains the lowest F1-score on grief (0), relief (.15) and realization (.21), which are the lowest frequency emotions. We find that less frequent emotions tend to be confused by the model with more frequent emotions related in sentiment and intensity (e.g., grief with sadness, pride with admiration, nervousness with fear)see Appendix G for a more detailed analysis."
      ]
    ]
  },
  {
    "question": "What is the attribute text about?",
    "answers": [
      [
        "Score: 0.01",
        "While reviewing the results from the pilot rounds, we identified and removed emotions that were scarcely selected by annotators and/or had low interrater agreement due to being very similar to other emotions or being too difficult to detect from text. These emotions were boredom, doubt, heartbroken, indifference and calmness. We also identified and added those emotions to our taxonomy that were frequently suggested by raters and/or seemed to be represented in the data upon manual inspection. These emotions were desire, disappointment, pride, realization, relief and remorse. In this process, we also refined the category names (e.g. replacing ecstasy with excitement), to ones that seemed interpretable to annotators. This is how we arrived at the final set of 27 emotions + Neutral. Our high interrater agreement in the final data can be partially explained by the fact that we took interpretability into consideration while constructing the taxonomy. The dataset is we are releasing was labeled in the third round over the final taxonomy."
      ],
      [
        "Score: 0.01",
        "Finding something impressive or worthy of respect."
      ],
      [
        "Score: 0.01",
        "Visualization Here you can view a TSNE projection showing a random sample of the data. The plot is generated using PPCA (see scripts below). Each point in the plot represents a single example and the text and the labels are shown on mouse-hover. The color of each point is the weighted average of the RGB values of the those emotions. Data Analysis See each script for more documentation and descriptive command line flags."
      ],
      [
        "Score: 0.01",
        "text: The text of the comment (with masked tokens, as described in the paper). id: The unique id of the comment. author: The Reddit username of the comment's author. subreddit: The subreddit that the comment belongs to. link_id: The link id of the comment. parent_id: The parent id of the comment. created_utc: The timestamp of the comment. rater_id: The unique id of the annotator. example_very_unclear: Whether the annotator marked the example as being very unclear or difficult to label (in this case they did not choose any emotion labels). separate columns representing each of the emotion categories, with binary labels (0 or 1)"
      ],
      [
        "Score: 0.00",
        "Ever since Affective Text (Strapparava and Mihalcea, 2007) , the first benchmark for emotion recognition was introduced, the field has seen several emotion datasets that vary in size, domain and taxonomy (cf. Bostan and Klinger, 2018) . The majority of emotion datasets are constructed manually, but tend to be relatively small. The largest manually labeled dataset is CrowdFlower (2016), with 39k labeled examples, which were found by Bostan and Klinger (2018) to be noisy in comparison with other emotion datasets. Other datasets are automatically weakly-labeled, based on emotion-related hashtags on Twitter (Wang et al., 2012; Abdul-Mageed and Ungar, 2017) . We build our dataset manually, making it the largest human annotated dataset, with multiple annotations per example for quality assurance."
      ]
    ]
  },
  {
    "question": "A description of the attribute text?",
    "answers": [
      [
        "Score: 0.04",
        "While reviewing the results from the pilot rounds, we identified and removed emotions that were scarcely selected by annotators and/or had low interrater agreement due to being very similar to other emotions or being too difficult to detect from text. These emotions were boredom, doubt, heartbroken, indifference and calmness. We also identified and added those emotions to our taxonomy that were frequently suggested by raters and/or seemed to be represented in the data upon manual inspection. These emotions were desire, disappointment, pride, realization, relief and remorse. In this process, we also refined the category names (e.g. replacing ecstasy with excitement), to ones that seemed interpretable to annotators. This is how we arrived at the final set of 27 emotions + Neutral. Our high interrater agreement in the final data can be partially explained by the fact that we took interpretability into consideration while constructing the taxonomy. The dataset is we are releasing was labeled in the third round over the final taxonomy."
      ],
      [
        "Score: 0.01",
        "Visualization Here you can view a TSNE projection showing a random sample of the data. The plot is generated using PPCA (see scripts below). Each point in the plot represents a single example and the text and the labels are shown on mouse-hover. The color of each point is the weighted average of the RGB values of the those emotions. Data Analysis See each script for more documentation and descriptive command line flags."
      ],
      [
        "Score: 0.01",
        "We find that all 27 PPCs are highly significant. Specifically, Bonferroni-corrected p-values are less than 1.5e-6 for all dimensions (corrected \u03b1 = 0.0017), suggesting that the emotions were highly dissociable. Such a high degree of significance for all dimensions is nontrivial. For example, Cowen et al. (2019b) find that only 12 out of their 30 emotion categories are significantly dissociable."
      ],
      [
        "Score: 0.01",
        "Finding something impressive or worthy of respect."
      ],
      [
        "Score: 0.01",
        "text: The text of the comment (with masked tokens, as described in the paper). id: The unique id of the comment. author: The Reddit username of the comment's author. subreddit: The subreddit that the comment belongs to. link_id: The link id of the comment. parent_id: The parent id of the comment. created_utc: The timestamp of the comment. rater_id: The unique id of the annotator. example_very_unclear: Whether the annotator marked the example as being very unclear or difficult to label (in this case they did not choose any emotion labels). separate columns representing each of the emotion categories, with binary labels (0 or 1)"
      ]
    ]
  },
  {
    "question": "What is the attribute id about?",
    "answers": [
      [
        "Score: 0.04",
        "text: The text of the comment (with masked tokens, as described in the paper). id: The unique id of the comment. author: The Reddit username of the comment's author. subreddit: The subreddit that the comment belongs to. link_id: The link id of the comment. parent_id: The parent id of the comment. created_utc: The timestamp of the comment. rater_id: The unique id of the annotator. example_very_unclear: Whether the annotator marked the example as being very unclear or difficult to label (in this case they did not choose any emotion labels). separate columns representing each of the emotion categories, with binary labels (0 or 1)"
      ],
      [
        "Score: 0.02",
        "Finding something impressive or worthy of respect."
      ],
      [
        "Score: 0.01",
        "text comma-separated list of emotion ids (the ids are indexed based on the order of emotions in emotions.txt) id of the comment"
      ],
      [
        "Score: 0.00",
        "To better understand agreement among raters and the latent structure of the emotion space, we apply Principal Preserved Component Analysis (PPCA) (Cowen et al., 2019b) to our data. PPCA extracts linear combinations of attributes (here, emotion judgments), that maximally covary across two sets of data that measure the same attributes (here, randomly split judgments for each example). Thus, PPCA allows us to uncover latent dimensions of J \u2208 R n\u00d7|R|\u00d7|E| \u2190 all ratings for the examples annotated by r 7:"
      ],
      [
        "Score: 0.00",
        "One of the main aspects distinguishing our dataset is its emotion taxonomy. The vast majority of existing datasets contain annotations for minor variations of the 6 basic emotion categories (joy, anger, fear, sadness, disgust, and surprise) proposed by Ekman (1992a) and/or along affective dimensions (valence and arousal) that underpin the circumplex model of affect (Russell, 2003; Buechel and Hahn, 2017) ."
      ]
    ]
  },
  {
    "question": "A description of the attribute id?",
    "answers": [
      [
        "Score: 0.10",
        "text: The text of the comment (with masked tokens, as described in the paper). id: The unique id of the comment. author: The Reddit username of the comment's author. subreddit: The subreddit that the comment belongs to. link_id: The link id of the comment. parent_id: The parent id of the comment. created_utc: The timestamp of the comment. rater_id: The unique id of the annotator. example_very_unclear: Whether the annotator marked the example as being very unclear or difficult to label (in this case they did not choose any emotion labels). separate columns representing each of the emotion categories, with binary labels (0 or 1)"
      ],
      [
        "Score: 0.02",
        "text comma-separated list of emotion ids (the ids are indexed based on the order of emotions in emotions.txt) id of the comment"
      ],
      [
        "Score: 0.02",
        "Finding something impressive or worthy of respect."
      ],
      [
        "Score: 0.01",
        "While reviewing the results from the pilot rounds, we identified and removed emotions that were scarcely selected by annotators and/or had low interrater agreement due to being very similar to other emotions or being too difficult to detect from text. These emotions were boredom, doubt, heartbroken, indifference and calmness. We also identified and added those emotions to our taxonomy that were frequently suggested by raters and/or seemed to be represented in the data upon manual inspection. These emotions were desire, disappointment, pride, realization, relief and remorse. In this process, we also refined the category names (e.g. replacing ecstasy with excitement), to ones that seemed interpretable to annotators. This is how we arrived at the final set of 27 emotions + Neutral. Our high interrater agreement in the final data can be partially explained by the fact that we took interpretability into consideration while constructing the taxonomy. The dataset is we are releasing was labeled in the third round over the final taxonomy."
      ],
      [
        "Score: 0.00",
        "To better understand agreement among raters and the latent structure of the emotion space, we apply Principal Preserved Component Analysis (PPCA) (Cowen et al., 2019b) to our data. PPCA extracts linear combinations of attributes (here, emotion judgments), that maximally covary across two sets of data that measure the same attributes (here, randomly split judgments for each example). Thus, PPCA allows us to uncover latent dimensions of J \u2208 R n\u00d7|R|\u00d7|E| \u2190 all ratings for the examples annotated by r 7:"
      ]
    ]
  },
  {
    "question": "What is the attribute author about?",
    "answers": [
      [
        "Score: 0.01",
        "The rater interface. Reddit comments were presented with no additional metadata (such as the author or subreddit). To help raters navigate the large space of emotion in our taxonomy, they were presented a table containing all emotion categories aggregated by sentiment (by the mapping in Figure 2 ) and whether that emotion is generally expressed towards something (e.g. disapproval) or is more of an intrinsic feeling (e.g. joy). The instructions highlighted that this separation of categories was by no means clear-cut, but captured general tendencies, and we encouraged raters to ignore the categorization whenever they saw fit. Emotions with a straightforward mapping onto emojis were shown with an emoji in the UI, to further ease their interpretation."
      ],
      [
        "Score: 0.01",
        "@inproceedings{demszky2020goemotions,  author = {Demszky, Dorottya and Movshovitz-Attias, Dana and Ko, Jeongwoo and Cowen, Alan and Nemade, Gaurav and Ravi, Sujith},  booktitle = {58th Annual Meeting of the Association for Computational Linguistics (ACL)},  title = {{GoEmotions: A Dataset of Fine-Grained Emotions}},  year = {2020} }"
      ],
      [
        "Score: 0.00",
        "To better understand agreement among raters and the latent structure of the emotion space, we apply Principal Preserved Component Analysis (PPCA) (Cowen et al., 2019b) to our data. PPCA extracts linear combinations of attributes (here, emotion judgments), that maximally covary across two sets of data that measure the same attributes (here, randomly split judgments for each example). Thus, PPCA allows us to uncover latent dimensions of J \u2208 R n\u00d7|R|\u00d7|E| \u2190 all ratings for the examples annotated by r 7:"
      ],
      [
        "Score: 0.00",
        "Finding something impressive or worthy of respect."
      ],
      [
        "Score: 0.00",
        "text: The text of the comment (with masked tokens, as described in the paper). id: The unique id of the comment. author: The Reddit username of the comment's author. subreddit: The subreddit that the comment belongs to. link_id: The link id of the comment. parent_id: The parent id of the comment. created_utc: The timestamp of the comment. rater_id: The unique id of the annotator. example_very_unclear: Whether the annotator marked the example as being very unclear or difficult to label (in this case they did not choose any emotion labels). separate columns representing each of the emotion categories, with binary labels (0 or 1)"
      ]
    ]
  },
  {
    "question": "A description of the attribute author?",
    "answers": [
      [
        "Score: 0.01",
        "While reviewing the results from the pilot rounds, we identified and removed emotions that were scarcely selected by annotators and/or had low interrater agreement due to being very similar to other emotions or being too difficult to detect from text. These emotions were boredom, doubt, heartbroken, indifference and calmness. We also identified and added those emotions to our taxonomy that were frequently suggested by raters and/or seemed to be represented in the data upon manual inspection. These emotions were desire, disappointment, pride, realization, relief and remorse. In this process, we also refined the category names (e.g. replacing ecstasy with excitement), to ones that seemed interpretable to annotators. This is how we arrived at the final set of 27 emotions + Neutral. Our high interrater agreement in the final data can be partially explained by the fact that we took interpretability into consideration while constructing the taxonomy. The dataset is we are releasing was labeled in the third round over the final taxonomy."
      ],
      [
        "Score: 0.01",
        "To better understand agreement among raters and the latent structure of the emotion space, we apply Principal Preserved Component Analysis (PPCA) (Cowen et al., 2019b) to our data. PPCA extracts linear combinations of attributes (here, emotion judgments), that maximally covary across two sets of data that measure the same attributes (here, randomly split judgments for each example). Thus, PPCA allows us to uncover latent dimensions of J \u2208 R n\u00d7|R|\u00d7|E| \u2190 all ratings for the examples annotated by r 7:"
      ],
      [
        "Score: 0.01",
        "The rater interface. Reddit comments were presented with no additional metadata (such as the author or subreddit). To help raters navigate the large space of emotion in our taxonomy, they were presented a table containing all emotion categories aggregated by sentiment (by the mapping in Figure 2 ) and whether that emotion is generally expressed towards something (e.g. disapproval) or is more of an intrinsic feeling (e.g. joy). The instructions highlighted that this separation of categories was by no means clear-cut, but captured general tendencies, and we encouraged raters to ignore the categorization whenever they saw fit. Emotions with a straightforward mapping onto emojis were shown with an emoji in the UI, to further ease their interpretation."
      ],
      [
        "Score: 0.00",
        "text: The text of the comment (with masked tokens, as described in the paper). id: The unique id of the comment. author: The Reddit username of the comment's author. subreddit: The subreddit that the comment belongs to. link_id: The link id of the comment. parent_id: The parent id of the comment. created_utc: The timestamp of the comment. rater_id: The unique id of the annotator. example_very_unclear: Whether the annotator marked the example as being very unclear or difficult to label (in this case they did not choose any emotion labels). separate columns representing each of the emotion categories, with binary labels (0 or 1)"
      ],
      [
        "Score: 0.00",
        "We present GoEmotions, a large, manually annotated, carefully curated dataset for fine-grained emotion prediction. We provide a detailed data analysis, demonstrating the reliability of the annotations for the full taxonomy. We show the generalizability of the data across domains and taxonomies via transfer learning experiments. We build a strong baseline by fine-tuning a BERT model, however, the results suggest much room for future improvement. Future work can explore the cross-cultural robustness of emotion ratings, and extend the taxonomy to other languages and domains."
      ]
    ]
  },
  {
    "question": "What is the attribute subreddit about?",
    "answers": [
      [
        "Score: 0.74",
        "The rater interface. Reddit comments were presented with no additional metadata (such as the author or subreddit). To help raters navigate the large space of emotion in our taxonomy, they were presented a table containing all emotion categories aggregated by sentiment (by the mapping in Figure 2 ) and whether that emotion is generally expressed towards something (e.g. disapproval) or is more of an intrinsic feeling (e.g. joy). The instructions highlighted that this separation of categories was by no means clear-cut, but captured general tendencies, and we encouraged raters to ignore the categorization whenever they saw fit. Emotions with a straightforward mapping onto emojis were shown with an emoji in the UI, to further ease their interpretation."
      ],
      [
        "Score: 0.52",
        "text: The text of the comment (with masked tokens, as described in the paper). id: The unique id of the comment. author: The Reddit username of the comment's author. subreddit: The subreddit that the comment belongs to. link_id: The link id of the comment. parent_id: The parent id of the comment. created_utc: The timestamp of the comment. rater_id: The unique id of the annotator. example_very_unclear: Whether the annotator marked the example as being very unclear or difficult to label (in this case they did not choose any emotion labels). separate columns representing each of the emotion categories, with binary labels (0 or 1)"
      ],
      [
        "Score: 0.50",
        "Sentiment balancing. We reduce sentiment bias by removing subreddits with little representation of positive, negative, ambiguous, or neutral sentiment. To estimate a comment's sentiment, we run our emotion prediction model, trained on a pilot batch of 2.2k annotated examples. The mapping of emotions into sentiment categories is found in Figure 2 . We exclude subreddits consisting of more than 30% neutral comments or less than 20% of negative, positive, or ambiguous comments."
      ],
      [
        "Score: 0.46",
        "Subreddit balancing. To avoid over representation of popular subreddits, we perform downsampling, capping by the median subreddit count."
      ],
      [
        "Score: 0.24",
        "We use a Reddit data dump originating in the redditdata-tools project 2 , which contains comments from 2005 (the start of Reddit) to January 2019. We select subreddits with at least 10k comments and remove deleted and non-English comments."
      ]
    ]
  },
  {
    "question": "A description of the attribute subreddit?",
    "answers": [
      [
        "Score: 0.74",
        "The rater interface. Reddit comments were presented with no additional metadata (such as the author or subreddit). To help raters navigate the large space of emotion in our taxonomy, they were presented a table containing all emotion categories aggregated by sentiment (by the mapping in Figure 2 ) and whether that emotion is generally expressed towards something (e.g. disapproval) or is more of an intrinsic feeling (e.g. joy). The instructions highlighted that this separation of categories was by no means clear-cut, but captured general tendencies, and we encouraged raters to ignore the categorization whenever they saw fit. Emotions with a straightforward mapping onto emojis were shown with an emoji in the UI, to further ease their interpretation."
      ],
      [
        "Score: 0.71",
        "text: The text of the comment (with masked tokens, as described in the paper). id: The unique id of the comment. author: The Reddit username of the comment's author. subreddit: The subreddit that the comment belongs to. link_id: The link id of the comment. parent_id: The parent id of the comment. created_utc: The timestamp of the comment. rater_id: The unique id of the annotator. example_very_unclear: Whether the annotator marked the example as being very unclear or difficult to label (in this case they did not choose any emotion labels). separate columns representing each of the emotion categories, with binary labels (0 or 1)"
      ],
      [
        "Score: 0.54",
        "Sentiment balancing. We reduce sentiment bias by removing subreddits with little representation of positive, negative, ambiguous, or neutral sentiment. To estimate a comment's sentiment, we run our emotion prediction model, trained on a pilot batch of 2.2k annotated examples. The mapping of emotions into sentiment categories is found in Figure 2 . We exclude subreddits consisting of more than 30% neutral comments or less than 20% of negative, positive, or ambiguous comments."
      ],
      [
        "Score: 0.52",
        "Subreddit balancing. To avoid over representation of popular subreddits, we perform downsampling, capping by the median subreddit count."
      ],
      [
        "Score: 0.44",
        "To this end, we compiled GoEmotions, the largest human annotated dataset of 58k carefully selected Reddit comments, labeled for 27 emotion categories or Neutral, with comments extracted from popular English subreddits. Table 1 shows an illustrative sample of our collected data. We design our emotion taxonomy considering related work in psychology and coverage in our data. In contrast to Ekman's taxonomy, which includes only one positive emotion (joy), our taxonomy includes a large number of positive, negative, and ambiguous emotion categories, making it suitable for downstream conversation understanding tasks that require a subtle understanding of emotion expression, such as the analysis of customer feedback or the enhancement of chatbots."
      ]
    ]
  },
  {
    "question": "What is the attribute link_id about?",
    "answers": [
      [
        "Score: 0.29",
        "text: The text of the comment (with masked tokens, as described in the paper). id: The unique id of the comment. author: The Reddit username of the comment's author. subreddit: The subreddit that the comment belongs to. link_id: The link id of the comment. parent_id: The parent id of the comment. created_utc: The timestamp of the comment. rater_id: The unique id of the annotator. example_very_unclear: Whether the annotator marked the example as being very unclear or difficult to label (in this case they did not choose any emotion labels). separate columns representing each of the emotion categories, with binary labels (0 or 1)"
      ],
      [
        "Score: 0.00",
        "text comma-separated list of emotion ids (the ids are indexed based on the order of emotions in emotions.txt) id of the comment"
      ],
      [
        "Score: 0.00",
        "We conduct transfer learning experiments on existing emotion benchmarks, in order to show our data generalizes across domains and taxonomies. The goal is to demonstrate that given little labeled data in a target domain, one can utilize GoEmotions as baseline emotion understanding data. ity and taxonomy. In the interest of space, we only discuss three of these datasets here, chosen based on their diversity of domains. In our experiments, we observe similar trends for the additional benchmarks, and all are included in the Appendix H. The International Survey on Emotion Antecedents and Reactions (ISEAR) (Scherer and Wallbott, 1994 ) is a collection of personal reports on emotional events, written by 3000 people from different cultural backgrounds. The dataset contains 8k sentences, each labeled with a single emotion. The categories are anger, disgust, fear, guilt, joy, sadness and shame."
      ],
      [
        "Score: 0.00",
        "We also perform hierarchical clustering over the normalized confusion matrix using correlation as a distance metric and ward as a linkage method. We find that the model learns relatively similar clusters as the ones in Figure 2 , even though the training data only includes a subset of the labels that have agreement (see Figure 5 )."
      ],
      [
        "Score: 0.00",
        "Finding something impressive or worthy of respect."
      ]
    ]
  },
  {
    "question": "A description of the attribute link_id?",
    "answers": [
      [
        "Score: 0.38",
        "text: The text of the comment (with masked tokens, as described in the paper). id: The unique id of the comment. author: The Reddit username of the comment's author. subreddit: The subreddit that the comment belongs to. link_id: The link id of the comment. parent_id: The parent id of the comment. created_utc: The timestamp of the comment. rater_id: The unique id of the annotator. example_very_unclear: Whether the annotator marked the example as being very unclear or difficult to label (in this case they did not choose any emotion labels). separate columns representing each of the emotion categories, with binary labels (0 or 1)"
      ],
      [
        "Score: 0.01",
        "text comma-separated list of emotion ids (the ids are indexed based on the order of emotions in emotions.txt) id of the comment"
      ],
      [
        "Score: 0.00",
        "While reviewing the results from the pilot rounds, we identified and removed emotions that were scarcely selected by annotators and/or had low interrater agreement due to being very similar to other emotions or being too difficult to detect from text. These emotions were boredom, doubt, heartbroken, indifference and calmness. We also identified and added those emotions to our taxonomy that were frequently suggested by raters and/or seemed to be represented in the data upon manual inspection. These emotions were desire, disappointment, pride, realization, relief and remorse. In this process, we also refined the category names (e.g. replacing ecstasy with excitement), to ones that seemed interpretable to annotators. This is how we arrived at the final set of 27 emotions + Neutral. Our high interrater agreement in the final data can be partially explained by the fact that we took interpretability into consideration while constructing the taxonomy. The dataset is we are releasing was labeled in the third round over the final taxonomy."
      ],
      [
        "Score: 0.00",
        "We present GoEmotions, a large, manually annotated, carefully curated dataset for fine-grained emotion prediction. We provide a detailed data analysis, demonstrating the reliability of the annotations for the full taxonomy. We show the generalizability of the data across domains and taxonomies via transfer learning experiments. We build a strong baseline by fine-tuning a BERT model, however, the results suggest much room for future improvement. Future work can explore the cross-cultural robustness of emotion ratings, and extend the taxonomy to other languages and domains."
      ],
      [
        "Score: 0.00",
        "We conduct transfer learning experiments on existing emotion benchmarks, in order to show our data generalizes across domains and taxonomies. The goal is to demonstrate that given little labeled data in a target domain, one can utilize GoEmotions as baseline emotion understanding data. ity and taxonomy. In the interest of space, we only discuss three of these datasets here, chosen based on their diversity of domains. In our experiments, we observe similar trends for the additional benchmarks, and all are included in the Appendix H. The International Survey on Emotion Antecedents and Reactions (ISEAR) (Scherer and Wallbott, 1994 ) is a collection of personal reports on emotional events, written by 3000 people from different cultural backgrounds. The dataset contains 8k sentences, each labeled with a single emotion. The categories are anger, disgust, fear, guilt, joy, sadness and shame."
      ]
    ]
  },
  {
    "question": "What is the attribute parent_id about?",
    "answers": [
      [
        "Score: 0.16",
        "text: The text of the comment (with masked tokens, as described in the paper). id: The unique id of the comment. author: The Reddit username of the comment's author. subreddit: The subreddit that the comment belongs to. link_id: The link id of the comment. parent_id: The parent id of the comment. created_utc: The timestamp of the comment. rater_id: The unique id of the annotator. example_very_unclear: Whether the annotator marked the example as being very unclear or difficult to label (in this case they did not choose any emotion labels). separate columns representing each of the emotion categories, with binary labels (0 or 1)"
      ],
      [
        "Score: 0.00",
        "text comma-separated list of emotion ids (the ids are indexed based on the order of emotions in emotions.txt) id of the comment"
      ],
      [
        "Score: 0.00",
        "Finding something impressive or worthy of respect."
      ],
      [
        "Score: 0.00",
        "To better understand agreement among raters and the latent structure of the emotion space, we apply Principal Preserved Component Analysis (PPCA) (Cowen et al., 2019b) to our data. PPCA extracts linear combinations of attributes (here, emotion judgments), that maximally covary across two sets of data that measure the same attributes (here, randomly split judgments for each example). Thus, PPCA allows us to uncover latent dimensions of J \u2208 R n\u00d7|R|\u00d7|E| \u2190 all ratings for the examples annotated by r 7:"
      ],
      [
        "Score: 0.00",
        "One of the main aspects distinguishing our dataset is its emotion taxonomy. The vast majority of existing datasets contain annotations for minor variations of the 6 basic emotion categories (joy, anger, fear, sadness, disgust, and surprise) proposed by Ekman (1992a) and/or along affective dimensions (valence and arousal) that underpin the circumplex model of affect (Russell, 2003; Buechel and Hahn, 2017) ."
      ]
    ]
  },
  {
    "question": "A description of the attribute parent_id?",
    "answers": [
      [
        "Score: 0.24",
        "text: The text of the comment (with masked tokens, as described in the paper). id: The unique id of the comment. author: The Reddit username of the comment's author. subreddit: The subreddit that the comment belongs to. link_id: The link id of the comment. parent_id: The parent id of the comment. created_utc: The timestamp of the comment. rater_id: The unique id of the annotator. example_very_unclear: Whether the annotator marked the example as being very unclear or difficult to label (in this case they did not choose any emotion labels). separate columns representing each of the emotion categories, with binary labels (0 or 1)"
      ],
      [
        "Score: 0.00",
        "text comma-separated list of emotion ids (the ids are indexed based on the order of emotions in emotions.txt) id of the comment"
      ],
      [
        "Score: 0.00",
        "While reviewing the results from the pilot rounds, we identified and removed emotions that were scarcely selected by annotators and/or had low interrater agreement due to being very similar to other emotions or being too difficult to detect from text. These emotions were boredom, doubt, heartbroken, indifference and calmness. We also identified and added those emotions to our taxonomy that were frequently suggested by raters and/or seemed to be represented in the data upon manual inspection. These emotions were desire, disappointment, pride, realization, relief and remorse. In this process, we also refined the category names (e.g. replacing ecstasy with excitement), to ones that seemed interpretable to annotators. This is how we arrived at the final set of 27 emotions + Neutral. Our high interrater agreement in the final data can be partially explained by the fact that we took interpretability into consideration while constructing the taxonomy. The dataset is we are releasing was labeled in the third round over the final taxonomy."
      ],
      [
        "Score: 0.00",
        "To better understand agreement among raters and the latent structure of the emotion space, we apply Principal Preserved Component Analysis (PPCA) (Cowen et al., 2019b) to our data. PPCA extracts linear combinations of attributes (here, emotion judgments), that maximally covary across two sets of data that measure the same attributes (here, randomly split judgments for each example). Thus, PPCA allows us to uncover latent dimensions of J \u2208 R n\u00d7|R|\u00d7|E| \u2190 all ratings for the examples annotated by r 7:"
      ],
      [
        "Score: 0.00",
        "One of the main aspects distinguishing our dataset is its emotion taxonomy. The vast majority of existing datasets contain annotations for minor variations of the 6 basic emotion categories (joy, anger, fear, sadness, disgust, and surprise) proposed by Ekman (1992a) and/or along affective dimensions (valence and arousal) that underpin the circumplex model of affect (Russell, 2003; Buechel and Hahn, 2017) ."
      ]
    ]
  },
  {
    "question": "What is the attribute created_utc about?",
    "answers": [
      [
        "Score: 0.01",
        "text: The text of the comment (with masked tokens, as described in the paper). id: The unique id of the comment. author: The Reddit username of the comment's author. subreddit: The subreddit that the comment belongs to. link_id: The link id of the comment. parent_id: The parent id of the comment. created_utc: The timestamp of the comment. rater_id: The unique id of the annotator. example_very_unclear: Whether the annotator marked the example as being very unclear or difficult to label (in this case they did not choose any emotion labels). separate columns representing each of the emotion categories, with binary labels (0 or 1)"
      ],
      [
        "Score: 0.00",
        "While reviewing the results from the pilot rounds, we identified and removed emotions that were scarcely selected by annotators and/or had low interrater agreement due to being very similar to other emotions or being too difficult to detect from text. These emotions were boredom, doubt, heartbroken, indifference and calmness. We also identified and added those emotions to our taxonomy that were frequently suggested by raters and/or seemed to be represented in the data upon manual inspection. These emotions were desire, disappointment, pride, realization, relief and remorse. In this process, we also refined the category names (e.g. replacing ecstasy with excitement), to ones that seemed interpretable to annotators. This is how we arrived at the final set of 27 emotions + Neutral. Our high interrater agreement in the final data can be partially explained by the fact that we took interpretability into consideration while constructing the taxonomy. The dataset is we are releasing was labeled in the third round over the final taxonomy."
      ],
      [
        "Score: 0.00",
        "To better understand agreement among raters and the latent structure of the emotion space, we apply Principal Preserved Component Analysis (PPCA) (Cowen et al., 2019b) to our data. PPCA extracts linear combinations of attributes (here, emotion judgments), that maximally covary across two sets of data that measure the same attributes (here, randomly split judgments for each example). Thus, PPCA allows us to uncover latent dimensions of J \u2208 R n\u00d7|R|\u00d7|E| \u2190 all ratings for the examples annotated by r 7:"
      ],
      [
        "Score: 0.00",
        "Ever since Affective Text (Strapparava and Mihalcea, 2007) , the first benchmark for emotion recognition was introduced, the field has seen several emotion datasets that vary in size, domain and taxonomy (cf. Bostan and Klinger, 2018) . The majority of emotion datasets are constructed manually, but tend to be relatively small. The largest manually labeled dataset is CrowdFlower (2016), with 39k labeled examples, which were found by Bostan and Klinger (2018) to be noisy in comparison with other emotion datasets. Other datasets are automatically weakly-labeled, based on emotion-related hashtags on Twitter (Wang et al., 2012; Abdul-Mageed and Ungar, 2017) . We build our dataset manually, making it the largest human annotated dataset, with multiple annotations per example for quality assurance."
      ],
      [
        "Score: 0.00",
        "One of the main aspects distinguishing our dataset is its emotion taxonomy. The vast majority of existing datasets contain annotations for minor variations of the 6 basic emotion categories (joy, anger, fear, sadness, disgust, and surprise) proposed by Ekman (1992a) and/or along affective dimensions (valence and arousal) that underpin the circumplex model of affect (Russell, 2003; Buechel and Hahn, 2017) ."
      ]
    ]
  },
  {
    "question": "A description of the attribute created_utc?",
    "answers": [
      [
        "Score: 0.01",
        "While reviewing the results from the pilot rounds, we identified and removed emotions that were scarcely selected by annotators and/or had low interrater agreement due to being very similar to other emotions or being too difficult to detect from text. These emotions were boredom, doubt, heartbroken, indifference and calmness. We also identified and added those emotions to our taxonomy that were frequently suggested by raters and/or seemed to be represented in the data upon manual inspection. These emotions were desire, disappointment, pride, realization, relief and remorse. In this process, we also refined the category names (e.g. replacing ecstasy with excitement), to ones that seemed interpretable to annotators. This is how we arrived at the final set of 27 emotions + Neutral. Our high interrater agreement in the final data can be partially explained by the fact that we took interpretability into consideration while constructing the taxonomy. The dataset is we are releasing was labeled in the third round over the final taxonomy."
      ],
      [
        "Score: 0.01",
        "text: The text of the comment (with masked tokens, as described in the paper). id: The unique id of the comment. author: The Reddit username of the comment's author. subreddit: The subreddit that the comment belongs to. link_id: The link id of the comment. parent_id: The parent id of the comment. created_utc: The timestamp of the comment. rater_id: The unique id of the annotator. example_very_unclear: Whether the annotator marked the example as being very unclear or difficult to label (in this case they did not choose any emotion labels). separate columns representing each of the emotion categories, with binary labels (0 or 1)"
      ],
      [
        "Score: 0.01",
        "To better understand agreement among raters and the latent structure of the emotion space, we apply Principal Preserved Component Analysis (PPCA) (Cowen et al., 2019b) to our data. PPCA extracts linear combinations of attributes (here, emotion judgments), that maximally covary across two sets of data that measure the same attributes (here, randomly split judgments for each example). Thus, PPCA allows us to uncover latent dimensions of J \u2208 R n\u00d7|R|\u00d7|E| \u2190 all ratings for the examples annotated by r 7:"
      ],
      [
        "Score: 0.00",
        "One of the main aspects distinguishing our dataset is its emotion taxonomy. The vast majority of existing datasets contain annotations for minor variations of the 6 basic emotion categories (joy, anger, fear, sadness, disgust, and surprise) proposed by Ekman (1992a) and/or along affective dimensions (valence and arousal) that underpin the circumplex model of affect (Russell, 2003; Buechel and Hahn, 2017) ."
      ],
      [
        "Score: 0.00",
        "Ever since Affective Text (Strapparava and Mihalcea, 2007) , the first benchmark for emotion recognition was introduced, the field has seen several emotion datasets that vary in size, domain and taxonomy (cf. Bostan and Klinger, 2018) . The majority of emotion datasets are constructed manually, but tend to be relatively small. The largest manually labeled dataset is CrowdFlower (2016), with 39k labeled examples, which were found by Bostan and Klinger (2018) to be noisy in comparison with other emotion datasets. Other datasets are automatically weakly-labeled, based on emotion-related hashtags on Twitter (Wang et al., 2012; Abdul-Mageed and Ungar, 2017) . We build our dataset manually, making it the largest human annotated dataset, with multiple annotations per example for quality assurance."
      ]
    ]
  },
  {
    "question": "What is the attribute rater_id about?",
    "answers": [
      [
        "Score: 0.37",
        "The rater interface. Reddit comments were presented with no additional metadata (such as the author or subreddit). To help raters navigate the large space of emotion in our taxonomy, they were presented a table containing all emotion categories aggregated by sentiment (by the mapping in Figure 2 ) and whether that emotion is generally expressed towards something (e.g. disapproval) or is more of an intrinsic feeling (e.g. joy). The instructions highlighted that this separation of categories was by no means clear-cut, but captured general tendencies, and we encouraged raters to ignore the categorization whenever they saw fit. Emotions with a straightforward mapping onto emojis were shown with an emoji in the UI, to further ease their interpretation."
      ],
      [
        "Score: 0.12",
        "X, Y \u2208 R n\u00d7|E| \u2190 randomly split J \u2212r and average ratings across raters for both sets 10:"
      ],
      [
        "Score: 0.11",
        "To better understand agreement among raters and the latent structure of the emotion space, we apply Principal Preserved Component Analysis (PPCA) (Cowen et al., 2019b) to our data. PPCA extracts linear combinations of attributes (here, emotion judgments), that maximally covary across two sets of data that measure the same attributes (here, randomly split judgments for each example). Thus, PPCA allows us to uncover latent dimensions of J \u2208 R n\u00d7|R|\u00d7|E| \u2190 all ratings for the examples annotated by r 7:"
      ],
      [
        "Score: 0.07",
        "We assigned three raters to each example. For those examples where no raters agree on at least one emotion label, we assigned two additional raters. All raters are native English speakers from India. 4   Instructions. Raters were asked to identify the emotions expressed by the writer of the text, given pre-defined emotion definitions (see Appendix A) and a few example texts for each emotion. Raters were free to select multiple emotions, but were asked to only select those ones for which they were reasonably confident that it is expressed in the text. If raters were not certain about any emotion being expressed, they were asked to select Neutral. We included a checkbox for raters to indicate if an example was particularly difficult to label, in which case they could select no emotions. We removed all examples for which no emotion was selected."
      ],
      [
        "Score: 0.04",
        "We estimate rater agreement for each emotion via interrater correlation (Delgado and Tibau, 2019) . 5 For each rater r \u2208 R, we calculate the Spearman correlation between r's judgments and the mean of other raters' judgments, for all examples that r rated. We then take the average of these rater-level correlation scores. In Section 4.3, we show that each emotion has significant interrater correlation, after controlling for several potential confounds."
      ]
    ]
  },
  {
    "question": "A description of the attribute rater_id?",
    "answers": [
      [
        "Score: 0.36",
        "The rater interface. Reddit comments were presented with no additional metadata (such as the author or subreddit). To help raters navigate the large space of emotion in our taxonomy, they were presented a table containing all emotion categories aggregated by sentiment (by the mapping in Figure 2 ) and whether that emotion is generally expressed towards something (e.g. disapproval) or is more of an intrinsic feeling (e.g. joy). The instructions highlighted that this separation of categories was by no means clear-cut, but captured general tendencies, and we encouraged raters to ignore the categorization whenever they saw fit. Emotions with a straightforward mapping onto emojis were shown with an emoji in the UI, to further ease their interpretation."
      ],
      [
        "Score: 0.18",
        "To better understand agreement among raters and the latent structure of the emotion space, we apply Principal Preserved Component Analysis (PPCA) (Cowen et al., 2019b) to our data. PPCA extracts linear combinations of attributes (here, emotion judgments), that maximally covary across two sets of data that measure the same attributes (here, randomly split judgments for each example). Thus, PPCA allows us to uncover latent dimensions of J \u2208 R n\u00d7|R|\u00d7|E| \u2190 all ratings for the examples annotated by r 7:"
      ],
      [
        "Score: 0.14",
        "We assigned three raters to each example. For those examples where no raters agree on at least one emotion label, we assigned two additional raters. All raters are native English speakers from India. 4   Instructions. Raters were asked to identify the emotions expressed by the writer of the text, given pre-defined emotion definitions (see Appendix A) and a few example texts for each emotion. Raters were free to select multiple emotions, but were asked to only select those ones for which they were reasonably confident that it is expressed in the text. If raters were not certain about any emotion being expressed, they were asked to select Neutral. We included a checkbox for raters to indicate if an example was particularly difficult to label, in which case they could select no emotions. We removed all examples for which no emotion was selected."
      ],
      [
        "Score: 0.10",
        "text: The text of the comment (with masked tokens, as described in the paper). id: The unique id of the comment. author: The Reddit username of the comment's author. subreddit: The subreddit that the comment belongs to. link_id: The link id of the comment. parent_id: The parent id of the comment. created_utc: The timestamp of the comment. rater_id: The unique id of the annotator. example_very_unclear: Whether the annotator marked the example as being very unclear or difficult to label (in this case they did not choose any emotion labels). separate columns representing each of the emotion categories, with binary labels (0 or 1)"
      ],
      [
        "Score: 0.07",
        "X, Y \u2208 R n\u00d7|E| \u2190 randomly split J \u2212r and average ratings across raters for both sets 10:"
      ]
    ]
  },
  {
    "question": "What is the attribute example_very_unclear about?",
    "answers": [
      [
        "Score: 0.23",
        "To better understand agreement among raters and the latent structure of the emotion space, we apply Principal Preserved Component Analysis (PPCA) (Cowen et al., 2019b) to our data. PPCA extracts linear combinations of attributes (here, emotion judgments), that maximally covary across two sets of data that measure the same attributes (here, randomly split judgments for each example). Thus, PPCA allows us to uncover latent dimensions of J \u2208 R n\u00d7|R|\u00d7|E| \u2190 all ratings for the examples annotated by r 7:"
      ],
      [
        "Score: 0.08",
        "In practice, since most of our examples only has a single label (see Figure 5 ), our confusion matrix is very similar to one calculated for a single-label classification task."
      ],
      [
        "Score: 0.07",
        "We find that all 27 PPCs are highly significant. Specifically, Bonferroni-corrected p-values are less than 1.5e-6 for all dimensions (corrected \u03b1 = 0.0017), suggesting that the emotions were highly dissociable. Such a high degree of significance for all dimensions is nontrivial. For example, Cowen et al. (2019b) find that only 12 out of their 30 emotion categories are significantly dissociable."
      ],
      [
        "Score: 0.06",
        "text: The text of the comment (with masked tokens, as described in the paper). id: The unique id of the comment. author: The Reddit username of the comment's author. subreddit: The subreddit that the comment belongs to. link_id: The link id of the comment. parent_id: The parent id of the comment. created_utc: The timestamp of the comment. rater_id: The unique id of the annotator. example_very_unclear: Whether the annotator marked the example as being very unclear or difficult to label (in this case they did not choose any emotion labels). separate columns representing each of the emotion categories, with binary labels (0 or 1)"
      ],
      [
        "Score: 0.05",
        "Given limited target domain data (100 or 200 examples), both FREEZE and NOFREEZE yield significantly higher performance than the BASELINE, for all three datasets. Importantly, NOFREEZE results show significantly higher performance for all training set sizes, except for \"max\", where NOFREEZE and BASELINE perform similarly."
      ]
    ]
  },
  {
    "question": "A description of the attribute example_very_unclear?",
    "answers": [
      [
        "Score: 0.25",
        "To better understand agreement among raters and the latent structure of the emotion space, we apply Principal Preserved Component Analysis (PPCA) (Cowen et al., 2019b) to our data. PPCA extracts linear combinations of attributes (here, emotion judgments), that maximally covary across two sets of data that measure the same attributes (here, randomly split judgments for each example). Thus, PPCA allows us to uncover latent dimensions of J \u2208 R n\u00d7|R|\u00d7|E| \u2190 all ratings for the examples annotated by r 7:"
      ],
      [
        "Score: 0.18",
        "We find that all 27 PPCs are highly significant. Specifically, Bonferroni-corrected p-values are less than 1.5e-6 for all dimensions (corrected \u03b1 = 0.0017), suggesting that the emotions were highly dissociable. Such a high degree of significance for all dimensions is nontrivial. For example, Cowen et al. (2019b) find that only 12 out of their 30 emotion categories are significantly dissociable."
      ],
      [
        "Score: 0.11",
        "text: The text of the comment (with masked tokens, as described in the paper). id: The unique id of the comment. author: The Reddit username of the comment's author. subreddit: The subreddit that the comment belongs to. link_id: The link id of the comment. parent_id: The parent id of the comment. created_utc: The timestamp of the comment. rater_id: The unique id of the annotator. example_very_unclear: Whether the annotator marked the example as being very unclear or difficult to label (in this case they did not choose any emotion labels). separate columns representing each of the emotion categories, with binary labels (0 or 1)"
      ],
      [
        "Score: 0.07",
        "In practice, since most of our examples only has a single label (see Figure 5 ), our confusion matrix is very similar to one calculated for a single-label classification task."
      ],
      [
        "Score: 0.04",
        "Given limited target domain data (100 or 200 examples), both FREEZE and NOFREEZE yield significantly higher performance than the BASELINE, for all three datasets. Importantly, NOFREEZE results show significantly higher performance for all training set sizes, except for \"max\", where NOFREEZE and BASELINE perform similarly."
      ]
    ]
  },
  {
    "question": "What is the attribute admiration about?",
    "answers": [
      [
        "Score: 0.95",
        "Finding something impressive or worthy of respect."
      ],
      [
        "Score: 0.31",
        "The emotion categories are: admiration, amusement, anger, annoyance, approval, caring, confusion, curiosity, desire, disappointment, disapproval, disgust, embarrassment, excitement, fear, gratitude, grief, joy, love, nervousness, optimism, pride, realization, relief, remorse, sadness, surprise. This directory includes the data and code for data analysis scripts. We also include code for our baseline model, which involves fine-tuning a pre-trained BERT-base model. For more details on the design and content of the dataset, please see our paper. Refer to our GoEmotions Model Card for recommended uses of models built with this data, as well as considerations and limitations relating to the GoEmotions data. Requirements See requirements.txt Setup Download the pre-trained BERT model from here and unzip them inside the bert directory. In the paper, we use the cased base model. Data Our raw dataset can be retrieved by running:"
      ],
      [
        "Score: 0.19",
        "Figure 1 shows that gratitude, admiration and amusement have the highest and grief and nervousness have the lowest interrater correlation. Emotion frequency correlates with interrater agreement but the two are not equivalent. Infrequent emotions can have relatively high interrater correlation (e.g., fear), and frequent emotions can have have relatively low interrater correlation (e.g., annoyance)."
      ],
      [
        "Score: 0.01",
        "Figure 1 shows the distribution of emotion labels. We can see a large disparity in terms of emotion frequencies (e.g. admiration is 30 times more frequent than grief ), despite our emotion and sentiment balancing steps taken during data selection. This is expected given the disparate frequencies of emotions in natural human expression."
      ],
      [
        "Score: 0.01",
        "Table 4 summarizes the performance of our best model, BERT, on the test set, which achieves an average F1-score of . 46 (std=.19 ). The model obtains the best performance on emotions with overt lexical markers, such as gratitude (.86), amusement (.8) and love (.78). The model obtains the lowest F1-score on grief (0), relief (.15) and realization (.21), which are the lowest frequency emotions. We find that less frequent emotions tend to be confused by the model with more frequent emotions related in sentiment and intensity (e.g., grief with sadness, pride with admiration, nervousness with fear)see Appendix G for a more detailed analysis."
      ]
    ]
  },
  {
    "question": "A description of the attribute admiration?",
    "answers": [
      [
        "Score: 0.93",
        "Finding something impressive or worthy of respect."
      ],
      [
        "Score: 0.25",
        "The emotion categories are: admiration, amusement, anger, annoyance, approval, caring, confusion, curiosity, desire, disappointment, disapproval, disgust, embarrassment, excitement, fear, gratitude, grief, joy, love, nervousness, optimism, pride, realization, relief, remorse, sadness, surprise. This directory includes the data and code for data analysis scripts. We also include code for our baseline model, which involves fine-tuning a pre-trained BERT-base model. For more details on the design and content of the dataset, please see our paper. Refer to our GoEmotions Model Card for recommended uses of models built with this data, as well as considerations and limitations relating to the GoEmotions data. Requirements See requirements.txt Setup Download the pre-trained BERT model from here and unzip them inside the bert directory. In the paper, we use the cased base model. Data Our raw dataset can be retrieved by running:"
      ],
      [
        "Score: 0.05",
        "While reviewing the results from the pilot rounds, we identified and removed emotions that were scarcely selected by annotators and/or had low interrater agreement due to being very similar to other emotions or being too difficult to detect from text. These emotions were boredom, doubt, heartbroken, indifference and calmness. We also identified and added those emotions to our taxonomy that were frequently suggested by raters and/or seemed to be represented in the data upon manual inspection. These emotions were desire, disappointment, pride, realization, relief and remorse. In this process, we also refined the category names (e.g. replacing ecstasy with excitement), to ones that seemed interpretable to annotators. This is how we arrived at the final set of 27 emotions + Neutral. Our high interrater agreement in the final data can be partially explained by the fact that we took interpretability into consideration while constructing the taxonomy. The dataset is we are releasing was labeled in the third round over the final taxonomy."
      ],
      [
        "Score: 0.04",
        "Figure 1 shows that gratitude, admiration and amusement have the highest and grief and nervousness have the lowest interrater correlation. Emotion frequency correlates with interrater agreement but the two are not equivalent. Infrequent emotions can have relatively high interrater correlation (e.g., fear), and frequent emotions can have have relatively low interrater correlation (e.g., annoyance)."
      ],
      [
        "Score: 0.02",
        "Table 4 summarizes the performance of our best model, BERT, on the test set, which achieves an average F1-score of . 46 (std=.19 ). The model obtains the best performance on emotions with overt lexical markers, such as gratitude (.86), amusement (.8) and love (.78). The model obtains the lowest F1-score on grief (0), relief (.15) and realization (.21), which are the lowest frequency emotions. We find that less frequent emotions tend to be confused by the model with more frequent emotions related in sentiment and intensity (e.g., grief with sadness, pride with admiration, nervousness with fear)see Appendix G for a more detailed analysis."
      ]
    ]
  },
  {
    "question": "What is the attribute amusement about?",
    "answers": [
      [
        "Score: 0.02",
        "Figure 1 shows that gratitude, admiration and amusement have the highest and grief and nervousness have the lowest interrater correlation. Emotion frequency correlates with interrater agreement but the two are not equivalent. Infrequent emotions can have relatively high interrater correlation (e.g., fear), and frequent emotions can have have relatively low interrater correlation (e.g., annoyance)."
      ],
      [
        "Score: 0.01",
        "The emotion categories are: admiration, amusement, anger, annoyance, approval, caring, confusion, curiosity, desire, disappointment, disapproval, disgust, embarrassment, excitement, fear, gratitude, grief, joy, love, nervousness, optimism, pride, realization, relief, remorse, sadness, surprise. This directory includes the data and code for data analysis scripts. We also include code for our baseline model, which involves fine-tuning a pre-trained BERT-base model. For more details on the design and content of the dataset, please see our paper. Refer to our GoEmotions Model Card for recommended uses of models built with this data, as well as considerations and limitations relating to the GoEmotions data. Requirements See requirements.txt Setup Download the pre-trained BERT model from here and unzip them inside the bert directory. In the paper, we use the cased base model. Data Our raw dataset can be retrieved by running:"
      ],
      [
        "Score: 0.00",
        "Finding something impressive or worthy of respect."
      ],
      [
        "Score: 0.00",
        "Table 4 summarizes the performance of our best model, BERT, on the test set, which achieves an average F1-score of . 46 (std=.19 ). The model obtains the best performance on emotions with overt lexical markers, such as gratitude (.86), amusement (.8) and love (.78). The model obtains the lowest F1-score on grief (0), relief (.15) and realization (.21), which are the lowest frequency emotions. We find that less frequent emotions tend to be confused by the model with more frequent emotions related in sentiment and intensity (e.g., grief with sadness, pride with admiration, nervousness with fear)see Appendix G for a more detailed analysis."
      ],
      [
        "Score: 0.00",
        "We extract the lexical correlates of each emotion by calculating the log odds ratio, informative Dirichlet prior (Monroe et al., 2008) of all tokens for each emotion category contrasting to all other emotions. Since the log odds are z-scored, all values greater than 3 indicate highly significant (>3 std) association with the corresponding emotion. We list the top 5 tokens for each category in Table 3 . We find that those emotions that are highly significantly associated with certain tokens (e.g. gratitude with \"thanks\", amusement with \"lol\") tend to have the highest interrater correlation (see Figure 1 ). Conversely, emotions that have fewer significantly associated tokens (e.g. grief and nervousness) tend to have low interrater correlation. These results suggest certain emotions are more verbally implicit and may require more context to be interpreted."
      ]
    ]
  },
  {
    "question": "A description of the attribute amusement?",
    "answers": [
      [
        "Score: 0.01",
        "The emotion categories are: admiration, amusement, anger, annoyance, approval, caring, confusion, curiosity, desire, disappointment, disapproval, disgust, embarrassment, excitement, fear, gratitude, grief, joy, love, nervousness, optimism, pride, realization, relief, remorse, sadness, surprise. This directory includes the data and code for data analysis scripts. We also include code for our baseline model, which involves fine-tuning a pre-trained BERT-base model. For more details on the design and content of the dataset, please see our paper. Refer to our GoEmotions Model Card for recommended uses of models built with this data, as well as considerations and limitations relating to the GoEmotions data. Requirements See requirements.txt Setup Download the pre-trained BERT model from here and unzip them inside the bert directory. In the paper, we use the cased base model. Data Our raw dataset can be retrieved by running:"
      ],
      [
        "Score: 0.01",
        "Figure 1 shows that gratitude, admiration and amusement have the highest and grief and nervousness have the lowest interrater correlation. Emotion frequency correlates with interrater agreement but the two are not equivalent. Infrequent emotions can have relatively high interrater correlation (e.g., fear), and frequent emotions can have have relatively low interrater correlation (e.g., annoyance)."
      ],
      [
        "Score: 0.00",
        "Table 4 summarizes the performance of our best model, BERT, on the test set, which achieves an average F1-score of . 46 (std=.19 ). The model obtains the best performance on emotions with overt lexical markers, such as gratitude (.86), amusement (.8) and love (.78). The model obtains the lowest F1-score on grief (0), relief (.15) and realization (.21), which are the lowest frequency emotions. We find that less frequent emotions tend to be confused by the model with more frequent emotions related in sentiment and intensity (e.g., grief with sadness, pride with admiration, nervousness with fear)see Appendix G for a more detailed analysis."
      ],
      [
        "Score: 0.00",
        "Finding something impressive or worthy of respect."
      ],
      [
        "Score: 0.00",
        "While reviewing the results from the pilot rounds, we identified and removed emotions that were scarcely selected by annotators and/or had low interrater agreement due to being very similar to other emotions or being too difficult to detect from text. These emotions were boredom, doubt, heartbroken, indifference and calmness. We also identified and added those emotions to our taxonomy that were frequently suggested by raters and/or seemed to be represented in the data upon manual inspection. These emotions were desire, disappointment, pride, realization, relief and remorse. In this process, we also refined the category names (e.g. replacing ecstasy with excitement), to ones that seemed interpretable to annotators. This is how we arrived at the final set of 27 emotions + Neutral. Our high interrater agreement in the final data can be partially explained by the fact that we took interpretability into consideration while constructing the taxonomy. The dataset is we are releasing was labeled in the third round over the final taxonomy."
      ]
    ]
  },
  {
    "question": "What is the attribute anger about?",
    "answers": [
      [
        "Score: 0.30",
        "The emotion categories are: admiration, amusement, anger, annoyance, approval, caring, confusion, curiosity, desire, disappointment, disapproval, disgust, embarrassment, excitement, fear, gratitude, grief, joy, love, nervousness, optimism, pride, realization, relief, remorse, sadness, surprise. This directory includes the data and code for data analysis scripts. We also include code for our baseline model, which involves fine-tuning a pre-trained BERT-base model. For more details on the design and content of the dataset, please see our paper. Refer to our GoEmotions Model Card for recommended uses of models built with this data, as well as considerations and limitations relating to the GoEmotions data. Requirements See requirements.txt Setup Download the pre-trained BERT model from here and unzip them inside the bert directory. In the paper, we use the cased base model. Data Our raw dataset can be retrieved by running:"
      ],
      [
        "Score: 0.26",
        "To better understand agreement among raters and the latent structure of the emotion space, we apply Principal Preserved Component Analysis (PPCA) (Cowen et al., 2019b) to our data. PPCA extracts linear combinations of attributes (here, emotion judgments), that maximally covary across two sets of data that measure the same attributes (here, randomly split judgments for each example). Thus, PPCA allows us to uncover latent dimensions of J \u2208 R n\u00d7|R|\u00d7|E| \u2190 all ratings for the examples annotated by r 7:"
      ],
      [
        "Score: 0.22",
        "One of the main aspects distinguishing our dataset is its emotion taxonomy. The vast majority of existing datasets contain annotations for minor variations of the 6 basic emotion categories (joy, anger, fear, sadness, disgust, and surprise) proposed by Ekman (1992a) and/or along affective dimensions (valence and arousal) that underpin the circumplex model of affect (Russell, 2003; Buechel and Hahn, 2017) ."
      ],
      [
        "Score: 0.08",
        "We conduct transfer learning experiments on existing emotion benchmarks, in order to show our data generalizes across domains and taxonomies. The goal is to demonstrate that given little labeled data in a target domain, one can utilize GoEmotions as baseline emotion understanding data. ity and taxonomy. In the interest of space, we only discuss three of these datasets here, chosen based on their diversity of domains. In our experiments, we observe similar trends for the additional benchmarks, and all are included in the Appendix H. The International Survey on Emotion Antecedents and Reactions (ISEAR) (Scherer and Wallbott, 1994 ) is a collection of personal reports on emotional events, written by 3000 people from different cultural backgrounds. The dataset contains 8k sentences, each labeled with a single emotion. The categories are anger, disgust, fear, guilt, joy, sadness and shame."
      ],
      [
        "Score: 0.05",
        "We conduct transfer learning experiments with existing emotion benchmarks to show that our data can generalize to different taxonomies and domains, such as tweets and personal narratives. Our experiments demonstrate that given limited resources to label additional emotion classification data for specialized domains, our data can provide baseline emotion understanding and contribute to increasing model accuracy for the target domain."
      ]
    ]
  },
  {
    "question": "A description of the attribute anger?",
    "answers": [
      [
        "Score: 0.31",
        "The emotion categories are: admiration, amusement, anger, annoyance, approval, caring, confusion, curiosity, desire, disappointment, disapproval, disgust, embarrassment, excitement, fear, gratitude, grief, joy, love, nervousness, optimism, pride, realization, relief, remorse, sadness, surprise. This directory includes the data and code for data analysis scripts. We also include code for our baseline model, which involves fine-tuning a pre-trained BERT-base model. For more details on the design and content of the dataset, please see our paper. Refer to our GoEmotions Model Card for recommended uses of models built with this data, as well as considerations and limitations relating to the GoEmotions data. Requirements See requirements.txt Setup Download the pre-trained BERT model from here and unzip them inside the bert directory. In the paper, we use the cased base model. Data Our raw dataset can be retrieved by running:"
      ],
      [
        "Score: 0.29",
        "To better understand agreement among raters and the latent structure of the emotion space, we apply Principal Preserved Component Analysis (PPCA) (Cowen et al., 2019b) to our data. PPCA extracts linear combinations of attributes (here, emotion judgments), that maximally covary across two sets of data that measure the same attributes (here, randomly split judgments for each example). Thus, PPCA allows us to uncover latent dimensions of J \u2208 R n\u00d7|R|\u00d7|E| \u2190 all ratings for the examples annotated by r 7:"
      ],
      [
        "Score: 0.25",
        "While reviewing the results from the pilot rounds, we identified and removed emotions that were scarcely selected by annotators and/or had low interrater agreement due to being very similar to other emotions or being too difficult to detect from text. These emotions were boredom, doubt, heartbroken, indifference and calmness. We also identified and added those emotions to our taxonomy that were frequently suggested by raters and/or seemed to be represented in the data upon manual inspection. These emotions were desire, disappointment, pride, realization, relief and remorse. In this process, we also refined the category names (e.g. replacing ecstasy with excitement), to ones that seemed interpretable to annotators. This is how we arrived at the final set of 27 emotions + Neutral. Our high interrater agreement in the final data can be partially explained by the fact that we took interpretability into consideration while constructing the taxonomy. The dataset is we are releasing was labeled in the third round over the final taxonomy."
      ],
      [
        "Score: 0.25",
        "One of the main aspects distinguishing our dataset is its emotion taxonomy. The vast majority of existing datasets contain annotations for minor variations of the 6 basic emotion categories (joy, anger, fear, sadness, disgust, and surprise) proposed by Ekman (1992a) and/or along affective dimensions (valence and arousal) that underpin the circumplex model of affect (Russell, 2003; Buechel and Hahn, 2017) ."
      ],
      [
        "Score: 0.11",
        "We present GoEmotions, a large, manually annotated, carefully curated dataset for fine-grained emotion prediction. We provide a detailed data analysis, demonstrating the reliability of the annotations for the full taxonomy. We show the generalizability of the data across domains and taxonomies via transfer learning experiments. We build a strong baseline by fine-tuning a BERT model, however, the results suggest much room for future improvement. Future work can explore the cross-cultural robustness of emotion ratings, and extend the taxonomy to other languages and domains."
      ]
    ]
  },
  {
    "question": "What is the attribute annoyance about?",
    "answers": [
      [
        "Score: 0.49",
        "The emotion categories are: admiration, amusement, anger, annoyance, approval, caring, confusion, curiosity, desire, disappointment, disapproval, disgust, embarrassment, excitement, fear, gratitude, grief, joy, love, nervousness, optimism, pride, realization, relief, remorse, sadness, surprise. This directory includes the data and code for data analysis scripts. We also include code for our baseline model, which involves fine-tuning a pre-trained BERT-base model. For more details on the design and content of the dataset, please see our paper. Refer to our GoEmotions Model Card for recommended uses of models built with this data, as well as considerations and limitations relating to the GoEmotions data. Requirements See requirements.txt Setup Download the pre-trained BERT model from here and unzip them inside the bert directory. In the paper, we use the cased base model. Data Our raw dataset can be retrieved by running:"
      ],
      [
        "Score: 0.10",
        "Finding something impressive or worthy of respect."
      ],
      [
        "Score: 0.05",
        "Figure 1 shows that gratitude, admiration and amusement have the highest and grief and nervousness have the lowest interrater correlation. Emotion frequency correlates with interrater agreement but the two are not equivalent. Infrequent emotions can have relatively high interrater correlation (e.g., fear), and frequent emotions can have have relatively low interrater correlation (e.g., annoyance)."
      ],
      [
        "Score: 0.04",
        "Grouping emotions. We create a hierarchical grouping of our taxonomy, and evaluate the model performance on each level of the hierarchy. A sentiment level divides the labels into 4 categoriespositive, negative, ambiguous and Neutral -with the Neutral category intact, and the rest of the mapping as shown in Figure 2 . The Ekman level further divides the taxonomy using the Neutral label and the following 6 groups: anger (maps to: anger, annoyance, disapproval), disgust (maps to: disgust), fear (maps to: fear, nervousness), joy (all positive emotions), sadness (maps to: sadness, disappointment, embarrassment, grief, remorse) and surprise (all ambiguous emotions)."
      ],
      [
        "Score: 0.03",
        "One of the main aspects distinguishing our dataset is its emotion taxonomy. The vast majority of existing datasets contain annotations for minor variations of the 6 basic emotion categories (joy, anger, fear, sadness, disgust, and surprise) proposed by Ekman (1992a) and/or along affective dimensions (valence and arousal) that underpin the circumplex model of affect (Russell, 2003; Buechel and Hahn, 2017) ."
      ]
    ]
  },
  {
    "question": "A description of the attribute annoyance?",
    "answers": [
      [
        "Score: 0.43",
        "The emotion categories are: admiration, amusement, anger, annoyance, approval, caring, confusion, curiosity, desire, disappointment, disapproval, disgust, embarrassment, excitement, fear, gratitude, grief, joy, love, nervousness, optimism, pride, realization, relief, remorse, sadness, surprise. This directory includes the data and code for data analysis scripts. We also include code for our baseline model, which involves fine-tuning a pre-trained BERT-base model. For more details on the design and content of the dataset, please see our paper. Refer to our GoEmotions Model Card for recommended uses of models built with this data, as well as considerations and limitations relating to the GoEmotions data. Requirements See requirements.txt Setup Download the pre-trained BERT model from here and unzip them inside the bert directory. In the paper, we use the cased base model. Data Our raw dataset can be retrieved by running:"
      ],
      [
        "Score: 0.15",
        "Grouping emotions. We create a hierarchical grouping of our taxonomy, and evaluate the model performance on each level of the hierarchy. A sentiment level divides the labels into 4 categoriespositive, negative, ambiguous and Neutral -with the Neutral category intact, and the rest of the mapping as shown in Figure 2 . The Ekman level further divides the taxonomy using the Neutral label and the following 6 groups: anger (maps to: anger, annoyance, disapproval), disgust (maps to: disgust), fear (maps to: fear, nervousness), joy (all positive emotions), sadness (maps to: sadness, disappointment, embarrassment, grief, remorse) and surprise (all ambiguous emotions)."
      ],
      [
        "Score: 0.09",
        "While reviewing the results from the pilot rounds, we identified and removed emotions that were scarcely selected by annotators and/or had low interrater agreement due to being very similar to other emotions or being too difficult to detect from text. These emotions were boredom, doubt, heartbroken, indifference and calmness. We also identified and added those emotions to our taxonomy that were frequently suggested by raters and/or seemed to be represented in the data upon manual inspection. These emotions were desire, disappointment, pride, realization, relief and remorse. In this process, we also refined the category names (e.g. replacing ecstasy with excitement), to ones that seemed interpretable to annotators. This is how we arrived at the final set of 27 emotions + Neutral. Our high interrater agreement in the final data can be partially explained by the fact that we took interpretability into consideration while constructing the taxonomy. The dataset is we are releasing was labeled in the third round over the final taxonomy."
      ],
      [
        "Score: 0.07",
        "Finding something impressive or worthy of respect."
      ],
      [
        "Score: 0.03",
        "One of the main aspects distinguishing our dataset is its emotion taxonomy. The vast majority of existing datasets contain annotations for minor variations of the 6 basic emotion categories (joy, anger, fear, sadness, disgust, and surprise) proposed by Ekman (1992a) and/or along affective dimensions (valence and arousal) that underpin the circumplex model of affect (Russell, 2003; Buechel and Hahn, 2017) ."
      ]
    ]
  },
  {
    "question": "What is the attribute approval about?",
    "answers": [
      [
        "Score: 0.01",
        "The emotion categories are: admiration, amusement, anger, annoyance, approval, caring, confusion, curiosity, desire, disappointment, disapproval, disgust, embarrassment, excitement, fear, gratitude, grief, joy, love, nervousness, optimism, pride, realization, relief, remorse, sadness, surprise. This directory includes the data and code for data analysis scripts. We also include code for our baseline model, which involves fine-tuning a pre-trained BERT-base model. For more details on the design and content of the dataset, please see our paper. Refer to our GoEmotions Model Card for recommended uses of models built with this data, as well as considerations and limitations relating to the GoEmotions data. Requirements See requirements.txt Setup Download the pre-trained BERT model from here and unzip them inside the bert directory. In the paper, we use the cased base model. Data Our raw dataset can be retrieved by running:"
      ],
      [
        "Score: 0.00",
        "To better understand agreement among raters and the latent structure of the emotion space, we apply Principal Preserved Component Analysis (PPCA) (Cowen et al., 2019b) to our data. PPCA extracts linear combinations of attributes (here, emotion judgments), that maximally covary across two sets of data that measure the same attributes (here, randomly split judgments for each example). Thus, PPCA allows us to uncover latent dimensions of J \u2208 R n\u00d7|R|\u00d7|E| \u2190 all ratings for the examples annotated by r 7:"
      ],
      [
        "Score: 0.00",
        "While reviewing the results from the pilot rounds, we identified and removed emotions that were scarcely selected by annotators and/or had low interrater agreement due to being very similar to other emotions or being too difficult to detect from text. These emotions were boredom, doubt, heartbroken, indifference and calmness. We also identified and added those emotions to our taxonomy that were frequently suggested by raters and/or seemed to be represented in the data upon manual inspection. These emotions were desire, disappointment, pride, realization, relief and remorse. In this process, we also refined the category names (e.g. replacing ecstasy with excitement), to ones that seemed interpretable to annotators. This is how we arrived at the final set of 27 emotions + Neutral. Our high interrater agreement in the final data can be partially explained by the fact that we took interpretability into consideration while constructing the taxonomy. The dataset is we are releasing was labeled in the third round over the final taxonomy."
      ],
      [
        "Score: 0.00",
        "Finding something impressive or worthy of respect."
      ],
      [
        "Score: 0.00",
        "The rater interface. Reddit comments were presented with no additional metadata (such as the author or subreddit). To help raters navigate the large space of emotion in our taxonomy, they were presented a table containing all emotion categories aggregated by sentiment (by the mapping in Figure 2 ) and whether that emotion is generally expressed towards something (e.g. disapproval) or is more of an intrinsic feeling (e.g. joy). The instructions highlighted that this separation of categories was by no means clear-cut, but captured general tendencies, and we encouraged raters to ignore the categorization whenever they saw fit. Emotions with a straightforward mapping onto emojis were shown with an emoji in the UI, to further ease their interpretation."
      ]
    ]
  },
  {
    "question": "A description of the attribute approval?",
    "answers": [
      [
        "Score: 0.01",
        "While reviewing the results from the pilot rounds, we identified and removed emotions that were scarcely selected by annotators and/or had low interrater agreement due to being very similar to other emotions or being too difficult to detect from text. These emotions were boredom, doubt, heartbroken, indifference and calmness. We also identified and added those emotions to our taxonomy that were frequently suggested by raters and/or seemed to be represented in the data upon manual inspection. These emotions were desire, disappointment, pride, realization, relief and remorse. In this process, we also refined the category names (e.g. replacing ecstasy with excitement), to ones that seemed interpretable to annotators. This is how we arrived at the final set of 27 emotions + Neutral. Our high interrater agreement in the final data can be partially explained by the fact that we took interpretability into consideration while constructing the taxonomy. The dataset is we are releasing was labeled in the third round over the final taxonomy."
      ],
      [
        "Score: 0.01",
        "The emotion categories are: admiration, amusement, anger, annoyance, approval, caring, confusion, curiosity, desire, disappointment, disapproval, disgust, embarrassment, excitement, fear, gratitude, grief, joy, love, nervousness, optimism, pride, realization, relief, remorse, sadness, surprise. This directory includes the data and code for data analysis scripts. We also include code for our baseline model, which involves fine-tuning a pre-trained BERT-base model. For more details on the design and content of the dataset, please see our paper. Refer to our GoEmotions Model Card for recommended uses of models built with this data, as well as considerations and limitations relating to the GoEmotions data. Requirements See requirements.txt Setup Download the pre-trained BERT model from here and unzip them inside the bert directory. In the paper, we use the cased base model. Data Our raw dataset can be retrieved by running:"
      ],
      [
        "Score: 0.00",
        "To better understand agreement among raters and the latent structure of the emotion space, we apply Principal Preserved Component Analysis (PPCA) (Cowen et al., 2019b) to our data. PPCA extracts linear combinations of attributes (here, emotion judgments), that maximally covary across two sets of data that measure the same attributes (here, randomly split judgments for each example). Thus, PPCA allows us to uncover latent dimensions of J \u2208 R n\u00d7|R|\u00d7|E| \u2190 all ratings for the examples annotated by r 7:"
      ],
      [
        "Score: 0.00",
        "We present GoEmotions, a large, manually annotated, carefully curated dataset for fine-grained emotion prediction. We provide a detailed data analysis, demonstrating the reliability of the annotations for the full taxonomy. We show the generalizability of the data across domains and taxonomies via transfer learning experiments. We build a strong baseline by fine-tuning a BERT model, however, the results suggest much room for future improvement. Future work can explore the cross-cultural robustness of emotion ratings, and extend the taxonomy to other languages and domains."
      ],
      [
        "Score: 0.00",
        "We find that all 27 PPCs are highly significant. Specifically, Bonferroni-corrected p-values are less than 1.5e-6 for all dimensions (corrected \u03b1 = 0.0017), suggesting that the emotions were highly dissociable. Such a high degree of significance for all dimensions is nontrivial. For example, Cowen et al. (2019b) find that only 12 out of their 30 emotion categories are significantly dissociable."
      ]
    ]
  },
  {
    "question": "What is the attribute caring about?",
    "answers": [
      [
        "Score: 0.02",
        "Finding something impressive or worthy of respect."
      ],
      [
        "Score: 0.01",
        "The emotion categories are: admiration, amusement, anger, annoyance, approval, caring, confusion, curiosity, desire, disappointment, disapproval, disgust, embarrassment, excitement, fear, gratitude, grief, joy, love, nervousness, optimism, pride, realization, relief, remorse, sadness, surprise. This directory includes the data and code for data analysis scripts. We also include code for our baseline model, which involves fine-tuning a pre-trained BERT-base model. For more details on the design and content of the dataset, please see our paper. Refer to our GoEmotions Model Card for recommended uses of models built with this data, as well as considerations and limitations relating to the GoEmotions data. Requirements See requirements.txt Setup Download the pre-trained BERT model from here and unzip them inside the bert directory. In the paper, we use the cased base model. Data Our raw dataset can be retrieved by running:"
      ],
      [
        "Score: 0.00",
        "To better understand agreement among raters and the latent structure of the emotion space, we apply Principal Preserved Component Analysis (PPCA) (Cowen et al., 2019b) to our data. PPCA extracts linear combinations of attributes (here, emotion judgments), that maximally covary across two sets of data that measure the same attributes (here, randomly split judgments for each example). Thus, PPCA allows us to uncover latent dimensions of J \u2208 R n\u00d7|R|\u00d7|E| \u2190 all ratings for the examples annotated by r 7:"
      ],
      [
        "Score: 0.00",
        "While reviewing the results from the pilot rounds, we identified and removed emotions that were scarcely selected by annotators and/or had low interrater agreement due to being very similar to other emotions or being too difficult to detect from text. These emotions were boredom, doubt, heartbroken, indifference and calmness. We also identified and added those emotions to our taxonomy that were frequently suggested by raters and/or seemed to be represented in the data upon manual inspection. These emotions were desire, disappointment, pride, realization, relief and remorse. In this process, we also refined the category names (e.g. replacing ecstasy with excitement), to ones that seemed interpretable to annotators. This is how we arrived at the final set of 27 emotions + Neutral. Our high interrater agreement in the final data can be partially explained by the fact that we took interpretability into consideration while constructing the taxonomy. The dataset is we are releasing was labeled in the third round over the final taxonomy."
      ],
      [
        "Score: 0.00",
        "We conduct transfer learning experiments on existing emotion benchmarks, in order to show our data generalizes across domains and taxonomies. The goal is to demonstrate that given little labeled data in a target domain, one can utilize GoEmotions as baseline emotion understanding data. ity and taxonomy. In the interest of space, we only discuss three of these datasets here, chosen based on their diversity of domains. In our experiments, we observe similar trends for the additional benchmarks, and all are included in the Appendix H. The International Survey on Emotion Antecedents and Reactions (ISEAR) (Scherer and Wallbott, 1994 ) is a collection of personal reports on emotional events, written by 3000 people from different cultural backgrounds. The dataset contains 8k sentences, each labeled with a single emotion. The categories are anger, disgust, fear, guilt, joy, sadness and shame."
      ]
    ]
  },
  {
    "question": "A description of the attribute caring?",
    "answers": [
      [
        "Score: 0.01",
        "While reviewing the results from the pilot rounds, we identified and removed emotions that were scarcely selected by annotators and/or had low interrater agreement due to being very similar to other emotions or being too difficult to detect from text. These emotions were boredom, doubt, heartbroken, indifference and calmness. We also identified and added those emotions to our taxonomy that were frequently suggested by raters and/or seemed to be represented in the data upon manual inspection. These emotions were desire, disappointment, pride, realization, relief and remorse. In this process, we also refined the category names (e.g. replacing ecstasy with excitement), to ones that seemed interpretable to annotators. This is how we arrived at the final set of 27 emotions + Neutral. Our high interrater agreement in the final data can be partially explained by the fact that we took interpretability into consideration while constructing the taxonomy. The dataset is we are releasing was labeled in the third round over the final taxonomy."
      ],
      [
        "Score: 0.01",
        "Finding something impressive or worthy of respect."
      ],
      [
        "Score: 0.01",
        "The emotion categories are: admiration, amusement, anger, annoyance, approval, caring, confusion, curiosity, desire, disappointment, disapproval, disgust, embarrassment, excitement, fear, gratitude, grief, joy, love, nervousness, optimism, pride, realization, relief, remorse, sadness, surprise. This directory includes the data and code for data analysis scripts. We also include code for our baseline model, which involves fine-tuning a pre-trained BERT-base model. For more details on the design and content of the dataset, please see our paper. Refer to our GoEmotions Model Card for recommended uses of models built with this data, as well as considerations and limitations relating to the GoEmotions data. Requirements See requirements.txt Setup Download the pre-trained BERT model from here and unzip them inside the bert directory. In the paper, we use the cased base model. Data Our raw dataset can be retrieved by running:"
      ],
      [
        "Score: 0.01",
        "To better understand agreement among raters and the latent structure of the emotion space, we apply Principal Preserved Component Analysis (PPCA) (Cowen et al., 2019b) to our data. PPCA extracts linear combinations of attributes (here, emotion judgments), that maximally covary across two sets of data that measure the same attributes (here, randomly split judgments for each example). Thus, PPCA allows us to uncover latent dimensions of J \u2208 R n\u00d7|R|\u00d7|E| \u2190 all ratings for the examples annotated by r 7:"
      ],
      [
        "Score: 0.00",
        "description: ### Context GoEmotions is a corpus of 58k carefully curated comments extracted from Reddit, with human annotations to 27 emotion categories or Neutral.  - Number of examples: 58,009. - Number of labels: 27 + Neutral. - Maximum sequence length in training and evaluation datasets: 30.   ### Content On top of the raw data, we also include a version filtered based on reter-agreement, which contains a train/test/validation split:  - Size of training dataset: 43,410. - Size of test dataset: 5,427. - Size of validation dataset: 5,426. The emotion categories are: admiration, amusement, anger, annoyance, approval, caring, confusion, curiosity, desire, disappointment, disapproval, disgust, embarrassment, excitement, fear, gratitude, grief, joy, love, nervousness, optimism, pride, realization, relief, remorse, sadness, surprise.  `analyze_data.py` and `extract_words.py` have already been run and the output files are stored in `plots` and `tables` respectively. What's inside is more than just rows and columns. Make it easy for others to get started by describing how you acquired the data and what time period it represents, too. Original repository from where the data is taken: [https://github.com/google-research/google-research/tree/master/goemotions](https://github.com/google-research/google-research/tree/master/goemotions) However, some of the scripts were giving errors which have been rectified and updated in this repository: [https://github.com/DebarshiChanda/google-research/tree/master/goemotions](https://github.com/DebarshiChanda/google-research/tree/master/goemotions)  ### Acknowledgements The entire Credit for creating this dataset goes to Google Research Team.   ### Inspiration Detect emotion from the text using Multi-label Classification"
      ]
    ]
  },
  {
    "question": "What is the attribute confusion about?",
    "answers": [
      [
        "Score: 0.02",
        "Figure 6 shows the normalized confusion matrix for our model predictions. Since GoEmotions is a multilabel dataset, we calculate the confusion matrix similarly as we would calculate a co-occurrence matrix: for each true label, we increase the count for each predicted label. Specifically, we define a matrix M where M i,j denotes the raw confusion count between the true label i and the predicted label j. For example, if the true labels are joy and admiration, and the predicted labels are joy and pride, then we increase the count for M joy,joy , M joy,pride , M admiration,joy and M admiration,pride ."
      ],
      [
        "Score: 0.02",
        "We also perform hierarchical clustering over the normalized confusion matrix using correlation as a distance metric and ward as a linkage method. We find that the model learns relatively similar clusters as the ones in Figure 2 , even though the training data only includes a subset of the labels that have agreement (see Figure 5 )."
      ],
      [
        "Score: 0.02",
        "The emotion categories are: admiration, amusement, anger, annoyance, approval, caring, confusion, curiosity, desire, disappointment, disapproval, disgust, embarrassment, excitement, fear, gratitude, grief, joy, love, nervousness, optimism, pride, realization, relief, remorse, sadness, surprise. This directory includes the data and code for data analysis scripts. We also include code for our baseline model, which involves fine-tuning a pre-trained BERT-base model. For more details on the design and content of the dataset, please see our paper. Refer to our GoEmotions Model Card for recommended uses of models built with this data, as well as considerations and limitations relating to the GoEmotions data. Requirements See requirements.txt Setup Download the pre-trained BERT model from here and unzip them inside the bert directory. In the paper, we use the cased base model. Data Our raw dataset can be retrieved by running:"
      ],
      [
        "Score: 0.01",
        "In practice, since most of our examples only has a single label (see Figure 5 ), our confusion matrix is very similar to one calculated for a single-label classification task."
      ],
      [
        "Score: 0.01",
        "One of the main aspects distinguishing our dataset is its emotion taxonomy. The vast majority of existing datasets contain annotations for minor variations of the 6 basic emotion categories (joy, anger, fear, sadness, disgust, and surprise) proposed by Ekman (1992a) and/or along affective dimensions (valence and arousal) that underpin the circumplex model of affect (Russell, 2003; Buechel and Hahn, 2017) ."
      ]
    ]
  },
  {
    "question": "A description of the attribute confusion?",
    "answers": [
      [
        "Score: 0.02",
        "We also perform hierarchical clustering over the normalized confusion matrix using correlation as a distance metric and ward as a linkage method. We find that the model learns relatively similar clusters as the ones in Figure 2 , even though the training data only includes a subset of the labels that have agreement (see Figure 5 )."
      ],
      [
        "Score: 0.02",
        "While reviewing the results from the pilot rounds, we identified and removed emotions that were scarcely selected by annotators and/or had low interrater agreement due to being very similar to other emotions or being too difficult to detect from text. These emotions were boredom, doubt, heartbroken, indifference and calmness. We also identified and added those emotions to our taxonomy that were frequently suggested by raters and/or seemed to be represented in the data upon manual inspection. These emotions were desire, disappointment, pride, realization, relief and remorse. In this process, we also refined the category names (e.g. replacing ecstasy with excitement), to ones that seemed interpretable to annotators. This is how we arrived at the final set of 27 emotions + Neutral. Our high interrater agreement in the final data can be partially explained by the fact that we took interpretability into consideration while constructing the taxonomy. The dataset is we are releasing was labeled in the third round over the final taxonomy."
      ],
      [
        "Score: 0.02",
        "The emotion categories are: admiration, amusement, anger, annoyance, approval, caring, confusion, curiosity, desire, disappointment, disapproval, disgust, embarrassment, excitement, fear, gratitude, grief, joy, love, nervousness, optimism, pride, realization, relief, remorse, sadness, surprise. This directory includes the data and code for data analysis scripts. We also include code for our baseline model, which involves fine-tuning a pre-trained BERT-base model. For more details on the design and content of the dataset, please see our paper. Refer to our GoEmotions Model Card for recommended uses of models built with this data, as well as considerations and limitations relating to the GoEmotions data. Requirements See requirements.txt Setup Download the pre-trained BERT model from here and unzip them inside the bert directory. In the paper, we use the cased base model. Data Our raw dataset can be retrieved by running:"
      ],
      [
        "Score: 0.01",
        "Figure 6 shows the normalized confusion matrix for our model predictions. Since GoEmotions is a multilabel dataset, we calculate the confusion matrix similarly as we would calculate a co-occurrence matrix: for each true label, we increase the count for each predicted label. Specifically, we define a matrix M where M i,j denotes the raw confusion count between the true label i and the predicted label j. For example, if the true labels are joy and admiration, and the predicted labels are joy and pride, then we increase the count for M joy,joy , M joy,pride , M admiration,joy and M admiration,pride ."
      ],
      [
        "Score: 0.01",
        "In practice, since most of our examples only has a single label (see Figure 5 ), our confusion matrix is very similar to one calculated for a single-label classification task."
      ]
    ]
  },
  {
    "question": "What is the attribute curiosity about?",
    "answers": [
      [
        "Score: 0.03",
        "The emotion categories are: admiration, amusement, anger, annoyance, approval, caring, confusion, curiosity, desire, disappointment, disapproval, disgust, embarrassment, excitement, fear, gratitude, grief, joy, love, nervousness, optimism, pride, realization, relief, remorse, sadness, surprise. This directory includes the data and code for data analysis scripts. We also include code for our baseline model, which involves fine-tuning a pre-trained BERT-base model. For more details on the design and content of the dataset, please see our paper. Refer to our GoEmotions Model Card for recommended uses of models built with this data, as well as considerations and limitations relating to the GoEmotions data. Requirements See requirements.txt Setup Download the pre-trained BERT model from here and unzip them inside the bert directory. In the paper, we use the cased base model. Data Our raw dataset can be retrieved by running:"
      ],
      [
        "Score: 0.01",
        "While reviewing the results from the pilot rounds, we identified and removed emotions that were scarcely selected by annotators and/or had low interrater agreement due to being very similar to other emotions or being too difficult to detect from text. These emotions were boredom, doubt, heartbroken, indifference and calmness. We also identified and added those emotions to our taxonomy that were frequently suggested by raters and/or seemed to be represented in the data upon manual inspection. These emotions were desire, disappointment, pride, realization, relief and remorse. In this process, we also refined the category names (e.g. replacing ecstasy with excitement), to ones that seemed interpretable to annotators. This is how we arrived at the final set of 27 emotions + Neutral. Our high interrater agreement in the final data can be partially explained by the fact that we took interpretability into consideration while constructing the taxonomy. The dataset is we are releasing was labeled in the third round over the final taxonomy."
      ],
      [
        "Score: 0.01",
        "Finding something impressive or worthy of respect."
      ],
      [
        "Score: 0.01",
        "Figure 1 shows that gratitude, admiration and amusement have the highest and grief and nervousness have the lowest interrater correlation. Emotion frequency correlates with interrater agreement but the two are not equivalent. Infrequent emotions can have relatively high interrater correlation (e.g., fear), and frequent emotions can have have relatively low interrater correlation (e.g., annoyance)."
      ],
      [
        "Score: 0.01",
        "To better understand agreement among raters and the latent structure of the emotion space, we apply Principal Preserved Component Analysis (PPCA) (Cowen et al., 2019b) to our data. PPCA extracts linear combinations of attributes (here, emotion judgments), that maximally covary across two sets of data that measure the same attributes (here, randomly split judgments for each example). Thus, PPCA allows us to uncover latent dimensions of J \u2208 R n\u00d7|R|\u00d7|E| \u2190 all ratings for the examples annotated by r 7:"
      ]
    ]
  },
  {
    "question": "A description of the attribute curiosity?",
    "answers": [
      [
        "Score: 0.09",
        "While reviewing the results from the pilot rounds, we identified and removed emotions that were scarcely selected by annotators and/or had low interrater agreement due to being very similar to other emotions or being too difficult to detect from text. These emotions were boredom, doubt, heartbroken, indifference and calmness. We also identified and added those emotions to our taxonomy that were frequently suggested by raters and/or seemed to be represented in the data upon manual inspection. These emotions were desire, disappointment, pride, realization, relief and remorse. In this process, we also refined the category names (e.g. replacing ecstasy with excitement), to ones that seemed interpretable to annotators. This is how we arrived at the final set of 27 emotions + Neutral. Our high interrater agreement in the final data can be partially explained by the fact that we took interpretability into consideration while constructing the taxonomy. The dataset is we are releasing was labeled in the third round over the final taxonomy."
      ],
      [
        "Score: 0.03",
        "The emotion categories are: admiration, amusement, anger, annoyance, approval, caring, confusion, curiosity, desire, disappointment, disapproval, disgust, embarrassment, excitement, fear, gratitude, grief, joy, love, nervousness, optimism, pride, realization, relief, remorse, sadness, surprise. This directory includes the data and code for data analysis scripts. We also include code for our baseline model, which involves fine-tuning a pre-trained BERT-base model. For more details on the design and content of the dataset, please see our paper. Refer to our GoEmotions Model Card for recommended uses of models built with this data, as well as considerations and limitations relating to the GoEmotions data. Requirements See requirements.txt Setup Download the pre-trained BERT model from here and unzip them inside the bert directory. In the paper, we use the cased base model. Data Our raw dataset can be retrieved by running:"
      ],
      [
        "Score: 0.01",
        "We present GoEmotions, a large, manually annotated, carefully curated dataset for fine-grained emotion prediction. We provide a detailed data analysis, demonstrating the reliability of the annotations for the full taxonomy. We show the generalizability of the data across domains and taxonomies via transfer learning experiments. We build a strong baseline by fine-tuning a BERT model, however, the results suggest much room for future improvement. Future work can explore the cross-cultural robustness of emotion ratings, and extend the taxonomy to other languages and domains."
      ],
      [
        "Score: 0.01",
        "Finding something impressive or worthy of respect."
      ],
      [
        "Score: 0.01",
        "To better understand agreement among raters and the latent structure of the emotion space, we apply Principal Preserved Component Analysis (PPCA) (Cowen et al., 2019b) to our data. PPCA extracts linear combinations of attributes (here, emotion judgments), that maximally covary across two sets of data that measure the same attributes (here, randomly split judgments for each example). Thus, PPCA allows us to uncover latent dimensions of J \u2208 R n\u00d7|R|\u00d7|E| \u2190 all ratings for the examples annotated by r 7:"
      ]
    ]
  },
  {
    "question": "What is the attribute desire about?",
    "answers": [
      [
        "Score: 0.90",
        "Finding something impressive or worthy of respect."
      ],
      [
        "Score: 0.02",
        "While reviewing the results from the pilot rounds, we identified and removed emotions that were scarcely selected by annotators and/or had low interrater agreement due to being very similar to other emotions or being too difficult to detect from text. These emotions were boredom, doubt, heartbroken, indifference and calmness. We also identified and added those emotions to our taxonomy that were frequently suggested by raters and/or seemed to be represented in the data upon manual inspection. These emotions were desire, disappointment, pride, realization, relief and remorse. In this process, we also refined the category names (e.g. replacing ecstasy with excitement), to ones that seemed interpretable to annotators. This is how we arrived at the final set of 27 emotions + Neutral. Our high interrater agreement in the final data can be partially explained by the fact that we took interpretability into consideration while constructing the taxonomy. The dataset is we are releasing was labeled in the third round over the final taxonomy."
      ],
      [
        "Score: 0.01",
        "To better understand agreement among raters and the latent structure of the emotion space, we apply Principal Preserved Component Analysis (PPCA) (Cowen et al., 2019b) to our data. PPCA extracts linear combinations of attributes (here, emotion judgments), that maximally covary across two sets of data that measure the same attributes (here, randomly split judgments for each example). Thus, PPCA allows us to uncover latent dimensions of J \u2208 R n\u00d7|R|\u00d7|E| \u2190 all ratings for the examples annotated by r 7:"
      ],
      [
        "Score: 0.01",
        "The emotion categories are: admiration, amusement, anger, annoyance, approval, caring, confusion, curiosity, desire, disappointment, disapproval, disgust, embarrassment, excitement, fear, gratitude, grief, joy, love, nervousness, optimism, pride, realization, relief, remorse, sadness, surprise. This directory includes the data and code for data analysis scripts. We also include code for our baseline model, which involves fine-tuning a pre-trained BERT-base model. For more details on the design and content of the dataset, please see our paper. Refer to our GoEmotions Model Card for recommended uses of models built with this data, as well as considerations and limitations relating to the GoEmotions data. Requirements See requirements.txt Setup Download the pre-trained BERT model from here and unzip them inside the bert directory. In the paper, we use the cased base model. Data Our raw dataset can be retrieved by running:"
      ],
      [
        "Score: 0.01",
        "One of the main aspects distinguishing our dataset is its emotion taxonomy. The vast majority of existing datasets contain annotations for minor variations of the 6 basic emotion categories (joy, anger, fear, sadness, disgust, and surprise) proposed by Ekman (1992a) and/or along affective dimensions (valence and arousal) that underpin the circumplex model of affect (Russell, 2003; Buechel and Hahn, 2017) ."
      ]
    ]
  },
  {
    "question": "A description of the attribute desire?",
    "answers": [
      [
        "Score: 0.84",
        "Finding something impressive or worthy of respect."
      ],
      [
        "Score: 0.16",
        "While reviewing the results from the pilot rounds, we identified and removed emotions that were scarcely selected by annotators and/or had low interrater agreement due to being very similar to other emotions or being too difficult to detect from text. These emotions were boredom, doubt, heartbroken, indifference and calmness. We also identified and added those emotions to our taxonomy that were frequently suggested by raters and/or seemed to be represented in the data upon manual inspection. These emotions were desire, disappointment, pride, realization, relief and remorse. In this process, we also refined the category names (e.g. replacing ecstasy with excitement), to ones that seemed interpretable to annotators. This is how we arrived at the final set of 27 emotions + Neutral. Our high interrater agreement in the final data can be partially explained by the fact that we took interpretability into consideration while constructing the taxonomy. The dataset is we are releasing was labeled in the third round over the final taxonomy."
      ],
      [
        "Score: 0.01",
        "To better understand agreement among raters and the latent structure of the emotion space, we apply Principal Preserved Component Analysis (PPCA) (Cowen et al., 2019b) to our data. PPCA extracts linear combinations of attributes (here, emotion judgments), that maximally covary across two sets of data that measure the same attributes (here, randomly split judgments for each example). Thus, PPCA allows us to uncover latent dimensions of J \u2208 R n\u00d7|R|\u00d7|E| \u2190 all ratings for the examples annotated by r 7:"
      ],
      [
        "Score: 0.01",
        "One of the main aspects distinguishing our dataset is its emotion taxonomy. The vast majority of existing datasets contain annotations for minor variations of the 6 basic emotion categories (joy, anger, fear, sadness, disgust, and surprise) proposed by Ekman (1992a) and/or along affective dimensions (valence and arousal) that underpin the circumplex model of affect (Russell, 2003; Buechel and Hahn, 2017) ."
      ],
      [
        "Score: 0.01",
        "We present GoEmotions, a large, manually annotated, carefully curated dataset for fine-grained emotion prediction. We provide a detailed data analysis, demonstrating the reliability of the annotations for the full taxonomy. We show the generalizability of the data across domains and taxonomies via transfer learning experiments. We build a strong baseline by fine-tuning a BERT model, however, the results suggest much room for future improvement. Future work can explore the cross-cultural robustness of emotion ratings, and extend the taxonomy to other languages and domains."
      ]
    ]
  },
  {
    "question": "What is the attribute disappointment about?",
    "answers": [
      [
        "Score: 0.24",
        "The emotion categories are: admiration, amusement, anger, annoyance, approval, caring, confusion, curiosity, desire, disappointment, disapproval, disgust, embarrassment, excitement, fear, gratitude, grief, joy, love, nervousness, optimism, pride, realization, relief, remorse, sadness, surprise. This directory includes the data and code for data analysis scripts. We also include code for our baseline model, which involves fine-tuning a pre-trained BERT-base model. For more details on the design and content of the dataset, please see our paper. Refer to our GoEmotions Model Card for recommended uses of models built with this data, as well as considerations and limitations relating to the GoEmotions data. Requirements See requirements.txt Setup Download the pre-trained BERT model from here and unzip them inside the bert directory. In the paper, we use the cased base model. Data Our raw dataset can be retrieved by running:"
      ],
      [
        "Score: 0.23",
        "Finding something impressive or worthy of respect."
      ],
      [
        "Score: 0.04",
        "While reviewing the results from the pilot rounds, we identified and removed emotions that were scarcely selected by annotators and/or had low interrater agreement due to being very similar to other emotions or being too difficult to detect from text. These emotions were boredom, doubt, heartbroken, indifference and calmness. We also identified and added those emotions to our taxonomy that were frequently suggested by raters and/or seemed to be represented in the data upon manual inspection. These emotions were desire, disappointment, pride, realization, relief and remorse. In this process, we also refined the category names (e.g. replacing ecstasy with excitement), to ones that seemed interpretable to annotators. This is how we arrived at the final set of 27 emotions + Neutral. Our high interrater agreement in the final data can be partially explained by the fact that we took interpretability into consideration while constructing the taxonomy. The dataset is we are releasing was labeled in the third round over the final taxonomy."
      ],
      [
        "Score: 0.04",
        "Grouping emotions. We create a hierarchical grouping of our taxonomy, and evaluate the model performance on each level of the hierarchy. A sentiment level divides the labels into 4 categoriespositive, negative, ambiguous and Neutral -with the Neutral category intact, and the rest of the mapping as shown in Figure 2 . The Ekman level further divides the taxonomy using the Neutral label and the following 6 groups: anger (maps to: anger, annoyance, disapproval), disgust (maps to: disgust), fear (maps to: fear, nervousness), joy (all positive emotions), sadness (maps to: sadness, disappointment, embarrassment, grief, remorse) and surprise (all ambiguous emotions)."
      ],
      [
        "Score: 0.03",
        "One of the main aspects distinguishing our dataset is its emotion taxonomy. The vast majority of existing datasets contain annotations for minor variations of the 6 basic emotion categories (joy, anger, fear, sadness, disgust, and surprise) proposed by Ekman (1992a) and/or along affective dimensions (valence and arousal) that underpin the circumplex model of affect (Russell, 2003; Buechel and Hahn, 2017) ."
      ]
    ]
  },
  {
    "question": "A description of the attribute disappointment?",
    "answers": [
      [
        "Score: 0.34",
        "While reviewing the results from the pilot rounds, we identified and removed emotions that were scarcely selected by annotators and/or had low interrater agreement due to being very similar to other emotions or being too difficult to detect from text. These emotions were boredom, doubt, heartbroken, indifference and calmness. We also identified and added those emotions to our taxonomy that were frequently suggested by raters and/or seemed to be represented in the data upon manual inspection. These emotions were desire, disappointment, pride, realization, relief and remorse. In this process, we also refined the category names (e.g. replacing ecstasy with excitement), to ones that seemed interpretable to annotators. This is how we arrived at the final set of 27 emotions + Neutral. Our high interrater agreement in the final data can be partially explained by the fact that we took interpretability into consideration while constructing the taxonomy. The dataset is we are releasing was labeled in the third round over the final taxonomy."
      ],
      [
        "Score: 0.15",
        "The emotion categories are: admiration, amusement, anger, annoyance, approval, caring, confusion, curiosity, desire, disappointment, disapproval, disgust, embarrassment, excitement, fear, gratitude, grief, joy, love, nervousness, optimism, pride, realization, relief, remorse, sadness, surprise. This directory includes the data and code for data analysis scripts. We also include code for our baseline model, which involves fine-tuning a pre-trained BERT-base model. For more details on the design and content of the dataset, please see our paper. Refer to our GoEmotions Model Card for recommended uses of models built with this data, as well as considerations and limitations relating to the GoEmotions data. Requirements See requirements.txt Setup Download the pre-trained BERT model from here and unzip them inside the bert directory. In the paper, we use the cased base model. Data Our raw dataset can be retrieved by running:"
      ],
      [
        "Score: 0.09",
        "Grouping emotions. We create a hierarchical grouping of our taxonomy, and evaluate the model performance on each level of the hierarchy. A sentiment level divides the labels into 4 categoriespositive, negative, ambiguous and Neutral -with the Neutral category intact, and the rest of the mapping as shown in Figure 2 . The Ekman level further divides the taxonomy using the Neutral label and the following 6 groups: anger (maps to: anger, annoyance, disapproval), disgust (maps to: disgust), fear (maps to: fear, nervousness), joy (all positive emotions), sadness (maps to: sadness, disappointment, embarrassment, grief, remorse) and surprise (all ambiguous emotions)."
      ],
      [
        "Score: 0.07",
        "Finding something impressive or worthy of respect."
      ],
      [
        "Score: 0.03",
        "One of the main aspects distinguishing our dataset is its emotion taxonomy. The vast majority of existing datasets contain annotations for minor variations of the 6 basic emotion categories (joy, anger, fear, sadness, disgust, and surprise) proposed by Ekman (1992a) and/or along affective dimensions (valence and arousal) that underpin the circumplex model of affect (Russell, 2003; Buechel and Hahn, 2017) ."
      ]
    ]
  },
  {
    "question": "What is the attribute disapproval about?",
    "answers": [
      [
        "Score: 0.12",
        "The emotion categories are: admiration, amusement, anger, annoyance, approval, caring, confusion, curiosity, desire, disappointment, disapproval, disgust, embarrassment, excitement, fear, gratitude, grief, joy, love, nervousness, optimism, pride, realization, relief, remorse, sadness, surprise. This directory includes the data and code for data analysis scripts. We also include code for our baseline model, which involves fine-tuning a pre-trained BERT-base model. For more details on the design and content of the dataset, please see our paper. Refer to our GoEmotions Model Card for recommended uses of models built with this data, as well as considerations and limitations relating to the GoEmotions data. Requirements See requirements.txt Setup Download the pre-trained BERT model from here and unzip them inside the bert directory. In the paper, we use the cased base model. Data Our raw dataset can be retrieved by running:"
      ],
      [
        "Score: 0.03",
        "The rater interface. Reddit comments were presented with no additional metadata (such as the author or subreddit). To help raters navigate the large space of emotion in our taxonomy, they were presented a table containing all emotion categories aggregated by sentiment (by the mapping in Figure 2 ) and whether that emotion is generally expressed towards something (e.g. disapproval) or is more of an intrinsic feeling (e.g. joy). The instructions highlighted that this separation of categories was by no means clear-cut, but captured general tendencies, and we encouraged raters to ignore the categorization whenever they saw fit. Emotions with a straightforward mapping onto emojis were shown with an emoji in the UI, to further ease their interpretation."
      ],
      [
        "Score: 0.02",
        "Finding something impressive or worthy of respect."
      ],
      [
        "Score: 0.02",
        "Grouping emotions. We create a hierarchical grouping of our taxonomy, and evaluate the model performance on each level of the hierarchy. A sentiment level divides the labels into 4 categoriespositive, negative, ambiguous and Neutral -with the Neutral category intact, and the rest of the mapping as shown in Figure 2 . The Ekman level further divides the taxonomy using the Neutral label and the following 6 groups: anger (maps to: anger, annoyance, disapproval), disgust (maps to: disgust), fear (maps to: fear, nervousness), joy (all positive emotions), sadness (maps to: sadness, disappointment, embarrassment, grief, remorse) and surprise (all ambiguous emotions)."
      ],
      [
        "Score: 0.02",
        "One of the main aspects distinguishing our dataset is its emotion taxonomy. The vast majority of existing datasets contain annotations for minor variations of the 6 basic emotion categories (joy, anger, fear, sadness, disgust, and surprise) proposed by Ekman (1992a) and/or along affective dimensions (valence and arousal) that underpin the circumplex model of affect (Russell, 2003; Buechel and Hahn, 2017) ."
      ]
    ]
  },
  {
    "question": "A description of the attribute disapproval?",
    "answers": [
      [
        "Score: 0.13",
        "While reviewing the results from the pilot rounds, we identified and removed emotions that were scarcely selected by annotators and/or had low interrater agreement due to being very similar to other emotions or being too difficult to detect from text. These emotions were boredom, doubt, heartbroken, indifference and calmness. We also identified and added those emotions to our taxonomy that were frequently suggested by raters and/or seemed to be represented in the data upon manual inspection. These emotions were desire, disappointment, pride, realization, relief and remorse. In this process, we also refined the category names (e.g. replacing ecstasy with excitement), to ones that seemed interpretable to annotators. This is how we arrived at the final set of 27 emotions + Neutral. Our high interrater agreement in the final data can be partially explained by the fact that we took interpretability into consideration while constructing the taxonomy. The dataset is we are releasing was labeled in the third round over the final taxonomy."
      ],
      [
        "Score: 0.09",
        "The emotion categories are: admiration, amusement, anger, annoyance, approval, caring, confusion, curiosity, desire, disappointment, disapproval, disgust, embarrassment, excitement, fear, gratitude, grief, joy, love, nervousness, optimism, pride, realization, relief, remorse, sadness, surprise. This directory includes the data and code for data analysis scripts. We also include code for our baseline model, which involves fine-tuning a pre-trained BERT-base model. For more details on the design and content of the dataset, please see our paper. Refer to our GoEmotions Model Card for recommended uses of models built with this data, as well as considerations and limitations relating to the GoEmotions data. Requirements See requirements.txt Setup Download the pre-trained BERT model from here and unzip them inside the bert directory. In the paper, we use the cased base model. Data Our raw dataset can be retrieved by running:"
      ],
      [
        "Score: 0.05",
        "Grouping emotions. We create a hierarchical grouping of our taxonomy, and evaluate the model performance on each level of the hierarchy. A sentiment level divides the labels into 4 categoriespositive, negative, ambiguous and Neutral -with the Neutral category intact, and the rest of the mapping as shown in Figure 2 . The Ekman level further divides the taxonomy using the Neutral label and the following 6 groups: anger (maps to: anger, annoyance, disapproval), disgust (maps to: disgust), fear (maps to: fear, nervousness), joy (all positive emotions), sadness (maps to: sadness, disappointment, embarrassment, grief, remorse) and surprise (all ambiguous emotions)."
      ],
      [
        "Score: 0.03",
        "The rater interface. Reddit comments were presented with no additional metadata (such as the author or subreddit). To help raters navigate the large space of emotion in our taxonomy, they were presented a table containing all emotion categories aggregated by sentiment (by the mapping in Figure 2 ) and whether that emotion is generally expressed towards something (e.g. disapproval) or is more of an intrinsic feeling (e.g. joy). The instructions highlighted that this separation of categories was by no means clear-cut, but captured general tendencies, and we encouraged raters to ignore the categorization whenever they saw fit. Emotions with a straightforward mapping onto emojis were shown with an emoji in the UI, to further ease their interpretation."
      ],
      [
        "Score: 0.02",
        "One of the main aspects distinguishing our dataset is its emotion taxonomy. The vast majority of existing datasets contain annotations for minor variations of the 6 basic emotion categories (joy, anger, fear, sadness, disgust, and surprise) proposed by Ekman (1992a) and/or along affective dimensions (valence and arousal) that underpin the circumplex model of affect (Russell, 2003; Buechel and Hahn, 2017) ."
      ]
    ]
  },
  {
    "question": "What is the attribute disgust about?",
    "answers": [
      [
        "Score: 0.29",
        "The emotion categories are: admiration, amusement, anger, annoyance, approval, caring, confusion, curiosity, desire, disappointment, disapproval, disgust, embarrassment, excitement, fear, gratitude, grief, joy, love, nervousness, optimism, pride, realization, relief, remorse, sadness, surprise. This directory includes the data and code for data analysis scripts. We also include code for our baseline model, which involves fine-tuning a pre-trained BERT-base model. For more details on the design and content of the dataset, please see our paper. Refer to our GoEmotions Model Card for recommended uses of models built with this data, as well as considerations and limitations relating to the GoEmotions data. Requirements See requirements.txt Setup Download the pre-trained BERT model from here and unzip them inside the bert directory. In the paper, we use the cased base model. Data Our raw dataset can be retrieved by running:"
      ],
      [
        "Score: 0.08",
        "One of the main aspects distinguishing our dataset is its emotion taxonomy. The vast majority of existing datasets contain annotations for minor variations of the 6 basic emotion categories (joy, anger, fear, sadness, disgust, and surprise) proposed by Ekman (1992a) and/or along affective dimensions (valence and arousal) that underpin the circumplex model of affect (Russell, 2003; Buechel and Hahn, 2017) ."
      ],
      [
        "Score: 0.06",
        "Grouping emotions. We create a hierarchical grouping of our taxonomy, and evaluate the model performance on each level of the hierarchy. A sentiment level divides the labels into 4 categoriespositive, negative, ambiguous and Neutral -with the Neutral category intact, and the rest of the mapping as shown in Figure 2 . The Ekman level further divides the taxonomy using the Neutral label and the following 6 groups: anger (maps to: anger, annoyance, disapproval), disgust (maps to: disgust), fear (maps to: fear, nervousness), joy (all positive emotions), sadness (maps to: sadness, disappointment, embarrassment, grief, remorse) and surprise (all ambiguous emotions)."
      ],
      [
        "Score: 0.02",
        "To better understand agreement among raters and the latent structure of the emotion space, we apply Principal Preserved Component Analysis (PPCA) (Cowen et al., 2019b) to our data. PPCA extracts linear combinations of attributes (here, emotion judgments), that maximally covary across two sets of data that measure the same attributes (here, randomly split judgments for each example). Thus, PPCA allows us to uncover latent dimensions of J \u2208 R n\u00d7|R|\u00d7|E| \u2190 all ratings for the examples annotated by r 7:"
      ],
      [
        "Score: 0.02",
        "Finding something impressive or worthy of respect."
      ]
    ]
  },
  {
    "question": "A description of the attribute disgust?",
    "answers": [
      [
        "Score: 0.23",
        "The emotion categories are: admiration, amusement, anger, annoyance, approval, caring, confusion, curiosity, desire, disappointment, disapproval, disgust, embarrassment, excitement, fear, gratitude, grief, joy, love, nervousness, optimism, pride, realization, relief, remorse, sadness, surprise. This directory includes the data and code for data analysis scripts. We also include code for our baseline model, which involves fine-tuning a pre-trained BERT-base model. For more details on the design and content of the dataset, please see our paper. Refer to our GoEmotions Model Card for recommended uses of models built with this data, as well as considerations and limitations relating to the GoEmotions data. Requirements See requirements.txt Setup Download the pre-trained BERT model from here and unzip them inside the bert directory. In the paper, we use the cased base model. Data Our raw dataset can be retrieved by running:"
      ],
      [
        "Score: 0.18",
        "Grouping emotions. We create a hierarchical grouping of our taxonomy, and evaluate the model performance on each level of the hierarchy. A sentiment level divides the labels into 4 categoriespositive, negative, ambiguous and Neutral -with the Neutral category intact, and the rest of the mapping as shown in Figure 2 . The Ekman level further divides the taxonomy using the Neutral label and the following 6 groups: anger (maps to: anger, annoyance, disapproval), disgust (maps to: disgust), fear (maps to: fear, nervousness), joy (all positive emotions), sadness (maps to: sadness, disappointment, embarrassment, grief, remorse) and surprise (all ambiguous emotions)."
      ],
      [
        "Score: 0.11",
        "While reviewing the results from the pilot rounds, we identified and removed emotions that were scarcely selected by annotators and/or had low interrater agreement due to being very similar to other emotions or being too difficult to detect from text. These emotions were boredom, doubt, heartbroken, indifference and calmness. We also identified and added those emotions to our taxonomy that were frequently suggested by raters and/or seemed to be represented in the data upon manual inspection. These emotions were desire, disappointment, pride, realization, relief and remorse. In this process, we also refined the category names (e.g. replacing ecstasy with excitement), to ones that seemed interpretable to annotators. This is how we arrived at the final set of 27 emotions + Neutral. Our high interrater agreement in the final data can be partially explained by the fact that we took interpretability into consideration while constructing the taxonomy. The dataset is we are releasing was labeled in the third round over the final taxonomy."
      ],
      [
        "Score: 0.09",
        "One of the main aspects distinguishing our dataset is its emotion taxonomy. The vast majority of existing datasets contain annotations for minor variations of the 6 basic emotion categories (joy, anger, fear, sadness, disgust, and surprise) proposed by Ekman (1992a) and/or along affective dimensions (valence and arousal) that underpin the circumplex model of affect (Russell, 2003; Buechel and Hahn, 2017) ."
      ],
      [
        "Score: 0.01",
        "To better understand agreement among raters and the latent structure of the emotion space, we apply Principal Preserved Component Analysis (PPCA) (Cowen et al., 2019b) to our data. PPCA extracts linear combinations of attributes (here, emotion judgments), that maximally covary across two sets of data that measure the same attributes (here, randomly split judgments for each example). Thus, PPCA allows us to uncover latent dimensions of J \u2208 R n\u00d7|R|\u00d7|E| \u2190 all ratings for the examples annotated by r 7:"
      ]
    ]
  },
  {
    "question": "What is the attribute embarrassment about?",
    "answers": [
      [
        "Score: 0.81",
        "The emotion categories are: admiration, amusement, anger, annoyance, approval, caring, confusion, curiosity, desire, disappointment, disapproval, disgust, embarrassment, excitement, fear, gratitude, grief, joy, love, nervousness, optimism, pride, realization, relief, remorse, sadness, surprise. This directory includes the data and code for data analysis scripts. We also include code for our baseline model, which involves fine-tuning a pre-trained BERT-base model. For more details on the design and content of the dataset, please see our paper. Refer to our GoEmotions Model Card for recommended uses of models built with this data, as well as considerations and limitations relating to the GoEmotions data. Requirements See requirements.txt Setup Download the pre-trained BERT model from here and unzip them inside the bert directory. In the paper, we use the cased base model. Data Our raw dataset can be retrieved by running:"
      ],
      [
        "Score: 0.46",
        "Finding something impressive or worthy of respect."
      ],
      [
        "Score: 0.15",
        "One of the main aspects distinguishing our dataset is its emotion taxonomy. The vast majority of existing datasets contain annotations for minor variations of the 6 basic emotion categories (joy, anger, fear, sadness, disgust, and surprise) proposed by Ekman (1992a) and/or along affective dimensions (valence and arousal) that underpin the circumplex model of affect (Russell, 2003; Buechel and Hahn, 2017) ."
      ],
      [
        "Score: 0.14",
        "Figure 1 shows that gratitude, admiration and amusement have the highest and grief and nervousness have the lowest interrater correlation. Emotion frequency correlates with interrater agreement but the two are not equivalent. Infrequent emotions can have relatively high interrater correlation (e.g., fear), and frequent emotions can have have relatively low interrater correlation (e.g., annoyance)."
      ],
      [
        "Score: 0.10",
        "The rater interface. Reddit comments were presented with no additional metadata (such as the author or subreddit). To help raters navigate the large space of emotion in our taxonomy, they were presented a table containing all emotion categories aggregated by sentiment (by the mapping in Figure 2 ) and whether that emotion is generally expressed towards something (e.g. disapproval) or is more of an intrinsic feeling (e.g. joy). The instructions highlighted that this separation of categories was by no means clear-cut, but captured general tendencies, and we encouraged raters to ignore the categorization whenever they saw fit. Emotions with a straightforward mapping onto emojis were shown with an emoji in the UI, to further ease their interpretation."
      ]
    ]
  },
  {
    "question": "A description of the attribute embarrassment?",
    "answers": [
      [
        "Score: 0.71",
        "The emotion categories are: admiration, amusement, anger, annoyance, approval, caring, confusion, curiosity, desire, disappointment, disapproval, disgust, embarrassment, excitement, fear, gratitude, grief, joy, love, nervousness, optimism, pride, realization, relief, remorse, sadness, surprise. This directory includes the data and code for data analysis scripts. We also include code for our baseline model, which involves fine-tuning a pre-trained BERT-base model. For more details on the design and content of the dataset, please see our paper. Refer to our GoEmotions Model Card for recommended uses of models built with this data, as well as considerations and limitations relating to the GoEmotions data. Requirements See requirements.txt Setup Download the pre-trained BERT model from here and unzip them inside the bert directory. In the paper, we use the cased base model. Data Our raw dataset can be retrieved by running:"
      ],
      [
        "Score: 0.35",
        "Finding something impressive or worthy of respect."
      ],
      [
        "Score: 0.30",
        "While reviewing the results from the pilot rounds, we identified and removed emotions that were scarcely selected by annotators and/or had low interrater agreement due to being very similar to other emotions or being too difficult to detect from text. These emotions were boredom, doubt, heartbroken, indifference and calmness. We also identified and added those emotions to our taxonomy that were frequently suggested by raters and/or seemed to be represented in the data upon manual inspection. These emotions were desire, disappointment, pride, realization, relief and remorse. In this process, we also refined the category names (e.g. replacing ecstasy with excitement), to ones that seemed interpretable to annotators. This is how we arrived at the final set of 27 emotions + Neutral. Our high interrater agreement in the final data can be partially explained by the fact that we took interpretability into consideration while constructing the taxonomy. The dataset is we are releasing was labeled in the third round over the final taxonomy."
      ],
      [
        "Score: 0.15",
        "One of the main aspects distinguishing our dataset is its emotion taxonomy. The vast majority of existing datasets contain annotations for minor variations of the 6 basic emotion categories (joy, anger, fear, sadness, disgust, and surprise) proposed by Ekman (1992a) and/or along affective dimensions (valence and arousal) that underpin the circumplex model of affect (Russell, 2003; Buechel and Hahn, 2017) ."
      ],
      [
        "Score: 0.14",
        "Grouping emotions. We create a hierarchical grouping of our taxonomy, and evaluate the model performance on each level of the hierarchy. A sentiment level divides the labels into 4 categoriespositive, negative, ambiguous and Neutral -with the Neutral category intact, and the rest of the mapping as shown in Figure 2 . The Ekman level further divides the taxonomy using the Neutral label and the following 6 groups: anger (maps to: anger, annoyance, disapproval), disgust (maps to: disgust), fear (maps to: fear, nervousness), joy (all positive emotions), sadness (maps to: sadness, disappointment, embarrassment, grief, remorse) and surprise (all ambiguous emotions)."
      ]
    ]
  },
  {
    "question": "What is the attribute excitement about?",
    "answers": [
      [
        "Score: 0.33",
        "Finding something impressive or worthy of respect."
      ],
      [
        "Score: 0.15",
        "The emotion categories are: admiration, amusement, anger, annoyance, approval, caring, confusion, curiosity, desire, disappointment, disapproval, disgust, embarrassment, excitement, fear, gratitude, grief, joy, love, nervousness, optimism, pride, realization, relief, remorse, sadness, surprise. This directory includes the data and code for data analysis scripts. We also include code for our baseline model, which involves fine-tuning a pre-trained BERT-base model. For more details on the design and content of the dataset, please see our paper. Refer to our GoEmotions Model Card for recommended uses of models built with this data, as well as considerations and limitations relating to the GoEmotions data. Requirements See requirements.txt Setup Download the pre-trained BERT model from here and unzip them inside the bert directory. In the paper, we use the cased base model. Data Our raw dataset can be retrieved by running:"
      ],
      [
        "Score: 0.04",
        "Figure 1 shows that gratitude, admiration and amusement have the highest and grief and nervousness have the lowest interrater correlation. Emotion frequency correlates with interrater agreement but the two are not equivalent. Infrequent emotions can have relatively high interrater correlation (e.g., fear), and frequent emotions can have have relatively low interrater correlation (e.g., annoyance)."
      ],
      [
        "Score: 0.02",
        "While reviewing the results from the pilot rounds, we identified and removed emotions that were scarcely selected by annotators and/or had low interrater agreement due to being very similar to other emotions or being too difficult to detect from text. These emotions were boredom, doubt, heartbroken, indifference and calmness. We also identified and added those emotions to our taxonomy that were frequently suggested by raters and/or seemed to be represented in the data upon manual inspection. These emotions were desire, disappointment, pride, realization, relief and remorse. In this process, we also refined the category names (e.g. replacing ecstasy with excitement), to ones that seemed interpretable to annotators. This is how we arrived at the final set of 27 emotions + Neutral. Our high interrater agreement in the final data can be partially explained by the fact that we took interpretability into consideration while constructing the taxonomy. The dataset is we are releasing was labeled in the third round over the final taxonomy."
      ],
      [
        "Score: 0.02",
        "One of the main aspects distinguishing our dataset is its emotion taxonomy. The vast majority of existing datasets contain annotations for minor variations of the 6 basic emotion categories (joy, anger, fear, sadness, disgust, and surprise) proposed by Ekman (1992a) and/or along affective dimensions (valence and arousal) that underpin the circumplex model of affect (Russell, 2003; Buechel and Hahn, 2017) ."
      ]
    ]
  },
  {
    "question": "A description of the attribute excitement?",
    "answers": [
      [
        "Score: 0.21",
        "While reviewing the results from the pilot rounds, we identified and removed emotions that were scarcely selected by annotators and/or had low interrater agreement due to being very similar to other emotions or being too difficult to detect from text. These emotions were boredom, doubt, heartbroken, indifference and calmness. We also identified and added those emotions to our taxonomy that were frequently suggested by raters and/or seemed to be represented in the data upon manual inspection. These emotions were desire, disappointment, pride, realization, relief and remorse. In this process, we also refined the category names (e.g. replacing ecstasy with excitement), to ones that seemed interpretable to annotators. This is how we arrived at the final set of 27 emotions + Neutral. Our high interrater agreement in the final data can be partially explained by the fact that we took interpretability into consideration while constructing the taxonomy. The dataset is we are releasing was labeled in the third round over the final taxonomy."
      ],
      [
        "Score: 0.13",
        "Finding something impressive or worthy of respect."
      ],
      [
        "Score: 0.09",
        "The emotion categories are: admiration, amusement, anger, annoyance, approval, caring, confusion, curiosity, desire, disappointment, disapproval, disgust, embarrassment, excitement, fear, gratitude, grief, joy, love, nervousness, optimism, pride, realization, relief, remorse, sadness, surprise. This directory includes the data and code for data analysis scripts. We also include code for our baseline model, which involves fine-tuning a pre-trained BERT-base model. For more details on the design and content of the dataset, please see our paper. Refer to our GoEmotions Model Card for recommended uses of models built with this data, as well as considerations and limitations relating to the GoEmotions data. Requirements See requirements.txt Setup Download the pre-trained BERT model from here and unzip them inside the bert directory. In the paper, we use the cased base model. Data Our raw dataset can be retrieved by running:"
      ],
      [
        "Score: 0.02",
        "One of the main aspects distinguishing our dataset is its emotion taxonomy. The vast majority of existing datasets contain annotations for minor variations of the 6 basic emotion categories (joy, anger, fear, sadness, disgust, and surprise) proposed by Ekman (1992a) and/or along affective dimensions (valence and arousal) that underpin the circumplex model of affect (Russell, 2003; Buechel and Hahn, 2017) ."
      ],
      [
        "Score: 0.01",
        "We present GoEmotions, a large, manually annotated, carefully curated dataset for fine-grained emotion prediction. We provide a detailed data analysis, demonstrating the reliability of the annotations for the full taxonomy. We show the generalizability of the data across domains and taxonomies via transfer learning experiments. We build a strong baseline by fine-tuning a BERT model, however, the results suggest much room for future improvement. Future work can explore the cross-cultural robustness of emotion ratings, and extend the taxonomy to other languages and domains."
      ]
    ]
  },
  {
    "question": "What is the attribute fear about?",
    "answers": [
      [
        "Score: 0.04",
        "To better understand agreement among raters and the latent structure of the emotion space, we apply Principal Preserved Component Analysis (PPCA) (Cowen et al., 2019b) to our data. PPCA extracts linear combinations of attributes (here, emotion judgments), that maximally covary across two sets of data that measure the same attributes (here, randomly split judgments for each example). Thus, PPCA allows us to uncover latent dimensions of J \u2208 R n\u00d7|R|\u00d7|E| \u2190 all ratings for the examples annotated by r 7:"
      ],
      [
        "Score: 0.03",
        "One of the main aspects distinguishing our dataset is its emotion taxonomy. The vast majority of existing datasets contain annotations for minor variations of the 6 basic emotion categories (joy, anger, fear, sadness, disgust, and surprise) proposed by Ekman (1992a) and/or along affective dimensions (valence and arousal) that underpin the circumplex model of affect (Russell, 2003; Buechel and Hahn, 2017) ."
      ],
      [
        "Score: 0.02",
        "We conduct transfer learning experiments on existing emotion benchmarks, in order to show our data generalizes across domains and taxonomies. The goal is to demonstrate that given little labeled data in a target domain, one can utilize GoEmotions as baseline emotion understanding data. ity and taxonomy. In the interest of space, we only discuss three of these datasets here, chosen based on their diversity of domains. In our experiments, we observe similar trends for the additional benchmarks, and all are included in the Appendix H. The International Survey on Emotion Antecedents and Reactions (ISEAR) (Scherer and Wallbott, 1994 ) is a collection of personal reports on emotional events, written by 3000 people from different cultural backgrounds. The dataset contains 8k sentences, each labeled with a single emotion. The categories are anger, disgust, fear, guilt, joy, sadness and shame."
      ],
      [
        "Score: 0.02",
        "The emotion categories are: admiration, amusement, anger, annoyance, approval, caring, confusion, curiosity, desire, disappointment, disapproval, disgust, embarrassment, excitement, fear, gratitude, grief, joy, love, nervousness, optimism, pride, realization, relief, remorse, sadness, surprise. This directory includes the data and code for data analysis scripts. We also include code for our baseline model, which involves fine-tuning a pre-trained BERT-base model. For more details on the design and content of the dataset, please see our paper. Refer to our GoEmotions Model Card for recommended uses of models built with this data, as well as considerations and limitations relating to the GoEmotions data. Requirements See requirements.txt Setup Download the pre-trained BERT model from here and unzip them inside the bert directory. In the paper, we use the cased base model. Data Our raw dataset can be retrieved by running:"
      ],
      [
        "Score: 0.01",
        "Figure 1 shows that gratitude, admiration and amusement have the highest and grief and nervousness have the lowest interrater correlation. Emotion frequency correlates with interrater agreement but the two are not equivalent. Infrequent emotions can have relatively high interrater correlation (e.g., fear), and frequent emotions can have have relatively low interrater correlation (e.g., annoyance)."
      ]
    ]
  },
  {
    "question": "A description of the attribute fear?",
    "answers": [
      [
        "Score: 0.03",
        "To better understand agreement among raters and the latent structure of the emotion space, we apply Principal Preserved Component Analysis (PPCA) (Cowen et al., 2019b) to our data. PPCA extracts linear combinations of attributes (here, emotion judgments), that maximally covary across two sets of data that measure the same attributes (here, randomly split judgments for each example). Thus, PPCA allows us to uncover latent dimensions of J \u2208 R n\u00d7|R|\u00d7|E| \u2190 all ratings for the examples annotated by r 7:"
      ],
      [
        "Score: 0.03",
        "One of the main aspects distinguishing our dataset is its emotion taxonomy. The vast majority of existing datasets contain annotations for minor variations of the 6 basic emotion categories (joy, anger, fear, sadness, disgust, and surprise) proposed by Ekman (1992a) and/or along affective dimensions (valence and arousal) that underpin the circumplex model of affect (Russell, 2003; Buechel and Hahn, 2017) ."
      ],
      [
        "Score: 0.03",
        "While reviewing the results from the pilot rounds, we identified and removed emotions that were scarcely selected by annotators and/or had low interrater agreement due to being very similar to other emotions or being too difficult to detect from text. These emotions were boredom, doubt, heartbroken, indifference and calmness. We also identified and added those emotions to our taxonomy that were frequently suggested by raters and/or seemed to be represented in the data upon manual inspection. These emotions were desire, disappointment, pride, realization, relief and remorse. In this process, we also refined the category names (e.g. replacing ecstasy with excitement), to ones that seemed interpretable to annotators. This is how we arrived at the final set of 27 emotions + Neutral. Our high interrater agreement in the final data can be partially explained by the fact that we took interpretability into consideration while constructing the taxonomy. The dataset is we are releasing was labeled in the third round over the final taxonomy."
      ],
      [
        "Score: 0.02",
        "We present GoEmotions, a large, manually annotated, carefully curated dataset for fine-grained emotion prediction. We provide a detailed data analysis, demonstrating the reliability of the annotations for the full taxonomy. We show the generalizability of the data across domains and taxonomies via transfer learning experiments. We build a strong baseline by fine-tuning a BERT model, however, the results suggest much room for future improvement. Future work can explore the cross-cultural robustness of emotion ratings, and extend the taxonomy to other languages and domains."
      ],
      [
        "Score: 0.02",
        "We conduct transfer learning experiments on existing emotion benchmarks, in order to show our data generalizes across domains and taxonomies. The goal is to demonstrate that given little labeled data in a target domain, one can utilize GoEmotions as baseline emotion understanding data. ity and taxonomy. In the interest of space, we only discuss three of these datasets here, chosen based on their diversity of domains. In our experiments, we observe similar trends for the additional benchmarks, and all are included in the Appendix H. The International Survey on Emotion Antecedents and Reactions (ISEAR) (Scherer and Wallbott, 1994 ) is a collection of personal reports on emotional events, written by 3000 people from different cultural backgrounds. The dataset contains 8k sentences, each labeled with a single emotion. The categories are anger, disgust, fear, guilt, joy, sadness and shame."
      ]
    ]
  },
  {
    "question": "What is the attribute gratitude about?",
    "answers": [
      [
        "Score: 0.94",
        "Finding something impressive or worthy of respect."
      ],
      [
        "Score: 0.23",
        "The emotion categories are: admiration, amusement, anger, annoyance, approval, caring, confusion, curiosity, desire, disappointment, disapproval, disgust, embarrassment, excitement, fear, gratitude, grief, joy, love, nervousness, optimism, pride, realization, relief, remorse, sadness, surprise. This directory includes the data and code for data analysis scripts. We also include code for our baseline model, which involves fine-tuning a pre-trained BERT-base model. For more details on the design and content of the dataset, please see our paper. Refer to our GoEmotions Model Card for recommended uses of models built with this data, as well as considerations and limitations relating to the GoEmotions data. Requirements See requirements.txt Setup Download the pre-trained BERT model from here and unzip them inside the bert directory. In the paper, we use the cased base model. Data Our raw dataset can be retrieved by running:"
      ],
      [
        "Score: 0.23",
        "Figure 1 shows that gratitude, admiration and amusement have the highest and grief and nervousness have the lowest interrater correlation. Emotion frequency correlates with interrater agreement but the two are not equivalent. Infrequent emotions can have relatively high interrater correlation (e.g., fear), and frequent emotions can have have relatively low interrater correlation (e.g., annoyance)."
      ],
      [
        "Score: 0.01",
        "Table 4 summarizes the performance of our best model, BERT, on the test set, which achieves an average F1-score of . 46 (std=.19 ). The model obtains the best performance on emotions with overt lexical markers, such as gratitude (.86), amusement (.8) and love (.78). The model obtains the lowest F1-score on grief (0), relief (.15) and realization (.21), which are the lowest frequency emotions. We find that less frequent emotions tend to be confused by the model with more frequent emotions related in sentiment and intensity (e.g., grief with sadness, pride with admiration, nervousness with fear)see Appendix G for a more detailed analysis."
      ],
      [
        "Score: 0.01",
        "While reviewing the results from the pilot rounds, we identified and removed emotions that were scarcely selected by annotators and/or had low interrater agreement due to being very similar to other emotions or being too difficult to detect from text. These emotions were boredom, doubt, heartbroken, indifference and calmness. We also identified and added those emotions to our taxonomy that were frequently suggested by raters and/or seemed to be represented in the data upon manual inspection. These emotions were desire, disappointment, pride, realization, relief and remorse. In this process, we also refined the category names (e.g. replacing ecstasy with excitement), to ones that seemed interpretable to annotators. This is how we arrived at the final set of 27 emotions + Neutral. Our high interrater agreement in the final data can be partially explained by the fact that we took interpretability into consideration while constructing the taxonomy. The dataset is we are releasing was labeled in the third round over the final taxonomy."
      ]
    ]
  },
  {
    "question": "A description of the attribute gratitude?",
    "answers": [
      [
        "Score: 0.92",
        "Finding something impressive or worthy of respect."
      ],
      [
        "Score: 0.16",
        "The emotion categories are: admiration, amusement, anger, annoyance, approval, caring, confusion, curiosity, desire, disappointment, disapproval, disgust, embarrassment, excitement, fear, gratitude, grief, joy, love, nervousness, optimism, pride, realization, relief, remorse, sadness, surprise. This directory includes the data and code for data analysis scripts. We also include code for our baseline model, which involves fine-tuning a pre-trained BERT-base model. For more details on the design and content of the dataset, please see our paper. Refer to our GoEmotions Model Card for recommended uses of models built with this data, as well as considerations and limitations relating to the GoEmotions data. Requirements See requirements.txt Setup Download the pre-trained BERT model from here and unzip them inside the bert directory. In the paper, we use the cased base model. Data Our raw dataset can be retrieved by running:"
      ],
      [
        "Score: 0.05",
        "Figure 1 shows that gratitude, admiration and amusement have the highest and grief and nervousness have the lowest interrater correlation. Emotion frequency correlates with interrater agreement but the two are not equivalent. Infrequent emotions can have relatively high interrater correlation (e.g., fear), and frequent emotions can have have relatively low interrater correlation (e.g., annoyance)."
      ],
      [
        "Score: 0.04",
        "While reviewing the results from the pilot rounds, we identified and removed emotions that were scarcely selected by annotators and/or had low interrater agreement due to being very similar to other emotions or being too difficult to detect from text. These emotions were boredom, doubt, heartbroken, indifference and calmness. We also identified and added those emotions to our taxonomy that were frequently suggested by raters and/or seemed to be represented in the data upon manual inspection. These emotions were desire, disappointment, pride, realization, relief and remorse. In this process, we also refined the category names (e.g. replacing ecstasy with excitement), to ones that seemed interpretable to annotators. This is how we arrived at the final set of 27 emotions + Neutral. Our high interrater agreement in the final data can be partially explained by the fact that we took interpretability into consideration while constructing the taxonomy. The dataset is we are releasing was labeled in the third round over the final taxonomy."
      ],
      [
        "Score: 0.03",
        "Table 4 summarizes the performance of our best model, BERT, on the test set, which achieves an average F1-score of . 46 (std=.19 ). The model obtains the best performance on emotions with overt lexical markers, such as gratitude (.86), amusement (.8) and love (.78). The model obtains the lowest F1-score on grief (0), relief (.15) and realization (.21), which are the lowest frequency emotions. We find that less frequent emotions tend to be confused by the model with more frequent emotions related in sentiment and intensity (e.g., grief with sadness, pride with admiration, nervousness with fear)see Appendix G for a more detailed analysis."
      ]
    ]
  },
  {
    "question": "What is the attribute grief about?",
    "answers": [
      [
        "Score: 0.05",
        "The emotion categories are: admiration, amusement, anger, annoyance, approval, caring, confusion, curiosity, desire, disappointment, disapproval, disgust, embarrassment, excitement, fear, gratitude, grief, joy, love, nervousness, optimism, pride, realization, relief, remorse, sadness, surprise. This directory includes the data and code for data analysis scripts. We also include code for our baseline model, which involves fine-tuning a pre-trained BERT-base model. For more details on the design and content of the dataset, please see our paper. Refer to our GoEmotions Model Card for recommended uses of models built with this data, as well as considerations and limitations relating to the GoEmotions data. Requirements See requirements.txt Setup Download the pre-trained BERT model from here and unzip them inside the bert directory. In the paper, we use the cased base model. Data Our raw dataset can be retrieved by running:"
      ],
      [
        "Score: 0.02",
        "Figure 1 shows that gratitude, admiration and amusement have the highest and grief and nervousness have the lowest interrater correlation. Emotion frequency correlates with interrater agreement but the two are not equivalent. Infrequent emotions can have relatively high interrater correlation (e.g., fear), and frequent emotions can have have relatively low interrater correlation (e.g., annoyance)."
      ],
      [
        "Score: 0.01",
        "Table 4 summarizes the performance of our best model, BERT, on the test set, which achieves an average F1-score of . 46 (std=.19 ). The model obtains the best performance on emotions with overt lexical markers, such as gratitude (.86), amusement (.8) and love (.78). The model obtains the lowest F1-score on grief (0), relief (.15) and realization (.21), which are the lowest frequency emotions. We find that less frequent emotions tend to be confused by the model with more frequent emotions related in sentiment and intensity (e.g., grief with sadness, pride with admiration, nervousness with fear)see Appendix G for a more detailed analysis."
      ],
      [
        "Score: 0.01",
        "Grouping emotions. We create a hierarchical grouping of our taxonomy, and evaluate the model performance on each level of the hierarchy. A sentiment level divides the labels into 4 categoriespositive, negative, ambiguous and Neutral -with the Neutral category intact, and the rest of the mapping as shown in Figure 2 . The Ekman level further divides the taxonomy using the Neutral label and the following 6 groups: anger (maps to: anger, annoyance, disapproval), disgust (maps to: disgust), fear (maps to: fear, nervousness), joy (all positive emotions), sadness (maps to: sadness, disappointment, embarrassment, grief, remorse) and surprise (all ambiguous emotions)."
      ],
      [
        "Score: 0.01",
        "Figure 1 shows the distribution of emotion labels. We can see a large disparity in terms of emotion frequencies (e.g. admiration is 30 times more frequent than grief ), despite our emotion and sentiment balancing steps taken during data selection. This is expected given the disparate frequencies of emotions in natural human expression."
      ]
    ]
  },
  {
    "question": "A description of the attribute grief?",
    "answers": [
      [
        "Score: 0.06",
        "The emotion categories are: admiration, amusement, anger, annoyance, approval, caring, confusion, curiosity, desire, disappointment, disapproval, disgust, embarrassment, excitement, fear, gratitude, grief, joy, love, nervousness, optimism, pride, realization, relief, remorse, sadness, surprise. This directory includes the data and code for data analysis scripts. We also include code for our baseline model, which involves fine-tuning a pre-trained BERT-base model. For more details on the design and content of the dataset, please see our paper. Refer to our GoEmotions Model Card for recommended uses of models built with this data, as well as considerations and limitations relating to the GoEmotions data. Requirements See requirements.txt Setup Download the pre-trained BERT model from here and unzip them inside the bert directory. In the paper, we use the cased base model. Data Our raw dataset can be retrieved by running:"
      ],
      [
        "Score: 0.03",
        "While reviewing the results from the pilot rounds, we identified and removed emotions that were scarcely selected by annotators and/or had low interrater agreement due to being very similar to other emotions or being too difficult to detect from text. These emotions were boredom, doubt, heartbroken, indifference and calmness. We also identified and added those emotions to our taxonomy that were frequently suggested by raters and/or seemed to be represented in the data upon manual inspection. These emotions were desire, disappointment, pride, realization, relief and remorse. In this process, we also refined the category names (e.g. replacing ecstasy with excitement), to ones that seemed interpretable to annotators. This is how we arrived at the final set of 27 emotions + Neutral. Our high interrater agreement in the final data can be partially explained by the fact that we took interpretability into consideration while constructing the taxonomy. The dataset is we are releasing was labeled in the third round over the final taxonomy."
      ],
      [
        "Score: 0.02",
        "Table 4 summarizes the performance of our best model, BERT, on the test set, which achieves an average F1-score of . 46 (std=.19 ). The model obtains the best performance on emotions with overt lexical markers, such as gratitude (.86), amusement (.8) and love (.78). The model obtains the lowest F1-score on grief (0), relief (.15) and realization (.21), which are the lowest frequency emotions. We find that less frequent emotions tend to be confused by the model with more frequent emotions related in sentiment and intensity (e.g., grief with sadness, pride with admiration, nervousness with fear)see Appendix G for a more detailed analysis."
      ],
      [
        "Score: 0.01",
        "Grouping emotions. We create a hierarchical grouping of our taxonomy, and evaluate the model performance on each level of the hierarchy. A sentiment level divides the labels into 4 categoriespositive, negative, ambiguous and Neutral -with the Neutral category intact, and the rest of the mapping as shown in Figure 2 . The Ekman level further divides the taxonomy using the Neutral label and the following 6 groups: anger (maps to: anger, annoyance, disapproval), disgust (maps to: disgust), fear (maps to: fear, nervousness), joy (all positive emotions), sadness (maps to: sadness, disappointment, embarrassment, grief, remorse) and surprise (all ambiguous emotions)."
      ],
      [
        "Score: 0.01",
        "Figure 1 shows that gratitude, admiration and amusement have the highest and grief and nervousness have the lowest interrater correlation. Emotion frequency correlates with interrater agreement but the two are not equivalent. Infrequent emotions can have relatively high interrater correlation (e.g., fear), and frequent emotions can have have relatively low interrater correlation (e.g., annoyance)."
      ]
    ]
  },
  {
    "question": "What is the attribute joy about?",
    "answers": [
      [
        "Score: 0.04",
        "One of the main aspects distinguishing our dataset is its emotion taxonomy. The vast majority of existing datasets contain annotations for minor variations of the 6 basic emotion categories (joy, anger, fear, sadness, disgust, and surprise) proposed by Ekman (1992a) and/or along affective dimensions (valence and arousal) that underpin the circumplex model of affect (Russell, 2003; Buechel and Hahn, 2017) ."
      ],
      [
        "Score: 0.03",
        "The emotion categories are: admiration, amusement, anger, annoyance, approval, caring, confusion, curiosity, desire, disappointment, disapproval, disgust, embarrassment, excitement, fear, gratitude, grief, joy, love, nervousness, optimism, pride, realization, relief, remorse, sadness, surprise. This directory includes the data and code for data analysis scripts. We also include code for our baseline model, which involves fine-tuning a pre-trained BERT-base model. For more details on the design and content of the dataset, please see our paper. Refer to our GoEmotions Model Card for recommended uses of models built with this data, as well as considerations and limitations relating to the GoEmotions data. Requirements See requirements.txt Setup Download the pre-trained BERT model from here and unzip them inside the bert directory. In the paper, we use the cased base model. Data Our raw dataset can be retrieved by running:"
      ],
      [
        "Score: 0.02",
        "We conduct transfer learning experiments on existing emotion benchmarks, in order to show our data generalizes across domains and taxonomies. The goal is to demonstrate that given little labeled data in a target domain, one can utilize GoEmotions as baseline emotion understanding data. ity and taxonomy. In the interest of space, we only discuss three of these datasets here, chosen based on their diversity of domains. In our experiments, we observe similar trends for the additional benchmarks, and all are included in the Appendix H. The International Survey on Emotion Antecedents and Reactions (ISEAR) (Scherer and Wallbott, 1994 ) is a collection of personal reports on emotional events, written by 3000 people from different cultural backgrounds. The dataset contains 8k sentences, each labeled with a single emotion. The categories are anger, disgust, fear, guilt, joy, sadness and shame."
      ],
      [
        "Score: 0.01",
        "The rater interface. Reddit comments were presented with no additional metadata (such as the author or subreddit). To help raters navigate the large space of emotion in our taxonomy, they were presented a table containing all emotion categories aggregated by sentiment (by the mapping in Figure 2 ) and whether that emotion is generally expressed towards something (e.g. disapproval) or is more of an intrinsic feeling (e.g. joy). The instructions highlighted that this separation of categories was by no means clear-cut, but captured general tendencies, and we encouraged raters to ignore the categorization whenever they saw fit. Emotions with a straightforward mapping onto emojis were shown with an emoji in the UI, to further ease their interpretation."
      ],
      [
        "Score: 0.00",
        "Figure 1 shows that gratitude, admiration and amusement have the highest and grief and nervousness have the lowest interrater correlation. Emotion frequency correlates with interrater agreement but the two are not equivalent. Infrequent emotions can have relatively high interrater correlation (e.g., fear), and frequent emotions can have have relatively low interrater correlation (e.g., annoyance)."
      ]
    ]
  },
  {
    "question": "A description of the attribute joy?",
    "answers": [
      [
        "Score: 0.05",
        "One of the main aspects distinguishing our dataset is its emotion taxonomy. The vast majority of existing datasets contain annotations for minor variations of the 6 basic emotion categories (joy, anger, fear, sadness, disgust, and surprise) proposed by Ekman (1992a) and/or along affective dimensions (valence and arousal) that underpin the circumplex model of affect (Russell, 2003; Buechel and Hahn, 2017) ."
      ],
      [
        "Score: 0.02",
        "The emotion categories are: admiration, amusement, anger, annoyance, approval, caring, confusion, curiosity, desire, disappointment, disapproval, disgust, embarrassment, excitement, fear, gratitude, grief, joy, love, nervousness, optimism, pride, realization, relief, remorse, sadness, surprise. This directory includes the data and code for data analysis scripts. We also include code for our baseline model, which involves fine-tuning a pre-trained BERT-base model. For more details on the design and content of the dataset, please see our paper. Refer to our GoEmotions Model Card for recommended uses of models built with this data, as well as considerations and limitations relating to the GoEmotions data. Requirements See requirements.txt Setup Download the pre-trained BERT model from here and unzip them inside the bert directory. In the paper, we use the cased base model. Data Our raw dataset can be retrieved by running:"
      ],
      [
        "Score: 0.01",
        "We conduct transfer learning experiments on existing emotion benchmarks, in order to show our data generalizes across domains and taxonomies. The goal is to demonstrate that given little labeled data in a target domain, one can utilize GoEmotions as baseline emotion understanding data. ity and taxonomy. In the interest of space, we only discuss three of these datasets here, chosen based on their diversity of domains. In our experiments, we observe similar trends for the additional benchmarks, and all are included in the Appendix H. The International Survey on Emotion Antecedents and Reactions (ISEAR) (Scherer and Wallbott, 1994 ) is a collection of personal reports on emotional events, written by 3000 people from different cultural backgrounds. The dataset contains 8k sentences, each labeled with a single emotion. The categories are anger, disgust, fear, guilt, joy, sadness and shame."
      ],
      [
        "Score: 0.01",
        "The rater interface. Reddit comments were presented with no additional metadata (such as the author or subreddit). To help raters navigate the large space of emotion in our taxonomy, they were presented a table containing all emotion categories aggregated by sentiment (by the mapping in Figure 2 ) and whether that emotion is generally expressed towards something (e.g. disapproval) or is more of an intrinsic feeling (e.g. joy). The instructions highlighted that this separation of categories was by no means clear-cut, but captured general tendencies, and we encouraged raters to ignore the categorization whenever they saw fit. Emotions with a straightforward mapping onto emojis were shown with an emoji in the UI, to further ease their interpretation."
      ],
      [
        "Score: 0.01",
        "While reviewing the results from the pilot rounds, we identified and removed emotions that were scarcely selected by annotators and/or had low interrater agreement due to being very similar to other emotions or being too difficult to detect from text. These emotions were boredom, doubt, heartbroken, indifference and calmness. We also identified and added those emotions to our taxonomy that were frequently suggested by raters and/or seemed to be represented in the data upon manual inspection. These emotions were desire, disappointment, pride, realization, relief and remorse. In this process, we also refined the category names (e.g. replacing ecstasy with excitement), to ones that seemed interpretable to annotators. This is how we arrived at the final set of 27 emotions + Neutral. Our high interrater agreement in the final data can be partially explained by the fact that we took interpretability into consideration while constructing the taxonomy. The dataset is we are releasing was labeled in the third round over the final taxonomy."
      ]
    ]
  },
  {
    "question": "What is the attribute love about?",
    "answers": [
      [
        "Score: 0.00",
        "The emotion categories are: admiration, amusement, anger, annoyance, approval, caring, confusion, curiosity, desire, disappointment, disapproval, disgust, embarrassment, excitement, fear, gratitude, grief, joy, love, nervousness, optimism, pride, realization, relief, remorse, sadness, surprise. This directory includes the data and code for data analysis scripts. We also include code for our baseline model, which involves fine-tuning a pre-trained BERT-base model. For more details on the design and content of the dataset, please see our paper. Refer to our GoEmotions Model Card for recommended uses of models built with this data, as well as considerations and limitations relating to the GoEmotions data. Requirements See requirements.txt Setup Download the pre-trained BERT model from here and unzip them inside the bert directory. In the paper, we use the cased base model. Data Our raw dataset can be retrieved by running:"
      ],
      [
        "Score: 0.00",
        "Table 4 summarizes the performance of our best model, BERT, on the test set, which achieves an average F1-score of . 46 (std=.19 ). The model obtains the best performance on emotions with overt lexical markers, such as gratitude (.86), amusement (.8) and love (.78). The model obtains the lowest F1-score on grief (0), relief (.15) and realization (.21), which are the lowest frequency emotions. We find that less frequent emotions tend to be confused by the model with more frequent emotions related in sentiment and intensity (e.g., grief with sadness, pride with admiration, nervousness with fear)see Appendix G for a more detailed analysis."
      ],
      [
        "Score: 0.00",
        "Even though we filter our data for the baseline experiments, we see particular value in the 4K examples that lack agreement. This subset of the data likely contains edge/difficult examples for the emotion domain (e.g., emotion-ambiguous text), and present challenges for further exploration. That is 66 ) agree ( 24 ) you ( 12 ) fuck ( 24 ) annoying ( 14 ) disappointing ( 11 ) not ( 16 ) confused ( 18 ) awesome ( 32 ) haha ( 32 ) not ( 13 ) worry ( 11 ) hate ( 18 ) stupid ( 13 ) disappointed (10) don't ( 14 ) why ( 11 ) amazing ( 30 ) funny ( 27 ) don't ( 12 ) careful ( 9 ) fucking ( 18 ) fucking ( 12 ) bad ( 9 ) disagree ( 9 ) sure ( 10 ) good ( 28 ) lmao ( 21 ) yes ( 12 ) stay ( 9 ) angry ( 11 ) shit ( 10 ) disappointment ( 7 ) nope ( 8 ) what ( 10 ) beautiful ( 23 ) hilarious ( 18 ) agreed ( 11 ) your ( 8 ) dare ( 10 ) dumb ( 9 ) unfortunately ( 7 ) doesn't ( 7 ) understand (8) desire excitement gratitude joy disgust embarrassment fear grief curiosity wish ( 29 ) excited ( 21 ) thanks ( 75 ) happy ( 32 ) disgusting ( 22 ) embarrassing ( 12 ) scared ( 16 ) died ( 6 ) curious ( 22 ) want ( 8 ) happy ( 8 ) thank ( 69 ) glad ( 27 ) awful ( 14 ) shame ( 11 ) afraid ( 16 ) rip ( 4 ) what ( 18 ) wanted ( 6 ) cake ( 8 ) for ( 24 ) enjoy ( 20 ) worst ( 13 ) awkward ( 10 ) scary ( 15 ) why ( 13 ) could ( 6 ) wow ( 8 ) you ( 18 ) enjoyed ( 12 ) worse ( 12 ) embarrassment ( 8 ) terrible ( 12 ) how ( 11 ) ambitious ( 4 ) interesting ( 7 ) sharing ( 17 ) fun ( 12 ) weird ( 9 ) embarrassed ( 7 ) terrifying ( 11 ) did (10) love optimism pride relief nervousness remorse sadness realization surprise love ( 76 ) hope ( 45 ) proud ( 14 ) glad ( 5 ) nervous ( 8 ) sorry ( 39 ) sad ( 31 ) realize ( 14 ) wow ( 23 ) loved ( 21 ) hopefully ( 19 ) pride ( 4 ) relieved ( 4 ) worried ( 8 ) regret ( 9 ) sadly ( 16 ) realized ( 12 ) surprised ( 21 ) favorite ( 13 ) luck (18) accomplishment relieving (4) anxiety ( 6 ) apologies ( 7 ) sorry ( 15 ) realised ( 7 ) wonder ( 15 ) loves ( 12 ) hoping ( 16 ) (4) relief ( 4 ) anxious ( 4 ) apologize ( 6 ) painful ( 10 ) realization ( 6 ) shocked ( 12 ) like ( 9 ) will ( 8 ) worrying ( 4 ) guilt ( 5 ) crying ( 9 ) thought ( 6 ) omg ( 11 )"
      ],
      [
        "Score: 0.00",
        "Finding something impressive or worthy of respect."
      ],
      [
        "Score: 0.00",
        "One of the main aspects distinguishing our dataset is its emotion taxonomy. The vast majority of existing datasets contain annotations for minor variations of the 6 basic emotion categories (joy, anger, fear, sadness, disgust, and surprise) proposed by Ekman (1992a) and/or along affective dimensions (valence and arousal) that underpin the circumplex model of affect (Russell, 2003; Buechel and Hahn, 2017) ."
      ]
    ]
  },
  {
    "question": "A description of the attribute love?",
    "answers": [
      [
        "Score: 0.01",
        "Table 4 summarizes the performance of our best model, BERT, on the test set, which achieves an average F1-score of . 46 (std=.19 ). The model obtains the best performance on emotions with overt lexical markers, such as gratitude (.86), amusement (.8) and love (.78). The model obtains the lowest F1-score on grief (0), relief (.15) and realization (.21), which are the lowest frequency emotions. We find that less frequent emotions tend to be confused by the model with more frequent emotions related in sentiment and intensity (e.g., grief with sadness, pride with admiration, nervousness with fear)see Appendix G for a more detailed analysis."
      ],
      [
        "Score: 0.00",
        "While reviewing the results from the pilot rounds, we identified and removed emotions that were scarcely selected by annotators and/or had low interrater agreement due to being very similar to other emotions or being too difficult to detect from text. These emotions were boredom, doubt, heartbroken, indifference and calmness. We also identified and added those emotions to our taxonomy that were frequently suggested by raters and/or seemed to be represented in the data upon manual inspection. These emotions were desire, disappointment, pride, realization, relief and remorse. In this process, we also refined the category names (e.g. replacing ecstasy with excitement), to ones that seemed interpretable to annotators. This is how we arrived at the final set of 27 emotions + Neutral. Our high interrater agreement in the final data can be partially explained by the fact that we took interpretability into consideration while constructing the taxonomy. The dataset is we are releasing was labeled in the third round over the final taxonomy."
      ],
      [
        "Score: 0.00",
        "The emotion categories are: admiration, amusement, anger, annoyance, approval, caring, confusion, curiosity, desire, disappointment, disapproval, disgust, embarrassment, excitement, fear, gratitude, grief, joy, love, nervousness, optimism, pride, realization, relief, remorse, sadness, surprise. This directory includes the data and code for data analysis scripts. We also include code for our baseline model, which involves fine-tuning a pre-trained BERT-base model. For more details on the design and content of the dataset, please see our paper. Refer to our GoEmotions Model Card for recommended uses of models built with this data, as well as considerations and limitations relating to the GoEmotions data. Requirements See requirements.txt Setup Download the pre-trained BERT model from here and unzip them inside the bert directory. In the paper, we use the cased base model. Data Our raw dataset can be retrieved by running:"
      ],
      [
        "Score: 0.00",
        "description: ### Context GoEmotions is a corpus of 58k carefully curated comments extracted from Reddit, with human annotations to 27 emotion categories or Neutral.  - Number of examples: 58,009. - Number of labels: 27 + Neutral. - Maximum sequence length in training and evaluation datasets: 30.   ### Content On top of the raw data, we also include a version filtered based on reter-agreement, which contains a train/test/validation split:  - Size of training dataset: 43,410. - Size of test dataset: 5,427. - Size of validation dataset: 5,426. The emotion categories are: admiration, amusement, anger, annoyance, approval, caring, confusion, curiosity, desire, disappointment, disapproval, disgust, embarrassment, excitement, fear, gratitude, grief, joy, love, nervousness, optimism, pride, realization, relief, remorse, sadness, surprise.  `analyze_data.py` and `extract_words.py` have already been run and the output files are stored in `plots` and `tables` respectively. What's inside is more than just rows and columns. Make it easy for others to get started by describing how you acquired the data and what time period it represents, too. Original repository from where the data is taken: [https://github.com/google-research/google-research/tree/master/goemotions](https://github.com/google-research/google-research/tree/master/goemotions) However, some of the scripts were giving errors which have been rectified and updated in this repository: [https://github.com/DebarshiChanda/google-research/tree/master/goemotions](https://github.com/DebarshiChanda/google-research/tree/master/goemotions)  ### Acknowledgements The entire Credit for creating this dataset goes to Google Research Team.   ### Inspiration Detect emotion from the text using Multi-label Classification"
      ],
      [
        "Score: 0.00",
        "We present GoEmotions, a large, manually annotated, carefully curated dataset for fine-grained emotion prediction. We provide a detailed data analysis, demonstrating the reliability of the annotations for the full taxonomy. We show the generalizability of the data across domains and taxonomies via transfer learning experiments. We build a strong baseline by fine-tuning a BERT model, however, the results suggest much room for future improvement. Future work can explore the cross-cultural robustness of emotion ratings, and extend the taxonomy to other languages and domains."
      ]
    ]
  },
  {
    "question": "What is the attribute nervousness about?",
    "answers": [
      [
        "Score: 0.01",
        "Figure 1 shows that gratitude, admiration and amusement have the highest and grief and nervousness have the lowest interrater correlation. Emotion frequency correlates with interrater agreement but the two are not equivalent. Infrequent emotions can have relatively high interrater correlation (e.g., fear), and frequent emotions can have have relatively low interrater correlation (e.g., annoyance)."
      ],
      [
        "Score: 0.01",
        "The emotion categories are: admiration, amusement, anger, annoyance, approval, caring, confusion, curiosity, desire, disappointment, disapproval, disgust, embarrassment, excitement, fear, gratitude, grief, joy, love, nervousness, optimism, pride, realization, relief, remorse, sadness, surprise. This directory includes the data and code for data analysis scripts. We also include code for our baseline model, which involves fine-tuning a pre-trained BERT-base model. For more details on the design and content of the dataset, please see our paper. Refer to our GoEmotions Model Card for recommended uses of models built with this data, as well as considerations and limitations relating to the GoEmotions data. Requirements See requirements.txt Setup Download the pre-trained BERT model from here and unzip them inside the bert directory. In the paper, we use the cased base model. Data Our raw dataset can be retrieved by running:"
      ],
      [
        "Score: 0.00",
        "Given the disparate frequencies among the labels, we normalize M by dividing the counts in each row (representing counts for each true emotion label) by the sum of that row. The heatmap in Figure 6 shows these normalized counts. We find that the model tends to confuse emotions that are related in sentiment and intensity (e.g., grief and sadness, pride and admiration, nervousness and fear)."
      ],
      [
        "Score: 0.00",
        "To better understand the relationship between emotions in our data, we look at their correlations. Let N be the number of examples in our dataset. We obtain N dimensional vectors for each emotion by averaging raters' judgments for all examples labeled with that emotion. We calculate Pearson correlation values between each pair of emotions. The heatmap in Figure 2 shows that emotions that are related in intensity (e.g. annoyance and anger, joy and excitement, nervousness and fear) have a strong positive correlation. On the other hand, emotions that have the opposite sentiment are negatively correlated. We also perform hierarchical clustering to uncover the nested structure of our taxonomy. We use correlation as a distance metric and ward as a linkage method, applied to the averaged ratings. The dendrogram on the top of Figure 2 shows that emotions that are related by intensity are neighbors, and that larger clusters map closely onto sentiment categories."
      ],
      [
        "Score: 0.00",
        "Grouping emotions. We create a hierarchical grouping of our taxonomy, and evaluate the model performance on each level of the hierarchy. A sentiment level divides the labels into 4 categoriespositive, negative, ambiguous and Neutral -with the Neutral category intact, and the rest of the mapping as shown in Figure 2 . The Ekman level further divides the taxonomy using the Neutral label and the following 6 groups: anger (maps to: anger, annoyance, disapproval), disgust (maps to: disgust), fear (maps to: fear, nervousness), joy (all positive emotions), sadness (maps to: sadness, disappointment, embarrassment, grief, remorse) and surprise (all ambiguous emotions)."
      ]
    ]
  },
  {
    "question": "A description of the attribute nervousness?",
    "answers": [
      [
        "Score: 0.01",
        "description: ### Context GoEmotions is a corpus of 58k carefully curated comments extracted from Reddit, with human annotations to 27 emotion categories or Neutral.  - Number of examples: 58,009. - Number of labels: 27 + Neutral. - Maximum sequence length in training and evaluation datasets: 30.   ### Content On top of the raw data, we also include a version filtered based on reter-agreement, which contains a train/test/validation split:  - Size of training dataset: 43,410. - Size of test dataset: 5,427. - Size of validation dataset: 5,426. The emotion categories are: admiration, amusement, anger, annoyance, approval, caring, confusion, curiosity, desire, disappointment, disapproval, disgust, embarrassment, excitement, fear, gratitude, grief, joy, love, nervousness, optimism, pride, realization, relief, remorse, sadness, surprise.  `analyze_data.py` and `extract_words.py` have already been run and the output files are stored in `plots` and `tables` respectively. What's inside is more than just rows and columns. Make it easy for others to get started by describing how you acquired the data and what time period it represents, too. Original repository from where the data is taken: [https://github.com/google-research/google-research/tree/master/goemotions](https://github.com/google-research/google-research/tree/master/goemotions) However, some of the scripts were giving errors which have been rectified and updated in this repository: [https://github.com/DebarshiChanda/google-research/tree/master/goemotions](https://github.com/DebarshiChanda/google-research/tree/master/goemotions)  ### Acknowledgements The entire Credit for creating this dataset goes to Google Research Team.   ### Inspiration Detect emotion from the text using Multi-label Classification"
      ],
      [
        "Score: 0.01",
        "The emotion categories are: admiration, amusement, anger, annoyance, approval, caring, confusion, curiosity, desire, disappointment, disapproval, disgust, embarrassment, excitement, fear, gratitude, grief, joy, love, nervousness, optimism, pride, realization, relief, remorse, sadness, surprise. This directory includes the data and code for data analysis scripts. We also include code for our baseline model, which involves fine-tuning a pre-trained BERT-base model. For more details on the design and content of the dataset, please see our paper. Refer to our GoEmotions Model Card for recommended uses of models built with this data, as well as considerations and limitations relating to the GoEmotions data. Requirements See requirements.txt Setup Download the pre-trained BERT model from here and unzip them inside the bert directory. In the paper, we use the cased base model. Data Our raw dataset can be retrieved by running:"
      ],
      [
        "Score: 0.01",
        "Figure 1 shows that gratitude, admiration and amusement have the highest and grief and nervousness have the lowest interrater correlation. Emotion frequency correlates with interrater agreement but the two are not equivalent. Infrequent emotions can have relatively high interrater correlation (e.g., fear), and frequent emotions can have have relatively low interrater correlation (e.g., annoyance)."
      ],
      [
        "Score: 0.00",
        "Grouping emotions. We create a hierarchical grouping of our taxonomy, and evaluate the model performance on each level of the hierarchy. A sentiment level divides the labels into 4 categoriespositive, negative, ambiguous and Neutral -with the Neutral category intact, and the rest of the mapping as shown in Figure 2 . The Ekman level further divides the taxonomy using the Neutral label and the following 6 groups: anger (maps to: anger, annoyance, disapproval), disgust (maps to: disgust), fear (maps to: fear, nervousness), joy (all positive emotions), sadness (maps to: sadness, disappointment, embarrassment, grief, remorse) and surprise (all ambiguous emotions)."
      ],
      [
        "Score: 0.00",
        "While reviewing the results from the pilot rounds, we identified and removed emotions that were scarcely selected by annotators and/or had low interrater agreement due to being very similar to other emotions or being too difficult to detect from text. These emotions were boredom, doubt, heartbroken, indifference and calmness. We also identified and added those emotions to our taxonomy that were frequently suggested by raters and/or seemed to be represented in the data upon manual inspection. These emotions were desire, disappointment, pride, realization, relief and remorse. In this process, we also refined the category names (e.g. replacing ecstasy with excitement), to ones that seemed interpretable to annotators. This is how we arrived at the final set of 27 emotions + Neutral. Our high interrater agreement in the final data can be partially explained by the fact that we took interpretability into consideration while constructing the taxonomy. The dataset is we are releasing was labeled in the third round over the final taxonomy."
      ]
    ]
  },
  {
    "question": "What is the attribute optimism about?",
    "answers": [
      [
        "Score: 0.60",
        "The emotion categories are: admiration, amusement, anger, annoyance, approval, caring, confusion, curiosity, desire, disappointment, disapproval, disgust, embarrassment, excitement, fear, gratitude, grief, joy, love, nervousness, optimism, pride, realization, relief, remorse, sadness, surprise. This directory includes the data and code for data analysis scripts. We also include code for our baseline model, which involves fine-tuning a pre-trained BERT-base model. For more details on the design and content of the dataset, please see our paper. Refer to our GoEmotions Model Card for recommended uses of models built with this data, as well as considerations and limitations relating to the GoEmotions data. Requirements See requirements.txt Setup Download the pre-trained BERT model from here and unzip them inside the bert directory. In the paper, we use the cased base model. Data Our raw dataset can be retrieved by running:"
      ],
      [
        "Score: 0.15",
        "Finding something impressive or worthy of respect."
      ],
      [
        "Score: 0.09",
        "While reviewing the results from the pilot rounds, we identified and removed emotions that were scarcely selected by annotators and/or had low interrater agreement due to being very similar to other emotions or being too difficult to detect from text. These emotions were boredom, doubt, heartbroken, indifference and calmness. We also identified and added those emotions to our taxonomy that were frequently suggested by raters and/or seemed to be represented in the data upon manual inspection. These emotions were desire, disappointment, pride, realization, relief and remorse. In this process, we also refined the category names (e.g. replacing ecstasy with excitement), to ones that seemed interpretable to annotators. This is how we arrived at the final set of 27 emotions + Neutral. Our high interrater agreement in the final data can be partially explained by the fact that we took interpretability into consideration while constructing the taxonomy. The dataset is we are releasing was labeled in the third round over the final taxonomy."
      ],
      [
        "Score: 0.08",
        "Figure 1 shows that gratitude, admiration and amusement have the highest and grief and nervousness have the lowest interrater correlation. Emotion frequency correlates with interrater agreement but the two are not equivalent. Infrequent emotions can have relatively high interrater correlation (e.g., fear), and frequent emotions can have have relatively low interrater correlation (e.g., annoyance)."
      ],
      [
        "Score: 0.04",
        "Figure 1 shows the distribution of emotion labels. We can see a large disparity in terms of emotion frequencies (e.g. admiration is 30 times more frequent than grief ), despite our emotion and sentiment balancing steps taken during data selection. This is expected given the disparate frequencies of emotions in natural human expression."
      ]
    ]
  },
  {
    "question": "A description of the attribute optimism?",
    "answers": [
      [
        "Score: 0.54",
        "While reviewing the results from the pilot rounds, we identified and removed emotions that were scarcely selected by annotators and/or had low interrater agreement due to being very similar to other emotions or being too difficult to detect from text. These emotions were boredom, doubt, heartbroken, indifference and calmness. We also identified and added those emotions to our taxonomy that were frequently suggested by raters and/or seemed to be represented in the data upon manual inspection. These emotions were desire, disappointment, pride, realization, relief and remorse. In this process, we also refined the category names (e.g. replacing ecstasy with excitement), to ones that seemed interpretable to annotators. This is how we arrived at the final set of 27 emotions + Neutral. Our high interrater agreement in the final data can be partially explained by the fact that we took interpretability into consideration while constructing the taxonomy. The dataset is we are releasing was labeled in the third round over the final taxonomy."
      ],
      [
        "Score: 0.48",
        "The emotion categories are: admiration, amusement, anger, annoyance, approval, caring, confusion, curiosity, desire, disappointment, disapproval, disgust, embarrassment, excitement, fear, gratitude, grief, joy, love, nervousness, optimism, pride, realization, relief, remorse, sadness, surprise. This directory includes the data and code for data analysis scripts. We also include code for our baseline model, which involves fine-tuning a pre-trained BERT-base model. For more details on the design and content of the dataset, please see our paper. Refer to our GoEmotions Model Card for recommended uses of models built with this data, as well as considerations and limitations relating to the GoEmotions data. Requirements See requirements.txt Setup Download the pre-trained BERT model from here and unzip them inside the bert directory. In the paper, we use the cased base model. Data Our raw dataset can be retrieved by running:"
      ],
      [
        "Score: 0.06",
        "Finding something impressive or worthy of respect."
      ],
      [
        "Score: 0.03",
        "One of the main aspects distinguishing our dataset is its emotion taxonomy. The vast majority of existing datasets contain annotations for minor variations of the 6 basic emotion categories (joy, anger, fear, sadness, disgust, and surprise) proposed by Ekman (1992a) and/or along affective dimensions (valence and arousal) that underpin the circumplex model of affect (Russell, 2003; Buechel and Hahn, 2017) ."
      ],
      [
        "Score: 0.03",
        "We present GoEmotions, a large, manually annotated, carefully curated dataset for fine-grained emotion prediction. We provide a detailed data analysis, demonstrating the reliability of the annotations for the full taxonomy. We show the generalizability of the data across domains and taxonomies via transfer learning experiments. We build a strong baseline by fine-tuning a BERT model, however, the results suggest much room for future improvement. Future work can explore the cross-cultural robustness of emotion ratings, and extend the taxonomy to other languages and domains."
      ]
    ]
  },
  {
    "question": "What is the attribute pride about?",
    "answers": [
      [
        "Score: 0.09",
        "Finding something impressive or worthy of respect."
      ],
      [
        "Score: 0.02",
        "While reviewing the results from the pilot rounds, we identified and removed emotions that were scarcely selected by annotators and/or had low interrater agreement due to being very similar to other emotions or being too difficult to detect from text. These emotions were boredom, doubt, heartbroken, indifference and calmness. We also identified and added those emotions to our taxonomy that were frequently suggested by raters and/or seemed to be represented in the data upon manual inspection. These emotions were desire, disappointment, pride, realization, relief and remorse. In this process, we also refined the category names (e.g. replacing ecstasy with excitement), to ones that seemed interpretable to annotators. This is how we arrived at the final set of 27 emotions + Neutral. Our high interrater agreement in the final data can be partially explained by the fact that we took interpretability into consideration while constructing the taxonomy. The dataset is we are releasing was labeled in the third round over the final taxonomy."
      ],
      [
        "Score: 0.01",
        "The emotion categories are: admiration, amusement, anger, annoyance, approval, caring, confusion, curiosity, desire, disappointment, disapproval, disgust, embarrassment, excitement, fear, gratitude, grief, joy, love, nervousness, optimism, pride, realization, relief, remorse, sadness, surprise. This directory includes the data and code for data analysis scripts. We also include code for our baseline model, which involves fine-tuning a pre-trained BERT-base model. For more details on the design and content of the dataset, please see our paper. Refer to our GoEmotions Model Card for recommended uses of models built with this data, as well as considerations and limitations relating to the GoEmotions data. Requirements See requirements.txt Setup Download the pre-trained BERT model from here and unzip them inside the bert directory. In the paper, we use the cased base model. Data Our raw dataset can be retrieved by running:"
      ],
      [
        "Score: 0.01",
        "Table 4 summarizes the performance of our best model, BERT, on the test set, which achieves an average F1-score of . 46 (std=.19 ). The model obtains the best performance on emotions with overt lexical markers, such as gratitude (.86), amusement (.8) and love (.78). The model obtains the lowest F1-score on grief (0), relief (.15) and realization (.21), which are the lowest frequency emotions. We find that less frequent emotions tend to be confused by the model with more frequent emotions related in sentiment and intensity (e.g., grief with sadness, pride with admiration, nervousness with fear)see Appendix G for a more detailed analysis."
      ],
      [
        "Score: 0.00",
        "Given the disparate frequencies among the labels, we normalize M by dividing the counts in each row (representing counts for each true emotion label) by the sum of that row. The heatmap in Figure 6 shows these normalized counts. We find that the model tends to confuse emotions that are related in sentiment and intensity (e.g., grief and sadness, pride and admiration, nervousness and fear)."
      ]
    ]
  },
  {
    "question": "A description of the attribute pride?",
    "answers": [
      [
        "Score: 0.15",
        "While reviewing the results from the pilot rounds, we identified and removed emotions that were scarcely selected by annotators and/or had low interrater agreement due to being very similar to other emotions or being too difficult to detect from text. These emotions were boredom, doubt, heartbroken, indifference and calmness. We also identified and added those emotions to our taxonomy that were frequently suggested by raters and/or seemed to be represented in the data upon manual inspection. These emotions were desire, disappointment, pride, realization, relief and remorse. In this process, we also refined the category names (e.g. replacing ecstasy with excitement), to ones that seemed interpretable to annotators. This is how we arrived at the final set of 27 emotions + Neutral. Our high interrater agreement in the final data can be partially explained by the fact that we took interpretability into consideration while constructing the taxonomy. The dataset is we are releasing was labeled in the third round over the final taxonomy."
      ],
      [
        "Score: 0.02",
        "Finding something impressive or worthy of respect."
      ],
      [
        "Score: 0.01",
        "The emotion categories are: admiration, amusement, anger, annoyance, approval, caring, confusion, curiosity, desire, disappointment, disapproval, disgust, embarrassment, excitement, fear, gratitude, grief, joy, love, nervousness, optimism, pride, realization, relief, remorse, sadness, surprise. This directory includes the data and code for data analysis scripts. We also include code for our baseline model, which involves fine-tuning a pre-trained BERT-base model. For more details on the design and content of the dataset, please see our paper. Refer to our GoEmotions Model Card for recommended uses of models built with this data, as well as considerations and limitations relating to the GoEmotions data. Requirements See requirements.txt Setup Download the pre-trained BERT model from here and unzip them inside the bert directory. In the paper, we use the cased base model. Data Our raw dataset can be retrieved by running:"
      ],
      [
        "Score: 0.01",
        "Table 4 summarizes the performance of our best model, BERT, on the test set, which achieves an average F1-score of . 46 (std=.19 ). The model obtains the best performance on emotions with overt lexical markers, such as gratitude (.86), amusement (.8) and love (.78). The model obtains the lowest F1-score on grief (0), relief (.15) and realization (.21), which are the lowest frequency emotions. We find that less frequent emotions tend to be confused by the model with more frequent emotions related in sentiment and intensity (e.g., grief with sadness, pride with admiration, nervousness with fear)see Appendix G for a more detailed analysis."
      ],
      [
        "Score: 0.00",
        "Given the disparate frequencies among the labels, we normalize M by dividing the counts in each row (representing counts for each true emotion label) by the sum of that row. The heatmap in Figure 6 shows these normalized counts. We find that the model tends to confuse emotions that are related in sentiment and intensity (e.g., grief and sadness, pride and admiration, nervousness and fear)."
      ]
    ]
  },
  {
    "question": "What is the attribute realization about?",
    "answers": [
      [
        "Score: 0.02",
        "Finding something impressive or worthy of respect."
      ],
      [
        "Score: 0.01",
        "While reviewing the results from the pilot rounds, we identified and removed emotions that were scarcely selected by annotators and/or had low interrater agreement due to being very similar to other emotions or being too difficult to detect from text. These emotions were boredom, doubt, heartbroken, indifference and calmness. We also identified and added those emotions to our taxonomy that were frequently suggested by raters and/or seemed to be represented in the data upon manual inspection. These emotions were desire, disappointment, pride, realization, relief and remorse. In this process, we also refined the category names (e.g. replacing ecstasy with excitement), to ones that seemed interpretable to annotators. This is how we arrived at the final set of 27 emotions + Neutral. Our high interrater agreement in the final data can be partially explained by the fact that we took interpretability into consideration while constructing the taxonomy. The dataset is we are releasing was labeled in the third round over the final taxonomy."
      ],
      [
        "Score: 0.00",
        "To better understand agreement among raters and the latent structure of the emotion space, we apply Principal Preserved Component Analysis (PPCA) (Cowen et al., 2019b) to our data. PPCA extracts linear combinations of attributes (here, emotion judgments), that maximally covary across two sets of data that measure the same attributes (here, randomly split judgments for each example). Thus, PPCA allows us to uncover latent dimensions of J \u2208 R n\u00d7|R|\u00d7|E| \u2190 all ratings for the examples annotated by r 7:"
      ],
      [
        "Score: 0.00",
        "The emotion categories are: admiration, amusement, anger, annoyance, approval, caring, confusion, curiosity, desire, disappointment, disapproval, disgust, embarrassment, excitement, fear, gratitude, grief, joy, love, nervousness, optimism, pride, realization, relief, remorse, sadness, surprise. This directory includes the data and code for data analysis scripts. We also include code for our baseline model, which involves fine-tuning a pre-trained BERT-base model. For more details on the design and content of the dataset, please see our paper. Refer to our GoEmotions Model Card for recommended uses of models built with this data, as well as considerations and limitations relating to the GoEmotions data. Requirements See requirements.txt Setup Download the pre-trained BERT model from here and unzip them inside the bert directory. In the paper, we use the cased base model. Data Our raw dataset can be retrieved by running:"
      ],
      [
        "Score: 0.00",
        "We conduct transfer learning experiments on existing emotion benchmarks, in order to show our data generalizes across domains and taxonomies. The goal is to demonstrate that given little labeled data in a target domain, one can utilize GoEmotions as baseline emotion understanding data. ity and taxonomy. In the interest of space, we only discuss three of these datasets here, chosen based on their diversity of domains. In our experiments, we observe similar trends for the additional benchmarks, and all are included in the Appendix H. The International Survey on Emotion Antecedents and Reactions (ISEAR) (Scherer and Wallbott, 1994 ) is a collection of personal reports on emotional events, written by 3000 people from different cultural backgrounds. The dataset contains 8k sentences, each labeled with a single emotion. The categories are anger, disgust, fear, guilt, joy, sadness and shame."
      ]
    ]
  },
  {
    "question": "A description of the attribute realization?",
    "answers": [
      [
        "Score: 0.04",
        "While reviewing the results from the pilot rounds, we identified and removed emotions that were scarcely selected by annotators and/or had low interrater agreement due to being very similar to other emotions or being too difficult to detect from text. These emotions were boredom, doubt, heartbroken, indifference and calmness. We also identified and added those emotions to our taxonomy that were frequently suggested by raters and/or seemed to be represented in the data upon manual inspection. These emotions were desire, disappointment, pride, realization, relief and remorse. In this process, we also refined the category names (e.g. replacing ecstasy with excitement), to ones that seemed interpretable to annotators. This is how we arrived at the final set of 27 emotions + Neutral. Our high interrater agreement in the final data can be partially explained by the fact that we took interpretability into consideration while constructing the taxonomy. The dataset is we are releasing was labeled in the third round over the final taxonomy."
      ],
      [
        "Score: 0.01",
        "Finding something impressive or worthy of respect."
      ],
      [
        "Score: 0.01",
        "To better understand agreement among raters and the latent structure of the emotion space, we apply Principal Preserved Component Analysis (PPCA) (Cowen et al., 2019b) to our data. PPCA extracts linear combinations of attributes (here, emotion judgments), that maximally covary across two sets of data that measure the same attributes (here, randomly split judgments for each example). Thus, PPCA allows us to uncover latent dimensions of J \u2208 R n\u00d7|R|\u00d7|E| \u2190 all ratings for the examples annotated by r 7:"
      ],
      [
        "Score: 0.01",
        "We present GoEmotions, a large, manually annotated, carefully curated dataset for fine-grained emotion prediction. We provide a detailed data analysis, demonstrating the reliability of the annotations for the full taxonomy. We show the generalizability of the data across domains and taxonomies via transfer learning experiments. We build a strong baseline by fine-tuning a BERT model, however, the results suggest much room for future improvement. Future work can explore the cross-cultural robustness of emotion ratings, and extend the taxonomy to other languages and domains."
      ],
      [
        "Score: 0.00",
        "We find that all 27 PPCs are highly significant. Specifically, Bonferroni-corrected p-values are less than 1.5e-6 for all dimensions (corrected \u03b1 = 0.0017), suggesting that the emotions were highly dissociable. Such a high degree of significance for all dimensions is nontrivial. For example, Cowen et al. (2019b) find that only 12 out of their 30 emotion categories are significantly dissociable."
      ]
    ]
  },
  {
    "question": "What is the attribute relief about?",
    "answers": [
      [
        "Score: 0.01",
        "While reviewing the results from the pilot rounds, we identified and removed emotions that were scarcely selected by annotators and/or had low interrater agreement due to being very similar to other emotions or being too difficult to detect from text. These emotions were boredom, doubt, heartbroken, indifference and calmness. We also identified and added those emotions to our taxonomy that were frequently suggested by raters and/or seemed to be represented in the data upon manual inspection. These emotions were desire, disappointment, pride, realization, relief and remorse. In this process, we also refined the category names (e.g. replacing ecstasy with excitement), to ones that seemed interpretable to annotators. This is how we arrived at the final set of 27 emotions + Neutral. Our high interrater agreement in the final data can be partially explained by the fact that we took interpretability into consideration while constructing the taxonomy. The dataset is we are releasing was labeled in the third round over the final taxonomy."
      ],
      [
        "Score: 0.01",
        "The emotion categories are: admiration, amusement, anger, annoyance, approval, caring, confusion, curiosity, desire, disappointment, disapproval, disgust, embarrassment, excitement, fear, gratitude, grief, joy, love, nervousness, optimism, pride, realization, relief, remorse, sadness, surprise. This directory includes the data and code for data analysis scripts. We also include code for our baseline model, which involves fine-tuning a pre-trained BERT-base model. For more details on the design and content of the dataset, please see our paper. Refer to our GoEmotions Model Card for recommended uses of models built with this data, as well as considerations and limitations relating to the GoEmotions data. Requirements See requirements.txt Setup Download the pre-trained BERT model from here and unzip them inside the bert directory. In the paper, we use the cased base model. Data Our raw dataset can be retrieved by running:"
      ],
      [
        "Score: 0.00",
        "Table 4 summarizes the performance of our best model, BERT, on the test set, which achieves an average F1-score of . 46 (std=.19 ). The model obtains the best performance on emotions with overt lexical markers, such as gratitude (.86), amusement (.8) and love (.78). The model obtains the lowest F1-score on grief (0), relief (.15) and realization (.21), which are the lowest frequency emotions. We find that less frequent emotions tend to be confused by the model with more frequent emotions related in sentiment and intensity (e.g., grief with sadness, pride with admiration, nervousness with fear)see Appendix G for a more detailed analysis."
      ],
      [
        "Score: 0.00",
        "description: ### Context GoEmotions is a corpus of 58k carefully curated comments extracted from Reddit, with human annotations to 27 emotion categories or Neutral.  - Number of examples: 58,009. - Number of labels: 27 + Neutral. - Maximum sequence length in training and evaluation datasets: 30.   ### Content On top of the raw data, we also include a version filtered based on reter-agreement, which contains a train/test/validation split:  - Size of training dataset: 43,410. - Size of test dataset: 5,427. - Size of validation dataset: 5,426. The emotion categories are: admiration, amusement, anger, annoyance, approval, caring, confusion, curiosity, desire, disappointment, disapproval, disgust, embarrassment, excitement, fear, gratitude, grief, joy, love, nervousness, optimism, pride, realization, relief, remorse, sadness, surprise.  `analyze_data.py` and `extract_words.py` have already been run and the output files are stored in `plots` and `tables` respectively. What's inside is more than just rows and columns. Make it easy for others to get started by describing how you acquired the data and what time period it represents, too. Original repository from where the data is taken: [https://github.com/google-research/google-research/tree/master/goemotions](https://github.com/google-research/google-research/tree/master/goemotions) However, some of the scripts were giving errors which have been rectified and updated in this repository: [https://github.com/DebarshiChanda/google-research/tree/master/goemotions](https://github.com/DebarshiChanda/google-research/tree/master/goemotions)  ### Acknowledgements The entire Credit for creating this dataset goes to Google Research Team.   ### Inspiration Detect emotion from the text using Multi-label Classification"
      ],
      [
        "Score: 0.00",
        "Even though we filter our data for the baseline experiments, we see particular value in the 4K examples that lack agreement. This subset of the data likely contains edge/difficult examples for the emotion domain (e.g., emotion-ambiguous text), and present challenges for further exploration. That is 66 ) agree ( 24 ) you ( 12 ) fuck ( 24 ) annoying ( 14 ) disappointing ( 11 ) not ( 16 ) confused ( 18 ) awesome ( 32 ) haha ( 32 ) not ( 13 ) worry ( 11 ) hate ( 18 ) stupid ( 13 ) disappointed (10) don't ( 14 ) why ( 11 ) amazing ( 30 ) funny ( 27 ) don't ( 12 ) careful ( 9 ) fucking ( 18 ) fucking ( 12 ) bad ( 9 ) disagree ( 9 ) sure ( 10 ) good ( 28 ) lmao ( 21 ) yes ( 12 ) stay ( 9 ) angry ( 11 ) shit ( 10 ) disappointment ( 7 ) nope ( 8 ) what ( 10 ) beautiful ( 23 ) hilarious ( 18 ) agreed ( 11 ) your ( 8 ) dare ( 10 ) dumb ( 9 ) unfortunately ( 7 ) doesn't ( 7 ) understand (8) desire excitement gratitude joy disgust embarrassment fear grief curiosity wish ( 29 ) excited ( 21 ) thanks ( 75 ) happy ( 32 ) disgusting ( 22 ) embarrassing ( 12 ) scared ( 16 ) died ( 6 ) curious ( 22 ) want ( 8 ) happy ( 8 ) thank ( 69 ) glad ( 27 ) awful ( 14 ) shame ( 11 ) afraid ( 16 ) rip ( 4 ) what ( 18 ) wanted ( 6 ) cake ( 8 ) for ( 24 ) enjoy ( 20 ) worst ( 13 ) awkward ( 10 ) scary ( 15 ) why ( 13 ) could ( 6 ) wow ( 8 ) you ( 18 ) enjoyed ( 12 ) worse ( 12 ) embarrassment ( 8 ) terrible ( 12 ) how ( 11 ) ambitious ( 4 ) interesting ( 7 ) sharing ( 17 ) fun ( 12 ) weird ( 9 ) embarrassed ( 7 ) terrifying ( 11 ) did (10) love optimism pride relief nervousness remorse sadness realization surprise love ( 76 ) hope ( 45 ) proud ( 14 ) glad ( 5 ) nervous ( 8 ) sorry ( 39 ) sad ( 31 ) realize ( 14 ) wow ( 23 ) loved ( 21 ) hopefully ( 19 ) pride ( 4 ) relieved ( 4 ) worried ( 8 ) regret ( 9 ) sadly ( 16 ) realized ( 12 ) surprised ( 21 ) favorite ( 13 ) luck (18) accomplishment relieving (4) anxiety ( 6 ) apologies ( 7 ) sorry ( 15 ) realised ( 7 ) wonder ( 15 ) loves ( 12 ) hoping ( 16 ) (4) relief ( 4 ) anxious ( 4 ) apologize ( 6 ) painful ( 10 ) realization ( 6 ) shocked ( 12 ) like ( 9 ) will ( 8 ) worrying ( 4 ) guilt ( 5 ) crying ( 9 ) thought ( 6 ) omg ( 11 )"
      ]
    ]
  },
  {
    "question": "A description of the attribute relief?",
    "answers": [
      [
        "Score: 0.05",
        "While reviewing the results from the pilot rounds, we identified and removed emotions that were scarcely selected by annotators and/or had low interrater agreement due to being very similar to other emotions or being too difficult to detect from text. These emotions were boredom, doubt, heartbroken, indifference and calmness. We also identified and added those emotions to our taxonomy that were frequently suggested by raters and/or seemed to be represented in the data upon manual inspection. These emotions were desire, disappointment, pride, realization, relief and remorse. In this process, we also refined the category names (e.g. replacing ecstasy with excitement), to ones that seemed interpretable to annotators. This is how we arrived at the final set of 27 emotions + Neutral. Our high interrater agreement in the final data can be partially explained by the fact that we took interpretability into consideration while constructing the taxonomy. The dataset is we are releasing was labeled in the third round over the final taxonomy."
      ],
      [
        "Score: 0.01",
        "Table 4 summarizes the performance of our best model, BERT, on the test set, which achieves an average F1-score of . 46 (std=.19 ). The model obtains the best performance on emotions with overt lexical markers, such as gratitude (.86), amusement (.8) and love (.78). The model obtains the lowest F1-score on grief (0), relief (.15) and realization (.21), which are the lowest frequency emotions. We find that less frequent emotions tend to be confused by the model with more frequent emotions related in sentiment and intensity (e.g., grief with sadness, pride with admiration, nervousness with fear)see Appendix G for a more detailed analysis."
      ],
      [
        "Score: 0.01",
        "description: ### Context GoEmotions is a corpus of 58k carefully curated comments extracted from Reddit, with human annotations to 27 emotion categories or Neutral.  - Number of examples: 58,009. - Number of labels: 27 + Neutral. - Maximum sequence length in training and evaluation datasets: 30.   ### Content On top of the raw data, we also include a version filtered based on reter-agreement, which contains a train/test/validation split:  - Size of training dataset: 43,410. - Size of test dataset: 5,427. - Size of validation dataset: 5,426. The emotion categories are: admiration, amusement, anger, annoyance, approval, caring, confusion, curiosity, desire, disappointment, disapproval, disgust, embarrassment, excitement, fear, gratitude, grief, joy, love, nervousness, optimism, pride, realization, relief, remorse, sadness, surprise.  `analyze_data.py` and `extract_words.py` have already been run and the output files are stored in `plots` and `tables` respectively. What's inside is more than just rows and columns. Make it easy for others to get started by describing how you acquired the data and what time period it represents, too. Original repository from where the data is taken: [https://github.com/google-research/google-research/tree/master/goemotions](https://github.com/google-research/google-research/tree/master/goemotions) However, some of the scripts were giving errors which have been rectified and updated in this repository: [https://github.com/DebarshiChanda/google-research/tree/master/goemotions](https://github.com/DebarshiChanda/google-research/tree/master/goemotions)  ### Acknowledgements The entire Credit for creating this dataset goes to Google Research Team.   ### Inspiration Detect emotion from the text using Multi-label Classification"
      ],
      [
        "Score: 0.00",
        "The emotion categories are: admiration, amusement, anger, annoyance, approval, caring, confusion, curiosity, desire, disappointment, disapproval, disgust, embarrassment, excitement, fear, gratitude, grief, joy, love, nervousness, optimism, pride, realization, relief, remorse, sadness, surprise. This directory includes the data and code for data analysis scripts. We also include code for our baseline model, which involves fine-tuning a pre-trained BERT-base model. For more details on the design and content of the dataset, please see our paper. Refer to our GoEmotions Model Card for recommended uses of models built with this data, as well as considerations and limitations relating to the GoEmotions data. Requirements See requirements.txt Setup Download the pre-trained BERT model from here and unzip them inside the bert directory. In the paper, we use the cased base model. Data Our raw dataset can be retrieved by running:"
      ],
      [
        "Score: 0.00",
        "We present GoEmotions, a large, manually annotated, carefully curated dataset for fine-grained emotion prediction. We provide a detailed data analysis, demonstrating the reliability of the annotations for the full taxonomy. We show the generalizability of the data across domains and taxonomies via transfer learning experiments. We build a strong baseline by fine-tuning a BERT model, however, the results suggest much room for future improvement. Future work can explore the cross-cultural robustness of emotion ratings, and extend the taxonomy to other languages and domains."
      ]
    ]
  },
  {
    "question": "What is the attribute remorse about?",
    "answers": [
      [
        "Score: 0.73",
        "The emotion categories are: admiration, amusement, anger, annoyance, approval, caring, confusion, curiosity, desire, disappointment, disapproval, disgust, embarrassment, excitement, fear, gratitude, grief, joy, love, nervousness, optimism, pride, realization, relief, remorse, sadness, surprise. This directory includes the data and code for data analysis scripts. We also include code for our baseline model, which involves fine-tuning a pre-trained BERT-base model. For more details on the design and content of the dataset, please see our paper. Refer to our GoEmotions Model Card for recommended uses of models built with this data, as well as considerations and limitations relating to the GoEmotions data. Requirements See requirements.txt Setup Download the pre-trained BERT model from here and unzip them inside the bert directory. In the paper, we use the cased base model. Data Our raw dataset can be retrieved by running:"
      ],
      [
        "Score: 0.34",
        "One of the main aspects distinguishing our dataset is its emotion taxonomy. The vast majority of existing datasets contain annotations for minor variations of the 6 basic emotion categories (joy, anger, fear, sadness, disgust, and surprise) proposed by Ekman (1992a) and/or along affective dimensions (valence and arousal) that underpin the circumplex model of affect (Russell, 2003; Buechel and Hahn, 2017) ."
      ],
      [
        "Score: 0.30",
        "To better understand agreement among raters and the latent structure of the emotion space, we apply Principal Preserved Component Analysis (PPCA) (Cowen et al., 2019b) to our data. PPCA extracts linear combinations of attributes (here, emotion judgments), that maximally covary across two sets of data that measure the same attributes (here, randomly split judgments for each example). Thus, PPCA allows us to uncover latent dimensions of J \u2208 R n\u00d7|R|\u00d7|E| \u2190 all ratings for the examples annotated by r 7:"
      ],
      [
        "Score: 0.17",
        "While reviewing the results from the pilot rounds, we identified and removed emotions that were scarcely selected by annotators and/or had low interrater agreement due to being very similar to other emotions or being too difficult to detect from text. These emotions were boredom, doubt, heartbroken, indifference and calmness. We also identified and added those emotions to our taxonomy that were frequently suggested by raters and/or seemed to be represented in the data upon manual inspection. These emotions were desire, disappointment, pride, realization, relief and remorse. In this process, we also refined the category names (e.g. replacing ecstasy with excitement), to ones that seemed interpretable to annotators. This is how we arrived at the final set of 27 emotions + Neutral. Our high interrater agreement in the final data can be partially explained by the fact that we took interpretability into consideration while constructing the taxonomy. The dataset is we are releasing was labeled in the third round over the final taxonomy."
      ],
      [
        "Score: 0.15",
        "We conduct transfer learning experiments on existing emotion benchmarks, in order to show our data generalizes across domains and taxonomies. The goal is to demonstrate that given little labeled data in a target domain, one can utilize GoEmotions as baseline emotion understanding data. ity and taxonomy. In the interest of space, we only discuss three of these datasets here, chosen based on their diversity of domains. In our experiments, we observe similar trends for the additional benchmarks, and all are included in the Appendix H. The International Survey on Emotion Antecedents and Reactions (ISEAR) (Scherer and Wallbott, 1994 ) is a collection of personal reports on emotional events, written by 3000 people from different cultural backgrounds. The dataset contains 8k sentences, each labeled with a single emotion. The categories are anger, disgust, fear, guilt, joy, sadness and shame."
      ]
    ]
  },
  {
    "question": "A description of the attribute remorse?",
    "answers": [
      [
        "Score: 0.78",
        "While reviewing the results from the pilot rounds, we identified and removed emotions that were scarcely selected by annotators and/or had low interrater agreement due to being very similar to other emotions or being too difficult to detect from text. These emotions were boredom, doubt, heartbroken, indifference and calmness. We also identified and added those emotions to our taxonomy that were frequently suggested by raters and/or seemed to be represented in the data upon manual inspection. These emotions were desire, disappointment, pride, realization, relief and remorse. In this process, we also refined the category names (e.g. replacing ecstasy with excitement), to ones that seemed interpretable to annotators. This is how we arrived at the final set of 27 emotions + Neutral. Our high interrater agreement in the final data can be partially explained by the fact that we took interpretability into consideration while constructing the taxonomy. The dataset is we are releasing was labeled in the third round over the final taxonomy."
      ],
      [
        "Score: 0.71",
        "The emotion categories are: admiration, amusement, anger, annoyance, approval, caring, confusion, curiosity, desire, disappointment, disapproval, disgust, embarrassment, excitement, fear, gratitude, grief, joy, love, nervousness, optimism, pride, realization, relief, remorse, sadness, surprise. This directory includes the data and code for data analysis scripts. We also include code for our baseline model, which involves fine-tuning a pre-trained BERT-base model. For more details on the design and content of the dataset, please see our paper. Refer to our GoEmotions Model Card for recommended uses of models built with this data, as well as considerations and limitations relating to the GoEmotions data. Requirements See requirements.txt Setup Download the pre-trained BERT model from here and unzip them inside the bert directory. In the paper, we use the cased base model. Data Our raw dataset can be retrieved by running:"
      ],
      [
        "Score: 0.38",
        "One of the main aspects distinguishing our dataset is its emotion taxonomy. The vast majority of existing datasets contain annotations for minor variations of the 6 basic emotion categories (joy, anger, fear, sadness, disgust, and surprise) proposed by Ekman (1992a) and/or along affective dimensions (valence and arousal) that underpin the circumplex model of affect (Russell, 2003; Buechel and Hahn, 2017) ."
      ],
      [
        "Score: 0.27",
        "To better understand agreement among raters and the latent structure of the emotion space, we apply Principal Preserved Component Analysis (PPCA) (Cowen et al., 2019b) to our data. PPCA extracts linear combinations of attributes (here, emotion judgments), that maximally covary across two sets of data that measure the same attributes (here, randomly split judgments for each example). Thus, PPCA allows us to uncover latent dimensions of J \u2208 R n\u00d7|R|\u00d7|E| \u2190 all ratings for the examples annotated by r 7:"
      ],
      [
        "Score: 0.20",
        "Grouping emotions. We create a hierarchical grouping of our taxonomy, and evaluate the model performance on each level of the hierarchy. A sentiment level divides the labels into 4 categoriespositive, negative, ambiguous and Neutral -with the Neutral category intact, and the rest of the mapping as shown in Figure 2 . The Ekman level further divides the taxonomy using the Neutral label and the following 6 groups: anger (maps to: anger, annoyance, disapproval), disgust (maps to: disgust), fear (maps to: fear, nervousness), joy (all positive emotions), sadness (maps to: sadness, disappointment, embarrassment, grief, remorse) and surprise (all ambiguous emotions)."
      ]
    ]
  },
  {
    "question": "What is the attribute sadness about?",
    "answers": [
      [
        "Score: 0.85",
        "The emotion categories are: admiration, amusement, anger, annoyance, approval, caring, confusion, curiosity, desire, disappointment, disapproval, disgust, embarrassment, excitement, fear, gratitude, grief, joy, love, nervousness, optimism, pride, realization, relief, remorse, sadness, surprise. This directory includes the data and code for data analysis scripts. We also include code for our baseline model, which involves fine-tuning a pre-trained BERT-base model. For more details on the design and content of the dataset, please see our paper. Refer to our GoEmotions Model Card for recommended uses of models built with this data, as well as considerations and limitations relating to the GoEmotions data. Requirements See requirements.txt Setup Download the pre-trained BERT model from here and unzip them inside the bert directory. In the paper, we use the cased base model. Data Our raw dataset can be retrieved by running:"
      ],
      [
        "Score: 0.55",
        "One of the main aspects distinguishing our dataset is its emotion taxonomy. The vast majority of existing datasets contain annotations for minor variations of the 6 basic emotion categories (joy, anger, fear, sadness, disgust, and surprise) proposed by Ekman (1992a) and/or along affective dimensions (valence and arousal) that underpin the circumplex model of affect (Russell, 2003; Buechel and Hahn, 2017) ."
      ],
      [
        "Score: 0.55",
        "To better understand agreement among raters and the latent structure of the emotion space, we apply Principal Preserved Component Analysis (PPCA) (Cowen et al., 2019b) to our data. PPCA extracts linear combinations of attributes (here, emotion judgments), that maximally covary across two sets of data that measure the same attributes (here, randomly split judgments for each example). Thus, PPCA allows us to uncover latent dimensions of J \u2208 R n\u00d7|R|\u00d7|E| \u2190 all ratings for the examples annotated by r 7:"
      ],
      [
        "Score: 0.34",
        "We conduct transfer learning experiments on existing emotion benchmarks, in order to show our data generalizes across domains and taxonomies. The goal is to demonstrate that given little labeled data in a target domain, one can utilize GoEmotions as baseline emotion understanding data. ity and taxonomy. In the interest of space, we only discuss three of these datasets here, chosen based on their diversity of domains. In our experiments, we observe similar trends for the additional benchmarks, and all are included in the Appendix H. The International Survey on Emotion Antecedents and Reactions (ISEAR) (Scherer and Wallbott, 1994 ) is a collection of personal reports on emotional events, written by 3000 people from different cultural backgrounds. The dataset contains 8k sentences, each labeled with a single emotion. The categories are anger, disgust, fear, guilt, joy, sadness and shame."
      ],
      [
        "Score: 0.34",
        "Figure 1 shows that gratitude, admiration and amusement have the highest and grief and nervousness have the lowest interrater correlation. Emotion frequency correlates with interrater agreement but the two are not equivalent. Infrequent emotions can have relatively high interrater correlation (e.g., fear), and frequent emotions can have have relatively low interrater correlation (e.g., annoyance)."
      ]
    ]
  },
  {
    "question": "A description of the attribute sadness?",
    "answers": [
      [
        "Score: 0.84",
        "The emotion categories are: admiration, amusement, anger, annoyance, approval, caring, confusion, curiosity, desire, disappointment, disapproval, disgust, embarrassment, excitement, fear, gratitude, grief, joy, love, nervousness, optimism, pride, realization, relief, remorse, sadness, surprise. This directory includes the data and code for data analysis scripts. We also include code for our baseline model, which involves fine-tuning a pre-trained BERT-base model. For more details on the design and content of the dataset, please see our paper. Refer to our GoEmotions Model Card for recommended uses of models built with this data, as well as considerations and limitations relating to the GoEmotions data. Requirements See requirements.txt Setup Download the pre-trained BERT model from here and unzip them inside the bert directory. In the paper, we use the cased base model. Data Our raw dataset can be retrieved by running:"
      ],
      [
        "Score: 0.83",
        "While reviewing the results from the pilot rounds, we identified and removed emotions that were scarcely selected by annotators and/or had low interrater agreement due to being very similar to other emotions or being too difficult to detect from text. These emotions were boredom, doubt, heartbroken, indifference and calmness. We also identified and added those emotions to our taxonomy that were frequently suggested by raters and/or seemed to be represented in the data upon manual inspection. These emotions were desire, disappointment, pride, realization, relief and remorse. In this process, we also refined the category names (e.g. replacing ecstasy with excitement), to ones that seemed interpretable to annotators. This is how we arrived at the final set of 27 emotions + Neutral. Our high interrater agreement in the final data can be partially explained by the fact that we took interpretability into consideration while constructing the taxonomy. The dataset is we are releasing was labeled in the third round over the final taxonomy."
      ],
      [
        "Score: 0.66",
        "One of the main aspects distinguishing our dataset is its emotion taxonomy. The vast majority of existing datasets contain annotations for minor variations of the 6 basic emotion categories (joy, anger, fear, sadness, disgust, and surprise) proposed by Ekman (1992a) and/or along affective dimensions (valence and arousal) that underpin the circumplex model of affect (Russell, 2003; Buechel and Hahn, 2017) ."
      ],
      [
        "Score: 0.64",
        "To better understand agreement among raters and the latent structure of the emotion space, we apply Principal Preserved Component Analysis (PPCA) (Cowen et al., 2019b) to our data. PPCA extracts linear combinations of attributes (here, emotion judgments), that maximally covary across two sets of data that measure the same attributes (here, randomly split judgments for each example). Thus, PPCA allows us to uncover latent dimensions of J \u2208 R n\u00d7|R|\u00d7|E| \u2190 all ratings for the examples annotated by r 7:"
      ],
      [
        "Score: 0.57",
        "Emotion balancing. We assign a predicted emotion to each comment using the pilot model described above. Then, we reduce emotion bias by downsampling the weakly-labelled data, capping by the number of comments belonging to the median emotion count."
      ]
    ]
  },
  {
    "question": "What is the attribute surprise about?",
    "answers": [
      [
        "Score: 0.08",
        "The emotion categories are: admiration, amusement, anger, annoyance, approval, caring, confusion, curiosity, desire, disappointment, disapproval, disgust, embarrassment, excitement, fear, gratitude, grief, joy, love, nervousness, optimism, pride, realization, relief, remorse, sadness, surprise. This directory includes the data and code for data analysis scripts. We also include code for our baseline model, which involves fine-tuning a pre-trained BERT-base model. For more details on the design and content of the dataset, please see our paper. Refer to our GoEmotions Model Card for recommended uses of models built with this data, as well as considerations and limitations relating to the GoEmotions data. Requirements See requirements.txt Setup Download the pre-trained BERT model from here and unzip them inside the bert directory. In the paper, we use the cased base model. Data Our raw dataset can be retrieved by running:"
      ],
      [
        "Score: 0.02",
        "Interestingly, emotions that we labeled as \"ambiguous\" in terms of sentiment (e.g. surprise) are closer to the positive than to the negative category. This suggests that in our data, ambiguous emotions are more likely to occur in the context of positive sentiment than that of negative sentiment."
      ],
      [
        "Score: 0.02",
        "One of the main aspects distinguishing our dataset is its emotion taxonomy. The vast majority of existing datasets contain annotations for minor variations of the 6 basic emotion categories (joy, anger, fear, sadness, disgust, and surprise) proposed by Ekman (1992a) and/or along affective dimensions (valence and arousal) that underpin the circumplex model of affect (Russell, 2003; Buechel and Hahn, 2017) ."
      ],
      [
        "Score: 0.01",
        "Figure 1 shows that gratitude, admiration and amusement have the highest and grief and nervousness have the lowest interrater correlation. Emotion frequency correlates with interrater agreement but the two are not equivalent. Infrequent emotions can have relatively high interrater correlation (e.g., fear), and frequent emotions can have have relatively low interrater correlation (e.g., annoyance)."
      ],
      [
        "Score: 0.01",
        "Finding something impressive or worthy of respect."
      ]
    ]
  },
  {
    "question": "A description of the attribute surprise?",
    "answers": [
      [
        "Score: 0.06",
        "The emotion categories are: admiration, amusement, anger, annoyance, approval, caring, confusion, curiosity, desire, disappointment, disapproval, disgust, embarrassment, excitement, fear, gratitude, grief, joy, love, nervousness, optimism, pride, realization, relief, remorse, sadness, surprise. This directory includes the data and code for data analysis scripts. We also include code for our baseline model, which involves fine-tuning a pre-trained BERT-base model. For more details on the design and content of the dataset, please see our paper. Refer to our GoEmotions Model Card for recommended uses of models built with this data, as well as considerations and limitations relating to the GoEmotions data. Requirements See requirements.txt Setup Download the pre-trained BERT model from here and unzip them inside the bert directory. In the paper, we use the cased base model. Data Our raw dataset can be retrieved by running:"
      ],
      [
        "Score: 0.05",
        "While reviewing the results from the pilot rounds, we identified and removed emotions that were scarcely selected by annotators and/or had low interrater agreement due to being very similar to other emotions or being too difficult to detect from text. These emotions were boredom, doubt, heartbroken, indifference and calmness. We also identified and added those emotions to our taxonomy that were frequently suggested by raters and/or seemed to be represented in the data upon manual inspection. These emotions were desire, disappointment, pride, realization, relief and remorse. In this process, we also refined the category names (e.g. replacing ecstasy with excitement), to ones that seemed interpretable to annotators. This is how we arrived at the final set of 27 emotions + Neutral. Our high interrater agreement in the final data can be partially explained by the fact that we took interpretability into consideration while constructing the taxonomy. The dataset is we are releasing was labeled in the third round over the final taxonomy."
      ],
      [
        "Score: 0.03",
        "One of the main aspects distinguishing our dataset is its emotion taxonomy. The vast majority of existing datasets contain annotations for minor variations of the 6 basic emotion categories (joy, anger, fear, sadness, disgust, and surprise) proposed by Ekman (1992a) and/or along affective dimensions (valence and arousal) that underpin the circumplex model of affect (Russell, 2003; Buechel and Hahn, 2017) ."
      ],
      [
        "Score: 0.02",
        "Grouping emotions. We create a hierarchical grouping of our taxonomy, and evaluate the model performance on each level of the hierarchy. A sentiment level divides the labels into 4 categoriespositive, negative, ambiguous and Neutral -with the Neutral category intact, and the rest of the mapping as shown in Figure 2 . The Ekman level further divides the taxonomy using the Neutral label and the following 6 groups: anger (maps to: anger, annoyance, disapproval), disgust (maps to: disgust), fear (maps to: fear, nervousness), joy (all positive emotions), sadness (maps to: sadness, disappointment, embarrassment, grief, remorse) and surprise (all ambiguous emotions)."
      ],
      [
        "Score: 0.02",
        "Interestingly, emotions that we labeled as \"ambiguous\" in terms of sentiment (e.g. surprise) are closer to the positive than to the negative category. This suggests that in our data, ambiguous emotions are more likely to occur in the context of positive sentiment than that of negative sentiment."
      ]
    ]
  },
  {
    "question": "What is the attribute neutral about?",
    "answers": [
      [
        "Score: 0.01",
        "While reviewing the results from the pilot rounds, we identified and removed emotions that were scarcely selected by annotators and/or had low interrater agreement due to being very similar to other emotions or being too difficult to detect from text. These emotions were boredom, doubt, heartbroken, indifference and calmness. We also identified and added those emotions to our taxonomy that were frequently suggested by raters and/or seemed to be represented in the data upon manual inspection. These emotions were desire, disappointment, pride, realization, relief and remorse. In this process, we also refined the category names (e.g. replacing ecstasy with excitement), to ones that seemed interpretable to annotators. This is how we arrived at the final set of 27 emotions + Neutral. Our high interrater agreement in the final data can be partially explained by the fact that we took interpretability into consideration while constructing the taxonomy. The dataset is we are releasing was labeled in the third round over the final taxonomy."
      ],
      [
        "Score: 0.01",
        "Grouping emotions. We create a hierarchical grouping of our taxonomy, and evaluate the model performance on each level of the hierarchy. A sentiment level divides the labels into 4 categoriespositive, negative, ambiguous and Neutral -with the Neutral category intact, and the rest of the mapping as shown in Figure 2 . The Ekman level further divides the taxonomy using the Neutral label and the following 6 groups: anger (maps to: anger, annoyance, disapproval), disgust (maps to: disgust), fear (maps to: fear, nervousness), joy (all positive emotions), sadness (maps to: sadness, disappointment, embarrassment, grief, remorse) and surprise (all ambiguous emotions)."
      ],
      [
        "Score: 0.00",
        "Sentiment balancing. We reduce sentiment bias by removing subreddits with little representation of positive, negative, ambiguous, or neutral sentiment. To estimate a comment's sentiment, we run our emotion prediction model, trained on a pilot batch of 2.2k annotated examples. The mapping of emotions into sentiment categories is found in Figure 2 . We exclude subreddits consisting of more than 30% neutral comments or less than 20% of negative, positive, or ambiguous comments."
      ],
      [
        "Score: 0.00",
        "description: ### Context GoEmotions is a corpus of 58k carefully curated comments extracted from Reddit, with human annotations to 27 emotion categories or Neutral.  - Number of examples: 58,009. - Number of labels: 27 + Neutral. - Maximum sequence length in training and evaluation datasets: 30.   ### Content On top of the raw data, we also include a version filtered based on reter-agreement, which contains a train/test/validation split:  - Size of training dataset: 43,410. - Size of test dataset: 5,427. - Size of validation dataset: 5,426. The emotion categories are: admiration, amusement, anger, annoyance, approval, caring, confusion, curiosity, desire, disappointment, disapproval, disgust, embarrassment, excitement, fear, gratitude, grief, joy, love, nervousness, optimism, pride, realization, relief, remorse, sadness, surprise.  `analyze_data.py` and `extract_words.py` have already been run and the output files are stored in `plots` and `tables` respectively. What's inside is more than just rows and columns. Make it easy for others to get started by describing how you acquired the data and what time period it represents, too. Original repository from where the data is taken: [https://github.com/google-research/google-research/tree/master/goemotions](https://github.com/google-research/google-research/tree/master/goemotions) However, some of the scripts were giving errors which have been rectified and updated in this repository: [https://github.com/DebarshiChanda/google-research/tree/master/goemotions](https://github.com/DebarshiChanda/google-research/tree/master/goemotions)  ### Acknowledgements The entire Credit for creating this dataset goes to Google Research Team.   ### Inspiration Detect emotion from the text using Multi-label Classification"
      ],
      [
        "Score: 0.00",
        "To better understand agreement among raters and the latent structure of the emotion space, we apply Principal Preserved Component Analysis (PPCA) (Cowen et al., 2019b) to our data. PPCA extracts linear combinations of attributes (here, emotion judgments), that maximally covary across two sets of data that measure the same attributes (here, randomly split judgments for each example). Thus, PPCA allows us to uncover latent dimensions of J \u2208 R n\u00d7|R|\u00d7|E| \u2190 all ratings for the examples annotated by r 7:"
      ]
    ]
  },
  {
    "question": "A description of the attribute neutral?",
    "answers": [
      [
        "Score: 0.09",
        "While reviewing the results from the pilot rounds, we identified and removed emotions that were scarcely selected by annotators and/or had low interrater agreement due to being very similar to other emotions or being too difficult to detect from text. These emotions were boredom, doubt, heartbroken, indifference and calmness. We also identified and added those emotions to our taxonomy that were frequently suggested by raters and/or seemed to be represented in the data upon manual inspection. These emotions were desire, disappointment, pride, realization, relief and remorse. In this process, we also refined the category names (e.g. replacing ecstasy with excitement), to ones that seemed interpretable to annotators. This is how we arrived at the final set of 27 emotions + Neutral. Our high interrater agreement in the final data can be partially explained by the fact that we took interpretability into consideration while constructing the taxonomy. The dataset is we are releasing was labeled in the third round over the final taxonomy."
      ],
      [
        "Score: 0.02",
        "Grouping emotions. We create a hierarchical grouping of our taxonomy, and evaluate the model performance on each level of the hierarchy. A sentiment level divides the labels into 4 categoriespositive, negative, ambiguous and Neutral -with the Neutral category intact, and the rest of the mapping as shown in Figure 2 . The Ekman level further divides the taxonomy using the Neutral label and the following 6 groups: anger (maps to: anger, annoyance, disapproval), disgust (maps to: disgust), fear (maps to: fear, nervousness), joy (all positive emotions), sadness (maps to: sadness, disappointment, embarrassment, grief, remorse) and surprise (all ambiguous emotions)."
      ],
      [
        "Score: 0.01",
        "description: ### Context GoEmotions is a corpus of 58k carefully curated comments extracted from Reddit, with human annotations to 27 emotion categories or Neutral.  - Number of examples: 58,009. - Number of labels: 27 + Neutral. - Maximum sequence length in training and evaluation datasets: 30.   ### Content On top of the raw data, we also include a version filtered based on reter-agreement, which contains a train/test/validation split:  - Size of training dataset: 43,410. - Size of test dataset: 5,427. - Size of validation dataset: 5,426. The emotion categories are: admiration, amusement, anger, annoyance, approval, caring, confusion, curiosity, desire, disappointment, disapproval, disgust, embarrassment, excitement, fear, gratitude, grief, joy, love, nervousness, optimism, pride, realization, relief, remorse, sadness, surprise.  `analyze_data.py` and `extract_words.py` have already been run and the output files are stored in `plots` and `tables` respectively. What's inside is more than just rows and columns. Make it easy for others to get started by describing how you acquired the data and what time period it represents, too. Original repository from where the data is taken: [https://github.com/google-research/google-research/tree/master/goemotions](https://github.com/google-research/google-research/tree/master/goemotions) However, some of the scripts were giving errors which have been rectified and updated in this repository: [https://github.com/DebarshiChanda/google-research/tree/master/goemotions](https://github.com/DebarshiChanda/google-research/tree/master/goemotions)  ### Acknowledgements The entire Credit for creating this dataset goes to Google Research Team.   ### Inspiration Detect emotion from the text using Multi-label Classification"
      ],
      [
        "Score: 0.00",
        "To this end, we compiled GoEmotions, the largest human annotated dataset of 58k carefully selected Reddit comments, labeled for 27 emotion categories or Neutral, with comments extracted from popular English subreddits. Table 1 shows an illustrative sample of our collected data. We design our emotion taxonomy considering related work in psychology and coverage in our data. In contrast to Ekman's taxonomy, which includes only one positive emotion (joy), our taxonomy includes a large number of positive, negative, and ambiguous emotion categories, making it suitable for downstream conversation understanding tasks that require a subtle understanding of emotion expression, such as the analysis of customer feedback or the enhancement of chatbots."
      ],
      [
        "Score: 0.00",
        "Sentiment balancing. We reduce sentiment bias by removing subreddits with little representation of positive, negative, ambiguous, or neutral sentiment. To estimate a comment's sentiment, we run our emotion prediction model, trained on a pilot batch of 2.2k annotated examples. The mapping of emotions into sentiment categories is found in Figure 2 . We exclude subreddits consisting of more than 30% neutral comments or less than 20% of negative, positive, or ambiguous comments."
      ]
    ]
  }
]
